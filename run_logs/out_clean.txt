19/10/08 20:32:15 WARN RunHelpers$: Missing tag isolation client: java.util.NoSuchElementException: key not found: TagDefinition(clientType,The client type for a request, used for isolating resources for the request.)
19/10/08 20:32:15 WARN MetastoreMonitor$: Uri scheme postgresql is not supported.
19/10/08 20:32:15 WARN MetastoreMonitor$: Unexpected partial metastore configuration (userOpt=Some(tedbuat), pw.isDefined=true, uriOpt=None)
19/10/08 20:32:16 WARN SparkConf: The configuration key 'spark.scheduler.listenerbus.eventqueue.size' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.scheduler.listenerbus.eventqueue.capacity' instead.
19/10/08 20:32:16 WARN SparkConf: The configuration key 'spark.akka.frameSize' has been deprecated as of Spark 1.6 and may be removed in the future. Please use the new key 'spark.rpc.message.maxSize' instead.
19/10/08 20:32:16 WARN SparkConf: Detected deprecated memory fraction settings: [spark.storage.memoryFraction, spark.shuffle.memoryFraction]. As of Spark 1.6, execution and storage memory management are unified. All memory fractions used in the old model are now deprecated and no longer read. If you wish to use the old memory management, you may explicitly enable `spark.memory.useLegacyMode` (not recommended).
datanucleus.autoCreateSchema=true
datanucleus.fixedDatastore=false
eventLog.rolloverIntervalSeconds=3600
fs.azure.account.key.teuatsa.blob.core.windows.net=IsrxMnnzLeO9N44beU4eMw0LaiKJdi1hHmKZFUiPa8nFNKZAFyhH0Hv/UCoC4qhgcdq63zwTRGFhUKV9d4ZsBw==
spark.akka.frameSize=256
spark.app.name=Databricks Shell
spark.cleaner.referenceTracking.blocking=false
spark.databricks.acl.client=com.databricks.spark.sql.acl.client.SparkSqlAclClient
spark.databricks.acl.dfAclsEnabled=false
spark.databricks.acl.provider=com.databricks.sql.acl.ReflectionBackedAclProvider
spark.databricks.aggressiveWindowDownS=300
spark.databricks.cloudProvider=Azure
spark.databricks.clusterSource=JOB
spark.databricks.clusterUsageTags.autoTerminationMinutes=0
spark.databricks.clusterUsageTags.clusterAggressiveAutoscalingWindowDownSeconds=300
spark.databricks.clusterUsageTags.clusterAllTags=[{"key":"Vendor","value":"Databricks"},{"key":"Creator","value":"basheer.ahamed@nielsen.com"},{"key":"ClusterName","value":"job-65695-run-1"},{"key":"ClusterId","value":"1008-202845-vogue653"},{"key":"DatabricksEnvironment","value":"workerenv-5614106561397268"}]
spark.databricks.clusterUsageTags.clusterCreator=JobLauncher
spark.databricks.clusterUsageTags.clusterGeneration=0
spark.databricks.clusterUsageTags.clusterId=1008-202845-vogue653
spark.databricks.clusterUsageTags.clusterLogDeliveryEnabled=true
spark.databricks.clusterUsageTags.clusterLogDestination=dbfs:/mnt/te_mount/uat_logs/
spark.databricks.clusterUsageTags.clusterMaxWorkers=5
spark.databricks.clusterUsageTags.clusterMetastoreAccessType=RDS_DIRECT
spark.databricks.clusterUsageTags.clusterMinWorkers=1
spark.databricks.clusterUsageTags.clusterName=job-65695-run-1
spark.databricks.clusterUsageTags.clusterNoDriverDaemon=false
spark.databricks.clusterUsageTags.clusterNodeType=Standard_E16s_v3
spark.databricks.clusterUsageTags.clusterNumSshKeys=0
spark.databricks.clusterUsageTags.clusterOwnerOrgId=5614106561397268
spark.databricks.clusterUsageTags.clusterOwnerUserId=*********(redacted)
spark.databricks.clusterUsageTags.clusterPinned=false
spark.databricks.clusterUsageTags.clusterPythonVersion=2
spark.databricks.clusterUsageTags.clusterResourceClass=default
spark.databricks.clusterUsageTags.clusterScalingType=autoscaling
spark.databricks.clusterUsageTags.clusterSku=STANDARD_SKU
spark.databricks.clusterUsageTags.clusterState=Pending
spark.databricks.clusterUsageTags.clusterStateMessage=Starting Spark
spark.databricks.clusterUsageTags.clusterTargetWorkers=1
spark.databricks.clusterUsageTags.clusterWorkers=1
spark.databricks.clusterUsageTags.containerType=LXC
spark.databricks.clusterUsageTags.driverContainerId=9206d3fcddb044288db66217141d2db8
spark.databricks.clusterUsageTags.driverContainerPrivateIp=10.139.64.31
spark.databricks.clusterUsageTags.driverInstanceId=45b042d078c048ac981c2cf65c0b17ea
spark.databricks.clusterUsageTags.driverInstancePrivateIp=10.139.0.30
spark.databricks.clusterUsageTags.driverNodeType=Standard_E16s_v3
spark.databricks.clusterUsageTags.driverPublicDns=20.186.33.64
spark.databricks.clusterUsageTags.enableCredentialPassthrough=*********(redacted)
spark.databricks.clusterUsageTags.enableDfAcls=false
spark.databricks.clusterUsageTags.enableElasticDisk=true
spark.databricks.clusterUsageTags.enableJdbcAutoStart=true
spark.databricks.clusterUsageTags.enableJobsAutostart=true
spark.databricks.clusterUsageTags.enableSqlAclsOnly=false
spark.databricks.clusterUsageTags.isSingleUserCluster=*********(redacted)
spark.databricks.clusterUsageTags.numPerClusterInitScriptsV2=0
spark.databricks.clusterUsageTags.sparkVersion=5.2.x-scala2.11
spark.databricks.clusterUsageTags.userProvidedRemoteVolumeCount=*********(redacted)
spark.databricks.clusterUsageTags.userProvidedRemoteVolumeSizeGb=*********(redacted)
spark.databricks.clusterUsageTags.userProvidedRemoteVolumeType=*********(redacted)
spark.databricks.clusterUsageTags.workerEnvironmentId=workerenv-5614106561397268
spark.databricks.credential.redactor=*********(redacted)
spark.databricks.delta.logStore.crossCloud.fatal=true
spark.databricks.delta.multiClusterWrites.enabled=true
spark.databricks.delta.preview.enabled=true
spark.databricks.driverNodeTypeId=Standard_E16s_v3
spark.databricks.eventLog.dir=eventlogs
spark.databricks.io.directoryCommit.enableLogicalDelete=false
spark.databricks.overrideDefaultCommitProtocol=org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol
spark.databricks.passthrough.adls.tokenProviderClassName=*********(redacted)
spark.databricks.passthrough.s3a.tokenProviderClassName=*********(redacted)
spark.databricks.preemption.enabled=true
spark.databricks.r.cleanWorkspace=true
spark.databricks.redactor=com.databricks.spark.util.DatabricksSparkLogRedactorProxy
spark.databricks.session.share=false
spark.databricks.sparkContextId=617644649427700606
spark.databricks.sql.hive.msck.batchSize=200
spark.databricks.sql.hive.msck.parallelism=20
spark.databricks.tahoe.logStore.aws.class=com.databricks.tahoe.store.MultiClusterLogStore
spark.databricks.tahoe.logStore.azure.class=com.databricks.tahoe.store.AzureLogStore
spark.databricks.tahoe.logStore.class=com.databricks.tahoe.store.DelegatingLogStore
spark.databricks.workerNodeTypeId=Standard_E16s_v3
spark.driver.allowMultipleContexts=false
spark.driver.maxResultSize=0
spark.driver.tempDirectory=/local_disk0/tmp
spark.eventLog.enabled=false
spark.executor.extraClassPath=*********(redacted)
spark.executor.extraJavaOptions=-Djava.io.tmpdir=/local_disk0/tmp -XX:ReservedCodeCacheSize=256m -XX:+UseCodeCacheFlushing -Djava.security.properties=/databricks/spark/dbconf/java/extra.security -XX:+PrintFlagsFinal -XX:+PrintGCDateStamps -verbose:gc -XX:+PrintGCDetails -Xss4m -Djavax.xml.datatype.DatatypeFactory=com.sun.org.apache.xerces.internal.jaxp.datatype.DatatypeFactoryImpl -Djavax.xml.parsers.DocumentBuilderFactory=com.sun.org.apache.xerces.internal.jaxp.DocumentBuilderFactoryImpl -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -Djavax.xml.validation.SchemaFactory:http://www.w3.org/2001/XMLSchema=com.sun.org.apache.xerces.internal.jaxp.validation.XMLSchemaFactory -Dorg.xml.sax.driver=com.sun.org.apache.xerces.internal.parsers.SAXParser -Dorg.w3c.dom.DOMImplementationSourceList=com.sun.org.apache.xerces.internal.dom.DOMXSImplementationSourceImpl -Ddatabricks.serviceName=spark-executor-1
spark.executor.memory=97871m
spark.executor.tempDirectory=/local_disk0/tmp
spark.extraListeners=com.databricks.backend.daemon.driver.DBCEventLoggingListener
spark.files.fetchFailure.unRegisterOutputOnHost=true
spark.files.overwrite=true
spark.files.useFetchCache=false
spark.hadoop.databricks.dbfs.client.version=v2
spark.hadoop.databricks.s3commit.client.sslTrustAll=false
spark.hadoop.datanucleus.connectionPool.maxIdle=1
spark.hadoop.datanucleus.connectionPool.maxPoolSize=30
spark.hadoop.datanucleus.connectionPool.minPoolSize=15
spark.hadoop.dfs.adls.oauth2.access.token.provider.type=*********(redacted)
spark.hadoop.dfs.adls.oauth2.client.id=*********(redacted)
spark.hadoop.dfs.adls.oauth2.credential=*********(redacted)
spark.hadoop.dfs.adls.oauth2.refresh.url=*********(redacted)
spark.hadoop.fs.abfs.impl=shaded.databricks.v20180920_b33d810.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem
spark.hadoop.fs.abfs.impl.disable.cache=true
spark.hadoop.fs.abfss.impl=shaded.databricks.v20180920_b33d810.org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem
spark.hadoop.fs.abfss.impl.disable.cache=true
spark.hadoop.fs.adl.impl=com.databricks.adl.AdlFileSystem
spark.hadoop.fs.adl.impl.disable.cache=false
spark.hadoop.fs.azure.skip.metrics=true
spark.hadoop.fs.s3.impl=com.databricks.s3a.S3AFileSystem
spark.hadoop.fs.s3a.connection.maximum=200
spark.hadoop.fs.s3a.fast.upload=true
spark.hadoop.fs.s3a.fast.upload.default=true
spark.hadoop.fs.s3a.impl=com.databricks.s3a.S3AFileSystem
spark.hadoop.fs.s3a.multipart.size=10485760
spark.hadoop.fs.s3a.multipart.threshold=104857600
spark.hadoop.fs.s3a.threads.max=136
spark.hadoop.fs.s3n.impl=com.databricks.s3a.S3AFileSystem
spark.hadoop.fs.wasb.impl=shaded.databricks.org.apache.hadoop.fs.azure.NativeAzureFileSystem
spark.hadoop.fs.wasb.impl.disable.cache=true
spark.hadoop.fs.wasbs.impl=shaded.databricks.org.apache.hadoop.fs.azure.NativeAzureFileSystem
spark.hadoop.fs.wasbs.impl.disable.cache=true
spark.hadoop.hive.server2.idle.operation.timeout=7200000
spark.hadoop.hive.server2.idle.session.timeout=900000
spark.hadoop.hive.server2.keystore.password=*********(redacted)
spark.hadoop.hive.server2.keystore.path=/databricks/keys/jetty-ssl-driver-keystore.jks
spark.hadoop.hive.server2.session.check.interval=60000
spark.hadoop.hive.server2.thrift.http.port=10000
spark.hadoop.hive.server2.transport.mode=http
spark.hadoop.hive.server2.use.SSL=true
spark.hadoop.hive.warehouse.subdir.inherit.perms=false
spark.hadoop.javax.jdo.option.ConnectionDriverName=org.postgresql.Driver
spark.hadoop.javax.jdo.option.ConnectionPassword=*********(redacted)
spark.hadoop.javax.jdo.option.ConnectionURL=*********(redacted)
spark.hadoop.javax.jdo.option.ConnectionUserName=*********(redacted)
spark.hadoop.mapred.output.committer.class=com.databricks.backend.daemon.data.client.DirectOutputCommitter
spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version=2
spark.hadoop.parquet.memory.pool.ratio=0.5
spark.hadoop.spark.sql.parquet.output.committer.class=org.apache.spark.sql.parquet.DirectParquetOutputCommitter
spark.hadoop.spark.sql.sources.outputCommitterClass=com.databricks.backend.daemon.data.client.MapReduceDirectOutputCommitter
spark.hadoop.spark.thriftserver.customHeadersToProperties=*********(redacted)
spark.home=/databricks/spark
spark.logConf=true
spark.master=spark://10.139.64.31:7077
spark.metrics.conf=/databricks/spark/conf/metrics.properties
spark.r.backendConnectionTimeout=604800
spark.r.numRBackendThreads=1
spark.rdd.compress=true
spark.repl.class.outputDir=/local_disk0/tmp/repl/spark-617644649427700606-2741f5b8-99ac-4643-b73d-fe451ffd09c9
spark.scheduler.listenerbus.eventqueue.size=20000
spark.scheduler.mode=FAIR
spark.serializer.objectStreamReset=100
spark.shuffle.manager=SORT
spark.shuffle.memoryFraction=0.2
spark.shuffle.reduceLocality.enabled=false
spark.shuffle.service.enabled=true
spark.shuffle.service.port=4048
spark.sparkr.use.daemon=false
spark.speculation=false
spark.speculation.multiplier=3
spark.speculation.quantile=0.9
spark.sql.allowMultipleContexts=false
spark.sql.hive.convertCTAS=true
spark.sql.hive.convertMetastoreParquet=true
spark.sql.hive.metastore.jars=maven
spark.sql.hive.metastore.sharedPrefixes=org.mariadb.jdbc,com.mysql.jdbc,org.postgresql,com.microsoft.sqlserver,microsoft.sql.DateTimeOffset,microsoft.sql.Types,com.databricks,com.codahale,com.fasterxml.jackson,shaded.databricks
spark.sql.hive.metastore.version=2.3.0
spark.sql.parquet.cacheMetadata=true
spark.sql.parquet.compression.codec=snappy
spark.sql.parquet.filterPushdown=false
spark.sql.shuffle.partitions=2000
spark.sql.sources.commitProtocolClass=com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol
spark.sql.streaming.checkpointFileManagerClass=com.databricks.spark.sql.streaming.DatabricksCheckpointFileManager
spark.sql.ui.retainedExecutions=100
spark.sql.warehouse.dir=*********(redacted)
spark.storage.blockManagerTimeoutIntervalMs=300000
spark.storage.memoryFraction=0.5
spark.streaming.driver.writeAheadLog.allowBatching=true
spark.streaming.driver.writeAheadLog.closeFileAfterWrite=true
spark.task.maxFailures=10
spark.task.reaper.enabled=true
spark.task.reaper.killTimeout=60s
spark.ui.port=46241
spark.worker.cleanup.enabled=false
19/10/08 20:32:17 WARN MetricsSystem: Using default name SparkStatusTracker for source because neither spark.metrics.namespace nor spark.app.id is set.
19/10/08 20:32:18 WARN FairSchedulableBuilder: Fair Scheduler configuration file not found so jobs will be scheduled in FIFO order. To use fair scheduling, configure pools in fairscheduler.xml or set spark.scheduler.allocation.file to a file that contains the configuration.
19/10/08 20:32:26 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0
19/10/08 20:32:27 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
19/10/08 20:32:28 WARN EC2MetadataUtils: Unable to retrieve the requested metadata.
19/10/08 20:32:29 WARN SparkConf: The configuration key 'spark.scheduler.listenerbus.eventqueue.size' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.scheduler.listenerbus.eventqueue.capacity' instead.
19/10/08 20:32:29 WARN SparkConf: The configuration key 'spark.akka.frameSize' has been deprecated as of Spark 1.6 and may be removed in the future. Please use the new key 'spark.rpc.message.maxSize' instead.
19/10/08 20:32:29 WARN SecurityHandler: ServletContext@o.e.j.s.ServletContextHandler@4bd5eed1{/,null,STARTING} has uncovered http methods for path: /*
19/10/08 20:32:39 WARN SparkContext: Using an existing SparkContext; some configuration may not take effect.
19/10/08 20:33:59 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
19/10/08 20:33:59 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
19/10/08 20:34:01 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.
19/10/08 20:34:01 WARN SparkContext: The jar /local_disk0/tmp/addedFile4288559803743625421xml_apis_xml_apis_1_0_b2-167e6.jar has been added already. Overwriting of added jars is not supported in the current version.
19/10/08 20:34:01 WARN SparkContext: The jar /local_disk0/tmp/addedFile273006811757787853org_scalaj_scalaj_http_2_11_2_3_0-d69e4.jar has been added already. Overwriting of added jars is not supported in the current version.
19/10/08 20:34:01 WARN SparkContext: The jar /local_disk0/tmp/addedFile3861273472656763977dbfs__FileStore_saak9001_transengine_1_0_SNAPSHOT-ab9b9.jar has been added already. Overwriting of added jars is not supported in the current version.
19/10/08 20:34:01 WARN SparkContext: The jar /local_disk0/tmp/addedFile7181962443514742775org_rogach_scallop_2_11_3_3_1-ecc25.jar has been added already. Overwriting of added jars is not supported in the current version.
19/10/08 20:34:01 WARN SparkContext: The jar /local_disk0/tmp/addedFile4236943839527755406dbfs__FileStore_saak9001_pqm_core_models_1_0-b0ccd.jar has been added already. Overwriting of added jars is not supported in the current version.
19/10/08 20:34:01 WARN SparkContext: The jar /local_disk0/tmp/addedFile5664496069525579088com_typesafe_config_1_3_1-e3e42.jar has been added already. Overwriting of added jars is not supported in the current version.
19/10/08 20:34:01 WARN SparkContext: The jar /local_disk0/tmp/addedFile7734078261766822683io_spray_spray_json_2_11_1_3_5-d5f78.jar has been added already. Overwriting of added jars is not supported in the current version.
19/10/08 20:34:01 WARN SparkContext: The jar /local_disk0/tmp/addedFile462333793301146524dom4j_dom4j_1_6_1-3b35e.jar has been added already. Overwriting of added jars is not supported in the current version.
19/10/08 20:34:02 WARN ScalaDriverLocal: User entity information is incomplete, missing userId,orgId,user.
19/10/08 20:34:10 WARN SparkContext: The jar /local_disk0/tmp/addedFile4288559803743625421xml_apis_xml_apis_1_0_b2-167e6.jar has been added already. Overwriting of added jars is not supported in the current version.
19/10/08 20:34:10 WARN SparkContext: The jar /local_disk0/tmp/addedFile273006811757787853org_scalaj_scalaj_http_2_11_2_3_0-d69e4.jar has been added already. Overwriting of added jars is not supported in the current version.
19/10/08 20:34:10 WARN SparkContext: The jar /local_disk0/tmp/addedFile3861273472656763977dbfs__FileStore_saak9001_transengine_1_0_SNAPSHOT-ab9b9.jar has been added already. Overwriting of added jars is not supported in the current version.
19/10/08 20:34:10 WARN SparkContext: The jar /local_disk0/tmp/addedFile7181962443514742775org_rogach_scallop_2_11_3_3_1-ecc25.jar has been added already. Overwriting of added jars is not supported in the current version.
19/10/08 20:34:10 WARN SparkContext: The jar /local_disk0/tmp/addedFile4236943839527755406dbfs__FileStore_saak9001_pqm_core_models_1_0-b0ccd.jar has been added already. Overwriting of added jars is not supported in the current version.
19/10/08 20:34:10 WARN SparkContext: The jar /local_disk0/tmp/addedFile5664496069525579088com_typesafe_config_1_3_1-e3e42.jar has been added already. Overwriting of added jars is not supported in the current version.
19/10/08 20:34:10 WARN SparkContext: The jar /local_disk0/tmp/addedFile7734078261766822683io_spray_spray_json_2_11_1_3_5-d5f78.jar has been added already. Overwriting of added jars is not supported in the current version.
19/10/08 20:34:10 WARN SparkContext: The jar /local_disk0/tmp/addedFile462333793301146524dom4j_dom4j_1_6_1-3b35e.jar has been added already. Overwriting of added jars is not supported in the current version.
19/10/08 20:34:10 WARN ScalaDriverLocal: User entity information is incomplete, missing userId,user.
19/10/08 20:34:13 WARN SparkContext: Using an existing SparkContext; some configuration may not take effect.
19/10/08 20:34:13 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
19/10/08 20:34:14 WARN HiveConf: HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist
19/10/08 20:34:14 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
19/10/08 20:34:14 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
19/10/08 20:34:14 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException
19/10/08 20:34:15 DEBUG AccessTokenProvider: AADToken: no token. Returning expiring=true
19/10/08 20:34:15 DEBUG AccessTokenProvider: AAD Token is missing or expired: Calling refresh-token from abstract base class
19/10/08 20:34:15 DEBUG ClientCredsTokenProvider: AADToken: refreshing client-credential based token
19/10/08 20:34:15 DEBUG AzureADAuthenticator: AADToken: starting to fetch token using client creds for client ID 003106c3-ff8c-4c62-a060-24b8b32ea512
19/10/08 20:34:15 DEBUG AzureADAuthenticator: AADToken: fetched token with expiry Wed Oct 09 04:34:15 UTC 2019
19/10/08 20:34:15 DEBUG HttpTransport: HTTPRequest,Failed,cReqId:e6a193d8-db7e-4f1b-98fd-5aec956c7d79.0,lat:8,err:HTTP404(FileNotFoundException),Reqlen:0,Resplen:290,token_ns:5800,sReqId:414c131a-37ca-4c20-8357-ff34d390f55a,path:/user/hive/warehouse/PQM_TYSON_1000233_E2E.db/fg_prdc_dim_final_output,qp:op=GETFILESTATUS&tooid=true&api-version=2017-08-01
19/10/08 20:34:15 DEBUG HttpTransport: HTTPRequest,Failed,cReqId:26ca4b15-b08c-4e6d-bdfe-8a25965fff82.0,lat:8,err:HTTP404(FileNotFoundException),Reqlen:0,Resplen:289,token_ns:10700,sReqId:4689dc66-4b8f-41e9-914c-c45ef7dc97ec,path:/user/hive/warehouse/PQM_TYSON_1000233_E2E.db/fg_prdc_dim_int_output1,qp:op=GETFILESTATUS&tooid=true&api-version=2017-08-01
19/10/08 20:34:16 DEBUG HttpTransport: HTTPRequest,Failed,cReqId:af08736d-56b0-47e9-a2ae-3280299da16f.0,lat:6,err:HTTP404(FileNotFoundException),Reqlen:0,Resplen:274,token_ns:3700,sReqId:adbf72d6-6252-442c-9982-84b81a5cb034,path:/user/hive/warehouse/PQM_TYSON_1000233_E2E.db/mrkt_dim,qp:op=GETFILESTATUS&tooid=true&api-version=2017-08-01
19/10/08 20:34:16 DEBUG HttpTransport: HTTPRequest,Failed,cReqId:4813b550-8e6b-48ec-a3ae-2d6bbec7d06b.0,lat:7,err:HTTP404(FileNotFoundException),Reqlen:0,Resplen:283,token_ns:4800,sReqId:156dd308-0bd5-4aba-88fa-e4b1bb367f98,path:/user/hive/warehouse/PQM_TYSON_1000233_E2E.db/mt_period_mapping,qp:op=GETFILESTATUS&tooid=true&api-version=2017-08-01
19/10/08 20:34:16 DEBUG HttpTransport: HTTPRequest,Failed,cReqId:73ad18ec-4145-4653-8232-22dbdada099e.0,lat:6,err:HTTP404(FileNotFoundException),Reqlen:0,Resplen:274,token_ns:3900,sReqId:6f6d0ab6-e082-45d1-8a40-71f69464b3a9,path:/user/hive/warehouse/PQM_TYSON_1000233_E2E.db/prdc_dim,qp:op=GETFILESTATUS&tooid=true&api-version=2017-08-01
19/10/08 20:34:16 DEBUG HttpTransport: HTTPRequest,Failed,cReqId:442ed1e6-fb97-4450-8818-6e6acfc29054.0,lat:6,err:HTTP404(FileNotFoundException),Reqlen:0,Resplen:273,token_ns:3900,sReqId:e379493a-3bb2-40f2-b8aa-25de695dff48,path:/user/hive/warehouse/PQM_TYSON_1000233_E2E.db/prd_dim,qp:op=GETFILESTATUS&tooid=true&api-version=2017-08-01
19/10/08 20:34:16 DEBUG HttpTransport: HTTPRequest,Failed,cReqId:5cbf778c-934c-4455-9b72-f525f73e03ac.0,lat:7,err:HTTP404(FileNotFoundException),Reqlen:0,Resplen:278,token_ns:4500,sReqId:be9f1210-8cc5-404f-9714-96da77030391,path:/user/hive/warehouse/PQM_TYSON_1000233_E2E.db/prd_dim_prev,qp:op=GETFILESTATUS&tooid=true&api-version=2017-08-01
19/10/08 20:34:19 DEBUG HttpTransport: HTTPRequest,Failed,cReqId:7a32216b-8d76-485b-b5e1-e1cf4f99e88c.0,lat:6,err:HTTP404(FileNotFoundException),Reqlen:0,Resplen:286,token_ns:3400,sReqId:deeece04-511d-4c6a-a8d2-8181d2a7d9d5,path:/user/hive/warehouse/PQM_TYSON_1000233_E2E.db/trag_aggregated_data,qp:op=GETFILESTATUS&tooid=true&api-version=2017-08-01
19/10/08 20:34:19 DEBUG HttpTransport: HTTPRequest,Failed,cReqId:1f4bcc25-e22e-41a0-836a-b953d4f2f3d0.0,lat:8,err:HTTP404(FileNotFoundException),Reqlen:0,Resplen:288,token_ns:4100,sReqId:bbc984b6-5a39-4bc7-bac1-92e1204bcfd1,path:/user/hive/warehouse/PQM_TYSON_1000233_E2E.db/trag_master_track_data,qp:op=GETFILESTATUS&tooid=true&api-version=2017-08-01
19/10/08 20:34:21 DEBUG HttpTransport: HTTPRequest,Failed,cReqId:c9c5badf-7a40-457e-b8d6-c5e07fe07c33.0,lat:8,err:HTTP404(FileNotFoundException),Reqlen:0,Resplen:284,token_ns:3800,sReqId:f868567b-2d90-417d-b47e-54c8ce264ec4,path:/user/hive/warehouse/PQM_TYSON_1000233_E2E.db/trag_universe_data,qp:op=GETFILESTATUS&tooid=true&api-version=2017-08-01
19/10/08 20:34:21 DEBUG HttpTransport: HTTPRequest,Failed,cReqId:34999c8a-26fb-4870-8e1f-65f64d81087c.0,lat:7,err:HTTP404(FileNotFoundException),Reqlen:0,Resplen:265,token_ns:3900,sReqId:ca5d9203-5933-438b-b454-e89055e044a6,path:/user/hive/warehouse/PQM_TYSON_1000233_E2E.db,qp:op=GETFILESTATUS&tooid=true&api-version=2017-08-01
hiveDefaultUrl=null
hiveSiteURL=null
hiveServer2SiteUrl=null
hivemetastoreSiteUrl=null
Values omitted for security reason if present: [fs.s3n.awsAccessKeyId, fs.s3a.access.key, fs.s3.awsAccessKeyId, hive.server2.keystore.password, fs.s3a.proxy.password, javax.jdo.option.ConnectionPassword, fs.s3.awsSecretAccessKey, fs.s3n.awsSecretAccessKey, fs.s3a.secret.key]
_hive.hdfs.session.path=
    /tmp/hive/root/5299c531-93ff-4c80-b9a4-d52a5bb3de16:
_hive.local.session.path=
    /local_disk0/tmp/root/5299c531-93ff-4c80-b9a4-d52a5bb3de16:
_hive.tmp_table_space=/tmp/hive/root/5299c531-93ff-4c80-b9a4-d52a5bb3de16/_tmp_space.db
databricks.dbfs.client.version=v2
databricks.s3commit.client.sslTrustAll=false
datanucleus.autoCreateSchema=true
datanucleus.autoStartMechanismMode=ignored
datanucleus.cache.level2=false
datanucleus.cache.level2.type=none
datanucleus.connectionPool.initialPoolSize=0
datanucleus.connectionPool.maxIdle=1
datanucleus.connectionPool.maxPoolSize=30
datanucleus.connectionPool.minPoolSize=15
datanucleus.connectionPoolingType=BONECP
datanucleus.fixedDatastore=false
datanucleus.identifierFactory=datanucleus1
datanucleus.plugin.pluginRegistryBundleCheck=LOG
datanucleus.rdbms.initializeColumnInfo=NONE
datanucleus.rdbms.useLegacyNativeValueStrategy=true
datanucleus.schema.autoCreateAll=false
datanucleus.schema.validateColumns=false
datanucleus.schema.validateConstraints=false
datanucleus.schema.validateTables=false
datanucleus.storeManagerType=rdbms
datanucleus.transactionIsolation=read-committed
dfs.adls.oauth2.access.token.provider.type=ClientCredential
dfs.adls.oauth2.client.id=003106c3-ff8c-4c62-a060-24b8b32ea512
dfs.adls.oauth2.credential=viUHaEfNg8cHQRJSRNMzpIv1T6goiUg+c9R/XSbYBFA=
dfs.adls.oauth2.refresh.url=https://login.microsoftonline.com/6ac7a1f4-5fb1-4153-bb4f-12d2020a1f7d/oauth2/token
dfs.block.access.key.update.interval=600
dfs.block.access.token.enable=false
dfs.block.access.token.lifetime=600
dfs.block.scanner.volume.bytes.per.second=1048576
dfs.blockreport.initialDelay=0
dfs.blockreport.intervalMsec=21600000
dfs.blockreport.split.threshold=1000000
dfs.blocksize=134217728
dfs.bytes-per-checksum=512
dfs.cachereport.intervalMsec=10000
dfs.client-write-packet-size=65536
dfs.client.block.write.replace-datanode-on-failure.best-effort=false
dfs.client.block.write.replace-datanode-on-failure.enable=true
dfs.client.block.write.replace-datanode-on-failure.policy=DEFAULT
dfs.client.block.write.retries=3
dfs.client.cached.conn.retry=3
dfs.client.context=default
dfs.client.datanode-restart.timeout=30
dfs.client.domain.socket.data.traffic=false
dfs.client.failover.connection.retries=0
dfs.client.failover.connection.retries.on.timeouts=0
dfs.client.failover.max.attempts=15
dfs.client.failover.sleep.base.millis=500
dfs.client.failover.sleep.max.millis=15000
dfs.client.file-block-storage-locations.num-threads=10
dfs.client.file-block-storage-locations.timeout.millis=1000
dfs.client.https.keystore.resource=ssl-client.xml
dfs.client.https.need-auth=false
dfs.client.mmap.cache.size=256
dfs.client.mmap.cache.timeout.ms=3600000
dfs.client.mmap.enabled=true
dfs.client.mmap.retry.timeout.ms=300000
dfs.client.read.shortcircuit=false
dfs.client.read.shortcircuit.skip.checksum=false
dfs.client.read.shortcircuit.streams.cache.expiry.ms=300000
dfs.client.read.shortcircuit.streams.cache.size=256
dfs.client.short.circuit.replica.stale.threshold.ms=1800000
dfs.client.slow.io.warning.threshold.ms=30000
dfs.client.use.datanode.hostname=false
dfs.client.use.legacy.blockreader.local=false
dfs.client.write.exclude.nodes.cache.expiry.interval.millis=600000
dfs.datanode.address=0.0.0.0:50010
dfs.datanode.available-space-volume-choosing-policy.balanced-space-preference-fraction=0.75f
dfs.datanode.available-space-volume-choosing-policy.balanced-space-threshold=10737418240
dfs.datanode.balance.bandwidthPerSec=1048576
dfs.datanode.block-pinning.enabled=false
dfs.datanode.block.id.layout.upgrade.threads=12
dfs.datanode.bp-ready.timeout=20
dfs.datanode.cache.revocation.polling.ms=500
dfs.datanode.cache.revocation.timeout.ms=900000
dfs.datanode.data.dir=file:///local_disk0/tmp/dfs/data
dfs.datanode.data.dir.perm=700
dfs.datanode.directoryscan.interval=21600
dfs.datanode.directoryscan.threads=1
dfs.datanode.dns.interface=default
dfs.datanode.dns.nameserver=default
dfs.datanode.drop.cache.behind.reads=false
dfs.datanode.drop.cache.behind.writes=false
dfs.datanode.du.reserved=0
dfs.datanode.failed.volumes.tolerated=0
dfs.datanode.fsdatasetcache.max.threads.per.volume=4
dfs.datanode.handler.count=10
dfs.datanode.hdfs-blocks-metadata.enabled=false
dfs.datanode.http.address=0.0.0.0:50075
dfs.datanode.https.address=0.0.0.0:50475
dfs.datanode.ipc.address=0.0.0.0:50020
dfs.datanode.max.locked.memory=0
dfs.datanode.max.transfer.threads=4096
dfs.datanode.readahead.bytes=4194304
dfs.datanode.scan.period.hours=504
dfs.datanode.shared.file.descriptor.paths=
    /dev/shm,/tmp:
dfs.datanode.slow.io.warning.threshold.ms=300
dfs.datanode.sync.behind.writes=false
dfs.datanode.use.datanode.hostname=false
dfs.default.chunk.view.size=32768
dfs.encrypt.data.transfer=false
dfs.encrypt.data.transfer.cipher.key.bitlength=128
dfs.ha.automatic-failover.enabled=false
dfs.ha.fencing.ssh.connect-timeout=30000
dfs.ha.log-roll.period=120
dfs.ha.tail-edits.period=60
dfs.heartbeat.interval=3
dfs.http.policy=HTTP_ONLY
dfs.https.server.keystore.resource=ssl-server.xml
dfs.image.compress=false
dfs.image.compression.codec=org.apache.hadoop.io.compress.DefaultCodec
dfs.image.transfer.bandwidthPerSec=0
dfs.image.transfer.chunksize=65536
dfs.image.transfer.timeout=60000
dfs.journalnode.http-address=0.0.0.0:8480
dfs.journalnode.https-address=0.0.0.0:8481
dfs.journalnode.rpc-address=0.0.0.0:8485
dfs.namenode.accesstime.precision=3600000
dfs.namenode.acls.enabled=false
dfs.namenode.audit.loggers=default
dfs.namenode.avoid.read.stale.datanode=false
dfs.namenode.avoid.write.stale.datanode=false
dfs.namenode.backup.address=0.0.0.0:50100
dfs.namenode.backup.http-address=0.0.0.0:50105
dfs.namenode.block-placement-policy.default.prefer-local-node=true
dfs.namenode.blocks.per.postponedblocks.rescan=10000
dfs.namenode.checkpoint.check.period=60
dfs.namenode.checkpoint.dir=file:///local_disk0/tmp/dfs/namesecondary
dfs.namenode.checkpoint.edits.dir=file:///local_disk0/tmp/dfs/namesecondary
dfs.namenode.checkpoint.max-retries=3
dfs.namenode.checkpoint.period=3600
dfs.namenode.checkpoint.txns=1000000
dfs.namenode.datanode.registration.ip-hostname-check=true
dfs.namenode.decommission.blocks.per.interval=500000
dfs.namenode.decommission.interval=30
dfs.namenode.decommission.max.concurrent.tracked.nodes=100
dfs.namenode.delegation.key.update-interval=86400000
dfs.namenode.delegation.token.max-lifetime=604800000
dfs.namenode.delegation.token.renew-interval=86400000
dfs.namenode.edit.log.autoroll.check.interval.ms=300000
dfs.namenode.edit.log.autoroll.multiplier.threshold=2.0
dfs.namenode.edits.dir=file:///local_disk0/tmp/dfs/name
dfs.namenode.edits.journal-plugin.qjournal=org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager
dfs.namenode.edits.noeditlogchannelflush=false
dfs.namenode.enable.retrycache=true
dfs.namenode.fs-limits.max-blocks-per-file=1048576
dfs.namenode.fs-limits.max-component-length=255
dfs.namenode.fs-limits.max-directory-items=1048576
dfs.namenode.fs-limits.max-xattr-size=16384
dfs.namenode.fs-limits.max-xattrs-per-inode=32
dfs.namenode.fs-limits.min-block-size=1048576
dfs.namenode.handler.count=10
dfs.namenode.heartbeat.recheck-interval=300000
dfs.namenode.http-address=0.0.0.0:50070
dfs.namenode.https-address=0.0.0.0:50470
dfs.namenode.inotify.max.events.per.rpc=1000
dfs.namenode.invalidate.work.pct.per.iteration=0.32f
dfs.namenode.kerberos.internal.spnego.principal=${dfs.web.authentication.kerberos.principal}
dfs.namenode.kerberos.principal.pattern=*
dfs.namenode.lazypersist.file.scrub.interval.sec=300
dfs.namenode.list.cache.directives.num.responses=100
dfs.namenode.list.cache.pools.num.responses=100
dfs.namenode.list.encryption.zones.num.responses=100
dfs.namenode.max.extra.edits.segments.retained=10000
dfs.namenode.max.objects=0
dfs.namenode.name.dir=file:///local_disk0/tmp/dfs/name
dfs.namenode.name.dir.restore=false
dfs.namenode.num.checkpoints.retained=2
dfs.namenode.num.extra.edits.retained=1000000
dfs.namenode.path.based.cache.block.map.allocation.percent=
    0.25:
dfs.namenode.path.based.cache.refresh.interval.ms=
    30000:
dfs.namenode.path.based.cache.retry.interval.ms=
    30000:
dfs.namenode.reject-unresolved-dn-topology-mapping=false
dfs.namenode.replication.considerLoad=true
dfs.namenode.replication.interval=3
dfs.namenode.replication.min=1
dfs.namenode.replication.work.multiplier.per.iteration=2
dfs.namenode.resource.check.interval=5000
dfs.namenode.resource.checked.volumes.minimum=1
dfs.namenode.resource.du.reserved=104857600
dfs.namenode.retrycache.expirytime.millis=600000
dfs.namenode.retrycache.heap.percent=0.03f
dfs.namenode.safemode.extension=30000
dfs.namenode.safemode.min.datanodes=0
dfs.namenode.safemode.threshold-pct=0.999f
dfs.namenode.secondary.http-address=0.0.0.0:50090
dfs.namenode.secondary.https-address=0.0.0.0:50091
dfs.namenode.stale.datanode.interval=30000
dfs.namenode.startup.delay.block.deletion.sec=0
dfs.namenode.support.allow.format=true
dfs.namenode.top.enabled=true
dfs.namenode.top.num.users=10
dfs.namenode.top.window.num.buckets=10
dfs.namenode.top.windows.minutes=1,5,25
dfs.namenode.write.stale.datanode.ratio=0.5f
dfs.namenode.xattrs.enabled=true
dfs.permissions.enabled=true
dfs.permissions.superusergroup=supergroup
dfs.replication=3
dfs.replication.max=512
dfs.secondary.namenode.kerberos.internal.spnego.principal=${dfs.web.authentication.kerberos.principal}
dfs.short.circuit.shared.memory.watcher.interrupt.check.ms=60000
dfs.storage.policy.enabled=true
dfs.stream-buffer-size=4096
dfs.user.home.dir.prefix=/user
dfs.webhdfs.enabled=true
dfs.webhdfs.user.provider.user.pattern=^[A-Za-z_][A-Za-z0-9._-]*[$]?$
eventLog.rolloverIntervalSeconds=3600
file.blocksize=67108864
file.bytes-per-checksum=512
file.client-write-packet-size=65536
file.replication=1
file.stream-buffer-size=4096
fs.AbstractFileSystem.file.impl=org.apache.hadoop.fs.local.LocalFs
fs.AbstractFileSystem.ftp.impl=org.apache.hadoop.fs.ftp.FtpFs
fs.AbstractFileSystem.har.impl=org.apache.hadoop.fs.HarFs
fs.AbstractFileSystem.hdfs.impl=org.apache.hadoop.fs.Hdfs
fs.AbstractFileSystem.viewfs.impl=org.apache.hadoop.fs.viewfs.ViewFs
fs.abfs.impl=shaded.databricks.v20180920_b33d810.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem
fs.abfs.impl.disable.cache=true
fs.abfss.impl=shaded.databricks.v20180920_b33d810.org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem
fs.abfss.impl.disable.cache=true
fs.adl.impl=com.databricks.adl.AdlFileSystem
fs.adl.impl.disable.cache=false
fs.automatic.close=true
fs.azure.account.key.teuatsa.blob.core.windows.net=IsrxMnnzLeO9N44beU4eMw0LaiKJdi1hHmKZFUiPa8nFNKZAFyhH0Hv/UCoC4qhgcdq63zwTRGFhUKV9d4ZsBw==
fs.azure.skip.metrics=true
fs.client.resolve.remote.symlinks=true
fs.dbfs.impl=com.databricks.backend.daemon.data.client.DBFS
fs.dbfs2.impl=com.databricks.backend.daemon.data.client.DBFSV2
fs.dbfsartifacts.impl=com.databricks.backend.daemon.data.client.DBFSV1
fs.dbs3n.impl=com.databricks.backend.daemon.data.client.DBS3N
fs.dbtachyon.impl=com.databricks.backend.daemon.data.client.DBTACHYON
fs.defaultFS=dbfs:///
fs.df.interval=60000
fs.du.interval=600000
fs.ftp.host=0.0.0.0
fs.ftp.host.port=21
fs.har.impl=org.apache.hadoop.hive.shims.HiveHarFileSystem
fs.har.impl.disable.cache=true
fs.permissions.umask-mode=022
fs.s3.block.size=67108864
fs.s3.buffer.dir=/local_disk0/tmp/s3
fs.s3.impl=com.databricks.s3a.S3AFileSystem
fs.s3.maxRetries=4
fs.s3.sleepTimeSeconds=10
fs.s3a.attempts.maximum=10
fs.s3a.buffer.dir=/local_disk0/tmp/s3a
fs.s3a.connection.establish.timeout=5000
fs.s3a.connection.maximum=200
fs.s3a.connection.ssl.enabled=true
fs.s3a.connection.timeout=50000
fs.s3a.fast.buffer.size=1048576
fs.s3a.fast.upload=true
fs.s3a.fast.upload.default=true
fs.s3a.impl=com.databricks.s3a.S3AFileSystem
fs.s3a.max.total.tasks=1000
fs.s3a.multipart.purge=false
fs.s3a.multipart.purge.age=86400
fs.s3a.multipart.size=10485760
fs.s3a.multipart.threshold=104857600
fs.s3a.paging.maximum=5000
fs.s3a.threads.core=15
fs.s3a.threads.keepalivetime=60
fs.s3a.threads.max=136
fs.s3araw.impl=com.databricks.s3a.S3AFileSystem
fs.s3n.block.size=67108864
fs.s3n.impl=com.databricks.s3a.S3AFileSystem
fs.s3n.multipart.copy.block.size=5368709120
fs.s3n.multipart.uploads.block.size=67108864
fs.s3n.multipart.uploads.enabled=false
fs.s3raw.impl=com.databricks.s3.StableNativeS3FileSystem
fs.scheme.class=dfs
fs.swift.impl=org.apache.hadoop.fs.swift.snative.SwiftNativeFileSystem
fs.tachyon.impl=tachyon.hadoop.TFS
fs.trash.checkpoint.interval=0
fs.trash.interval=0
fs.wasb.impl=shaded.databricks.org.apache.hadoop.fs.azure.NativeAzureFileSystem
fs.wasb.impl.disable.cache=true
fs.wasbs.impl=shaded.databricks.org.apache.hadoop.fs.azure.NativeAzureFileSystem
fs.wasbs.impl.disable.cache=true
ftp.blocksize=67108864
ftp.bytes-per-checksum=512
ftp.client-write-packet-size=65536
ftp.replication=3
ftp.stream-buffer-size=4096
ha.failover-controller.cli-check.rpc-timeout.ms=20000
ha.failover-controller.graceful-fence.connection.retries=1
ha.failover-controller.graceful-fence.rpc-timeout.ms=5000
ha.failover-controller.new-active.rpc-timeout.ms=60000
ha.health-monitor.check-interval.ms=1000
ha.health-monitor.connect-retry-interval.ms=1000
ha.health-monitor.rpc-timeout.ms=45000
ha.health-monitor.sleep-after-disconnect.ms=1000
ha.zookeeper.acl=world:anyone:rwcda
ha.zookeeper.parent-znode=/hadoop-ha
ha.zookeeper.session-timeout.ms=5000
hadoop.bin.path=
    /usr/bin/hadoop:
hadoop.common.configuration.version=0.23.0
hadoop.fuse.connection.timeout=300
hadoop.fuse.timer.period=5
hadoop.hdfs.configuration.version=1
hadoop.http.authentication.kerberos.keytab=/root/hadoop.keytab
hadoop.http.authentication.kerberos.principal=HTTP/_HOST@LOCALHOST
hadoop.http.authentication.signature.secret.file=/root/hadoop-http-auth-signature-secret
hadoop.http.authentication.simple.anonymous.allowed=true
hadoop.http.authentication.token.validity=36000
hadoop.http.authentication.type=simple
hadoop.http.cross-origin.allowed-headers=X-Requested-With,Content-Type,Accept,Origin
hadoop.http.cross-origin.allowed-methods=GET,POST,HEAD
hadoop.http.cross-origin.allowed-origins=*
hadoop.http.cross-origin.enabled=false
hadoop.http.cross-origin.max-age=1800
hadoop.http.filter.initializers=org.apache.hadoop.http.lib.StaticUserWebFilter
hadoop.http.staticuser.user=dr.who
hadoop.jetty.logs.serve.aliases=true
hadoop.kerberos.kinit.command=kinit
hadoop.registry.jaas.context=Client
hadoop.registry.rm.enabled=false
hadoop.registry.secure=false
hadoop.registry.system.acls=sasl:yarn@, sasl:mapred@, sasl:hdfs@
hadoop.registry.zk.connection.timeout.ms=15000
hadoop.registry.zk.quorum=localhost:2181
hadoop.registry.zk.retry.ceiling.ms=60000
hadoop.registry.zk.retry.interval.ms=1000
hadoop.registry.zk.retry.times=5
hadoop.registry.zk.root=/registry
hadoop.registry.zk.session.timeout.ms=60000
hadoop.rpc.protection=authentication
hadoop.rpc.socket.factory.class.default=org.apache.hadoop.net.StandardSocketFactory
hadoop.security.authentication=simple
hadoop.security.authorization=false
hadoop.security.crypto.buffer.size=8192
hadoop.security.crypto.cipher.suite=AES/CTR/NoPadding
hadoop.security.crypto.codec.classes.aes.ctr.nopadding=org.apache.hadoop.crypto.OpensslAesCtrCryptoCodec,org.apache.hadoop.crypto.JceAesCtrCryptoCodec
hadoop.security.group.mapping=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback
hadoop.security.group.mapping.ldap.directory.search.timeout=10000
hadoop.security.group.mapping.ldap.search.attr.group.name=cn
hadoop.security.group.mapping.ldap.search.attr.member=member
hadoop.security.group.mapping.ldap.search.filter.group=(objectClass=group)
hadoop.security.group.mapping.ldap.search.filter.user=(&(objectClass=user)(sAMAccountName={0}))
hadoop.security.group.mapping.ldap.ssl=false
hadoop.security.groups.cache.secs=300
hadoop.security.groups.cache.warn.after.ms=5000
hadoop.security.groups.negative-cache.secs=30
hadoop.security.instrumentation.requires.admin=false
hadoop.security.java.secure.random.algorithm=SHA1PRNG
hadoop.security.kms.client.authentication.retry-count=1
hadoop.security.kms.client.encrypted.key.cache.expiry=43200000
hadoop.security.kms.client.encrypted.key.cache.low-watermark=0.3f
hadoop.security.kms.client.encrypted.key.cache.num.refill.threads=2
hadoop.security.kms.client.encrypted.key.cache.size=500
hadoop.security.random.device.file.path=
    /dev/urandom:
hadoop.security.uid.cache.secs=14400
hadoop.ssl.client.conf=ssl-client.xml
hadoop.ssl.enabled=false
hadoop.ssl.enabled.protocols=TLSv1
hadoop.ssl.hostname.verifier=DEFAULT
hadoop.ssl.keystores.factory.class=org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory
hadoop.ssl.require.client.cert=false
hadoop.ssl.server.conf=ssl-server.xml
hadoop.tmp.dir=/local_disk0/tmp
hadoop.user.group.static.mapping.overrides=dr.who=;
hadoop.util.hash.type=murmur
hadoop.work.around.non.threadsafe.getpwuid=false
hive.allow.udf.load.on.demand=false
hive.analyze.stmt.collect.partlevel.stats=true
hive.archive.enabled=false
hive.async.log.enabled=true
hive.ats.hook.queue.capacity=64
hive.auto.convert.join=true
hive.auto.convert.join.hashtable.max.entries=40000000
hive.auto.convert.join.noconditionaltask=true
hive.auto.convert.join.noconditionaltask.size=10000000
hive.auto.convert.join.use.nonstaged=false
hive.auto.convert.sortmerge.join=false
hive.auto.convert.sortmerge.join.bigtable.selection.policy=org.apache.hadoop.hive.ql.optimizer.AvgPartitionSizeBasedBigTableSelectorForAutoSMJ
hive.auto.convert.sortmerge.join.reduce.side=true
hive.auto.convert.sortmerge.join.to.mapjoin=false
hive.auto.progress.timeout=0
hive.autogen.columnalias.prefix.includefuncname=false
hive.autogen.columnalias.prefix.label=_c
hive.binary.record.max.length=1000
hive.blobstore.optimizations.enabled=true
hive.blobstore.supported.schemes=s3,s3a,s3n
hive.blobstore.use.blobstore.as.scratchdir=false
hive.cache.expr.evaluation=true
hive.cbo.cnf.maxnodes=-1
hive.cbo.costmodel.cpu=0.000001
hive.cbo.costmodel.extended=false
hive.cbo.costmodel.hdfs.read=1.5
hive.cbo.costmodel.hdfs.write=10.0
hive.cbo.costmodel.local.fs.read=4.0
hive.cbo.costmodel.local.fs.write=4.0
hive.cbo.costmodel.network=150.0
hive.cbo.enable=true
hive.cbo.returnpath.hiveop=
    false:
hive.cbo.show.warnings=true
hive.cli.errors.ignore=false
hive.cli.pretty.output.num.cols=-1
hive.cli.print.current.db=false
hive.cli.print.header=false
hive.cli.prompt=hive
hive.cli.tez.session.async=true
hive.cluster.delegation.token.store.class=org.apache.hadoop.hive.thrift.MemoryTokenStore
hive.cluster.delegation.token.store.zookeeper.znode=/hivedelegation
hive.compactor.abortedtxn.threshold=1000
hive.compactor.check.interval=300
hive.compactor.cleaner.run.interval=5000
hive.compactor.delta.num.threshold=10
hive.compactor.delta.pct.threshold=0.1
hive.compactor.history.reaper.interval=2m
hive.compactor.history.retention.attempted=2
hive.compactor.history.retention.failed=3
hive.compactor.history.retention.succeeded=3
hive.compactor.initiator.failed.compacts.threshold=2
hive.compactor.initiator.on=false
hive.compactor.max.num.delta=500
hive.compactor.worker.threads=0
hive.compactor.worker.timeout=86400
hive.compat=0.12
hive.compute.query.using.stats=true
hive.compute.splits.in.am=true
hive.conf.hidden.list=javax.jdo.option.ConnectionPassword,hive.server2.keystore.password,fs.s3.awsAccessKeyId,fs.s3.awsSecretAccessKey,fs.s3n.awsAccessKeyId,fs.s3n.awsSecretAccessKey,fs.s3a.access.key,fs.s3a.secret.key,fs.s3a.proxy.password
hive.conf.internal.variable.list=hive.added.files.path,hive.added.jars.path,hive.added.archives.path
hive.conf.restricted.list=hive.security.authenticator.manager,hive.security.authorization.manager,hive.security.metastore.authorization.manager,hive.security.metastore.authenticator.manager,hive.users.in.admin.role,hive.server2.xsrf.filter.enabled,hive.security.authorization.enabled,hive.server2.authentication.ldap.baseDN,hive.server2.authentication.ldap.url,hive.server2.authentication.ldap.Domain,hive.server2.authentication.ldap.groupDNPattern,hive.server2.authentication.ldap.groupFilter,hive.server2.authentication.ldap.userDNPattern,hive.server2.authentication.ldap.userFilter,hive.server2.authentication.ldap.groupMembershipKey,hive.server2.authentication.ldap.userMembershipKey,hive.server2.authentication.ldap.groupClassKey,hive.server2.authentication.ldap.customLDAPQuery
hive.conf.validation=true
hive.convert.join.bucket.mapjoin.tez=false
hive.count.open.txns.interval=1s
hive.counters.group.name=HIVE
hive.debug.localtask=false
hive.decode.partition.name=false
hive.default.fileformat=TextFile
hive.default.fileformat.managed=none
hive.default.rcfile.serde=org.apache.hadoop.hive.serde2.columnar.LazyBinaryColumnarSerDe
hive.default.serde=org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
hive.direct.sql.max.elements.in.clause=1000
hive.direct.sql.max.elements.values.clause=1000
hive.direct.sql.max.query.length=100
hive.display.partition.cols.separately=true
hive.downloaded.resources.dir=/local_disk0/tmp/5299c531-93ff-4c80-b9a4-d52a5bb3de16_resources
hive.driver.parallel.compilation=false
hive.druid.broker.address.default=localhost:8082
hive.druid.coordinator.address.default=localhost:8081
hive.druid.http.numConnection=20
hive.druid.http.read.timeout=PT1M
hive.druid.indexer.memory.rownum.max=75000
hive.druid.indexer.partition.size.max=5000000
hive.druid.indexer.segments.granularity=DAY
hive.druid.maxTries=5
hive.druid.metadata.base=druid
hive.druid.metadata.db.type=mysql
hive.druid.passiveWaitTimeMs=30000
hive.druid.select.distribute=true
hive.druid.select.threshold=10000
hive.druid.sleep.time=PT10S
hive.druid.storage.storageDirectory=/druid/segments
hive.druid.working.directory=/tmp/workingDirectory
hive.enforce.bucketmapjoin=false
hive.enforce.sortmergebucketmapjoin=false
hive.entity.capture.transform=false
hive.entity.separator=@
hive.error.on.empty.partition=false
hive.exec.check.crossproducts=true
hive.exec.compress.intermediate=false
hive.exec.compress.output=false
hive.exec.concatenate.check.index=true
hive.exec.copyfile.maxnumfiles=1
hive.exec.copyfile.maxsize=33554432
hive.exec.counters.pull.interval=1000
hive.exec.default.partition.name=__HIVE_DEFAULT_PARTITION__
hive.exec.drop.ignorenonexistent=true
hive.exec.dynamic.partition=true
hive.exec.dynamic.partition.mode=strict
hive.exec.infer.bucket.sort=false
hive.exec.infer.bucket.sort.num.buckets.power.two=false
hive.exec.input.listing.max.threads=0
hive.exec.job.debug.capture.stacktraces=true
hive.exec.job.debug.timeout=30000
hive.exec.local.scratchdir=/local_disk0/tmp/root
hive.exec.max.created.files=100000
hive.exec.max.dynamic.partitions=1000
hive.exec.max.dynamic.partitions.pernode=100
hive.exec.mode.local.auto=false
hive.exec.mode.local.auto.input.files.max=4
hive.exec.mode.local.auto.inputbytes.max=134217728
hive.exec.orc.base.delta.ratio=8
hive.exec.orc.split.strategy=HYBRID
hive.exec.parallel=false
hive.exec.parallel.thread.number=8
hive.exec.perf.logger=org.apache.hadoop.hive.ql.log.PerfLogger
hive.exec.rcfile.use.explicit.header=true
hive.exec.rcfile.use.sync.cache=true
hive.exec.reducers.bytes.per.reducer=256000000
hive.exec.reducers.max=1009
hive.exec.rowoffset=false
hive.exec.schema.evolution=true
hive.exec.scratchdir=/tmp/hive
hive.exec.script.allow.partial.consumption=false
hive.exec.script.maxerrsize=100000
hive.exec.script.trust=false
hive.exec.show.job.failure.debug.info=true
hive.exec.stagingdir=.hive-staging
hive.exec.submit.local.task.via.child=true
hive.exec.submitviachild=false
hive.exec.tasklog.debug.timeout=20000
hive.exec.temporary.table.storage=default
hive.execution.engine=mr
hive.execution.mode=container
hive.exim.strict.repl.tables=true
hive.exim.uri.scheme.whitelist=hdfs,pfile,file,s3,s3a
hive.explain.dependency.append.tasktype=false
hive.explain.user=true
hive.fetch.output.serde=org.apache.hadoop.hive.serde2.DelimitedJSONSerDe
hive.fetch.task.aggr=false
hive.fetch.task.conversion=more
hive.fetch.task.conversion.threshold=1073741824
hive.file.max.footer=100
hive.fileformat.check=true
hive.groupby.limit.extrastep=true
hive.groupby.mapaggr.checkinterval=100000
hive.groupby.orderby.position.alias=false
hive.groupby.position.alias=false
hive.groupby.skewindata=false
hive.hash.table.inflation.factor=2.0
hive.hashtable.initialCapacity=100000
hive.hashtable.key.count.adjustment=1.0
hive.hashtable.loadfactor=0.75
hive.hbase.generatehfiles=false
hive.hbase.snapshot.restoredir=/tmp
hive.hbase.wal.enabled=true
hive.heartbeat.interval=1000
hive.hmshandler.force.reload.conf=false
hive.hmshandler.retry.attempts=10
hive.hmshandler.retry.interval=2000
hive.ignore.mapjoin.hint=true
hive.in.test=false
hive.in.tez.test=false
hive.index.compact.binary.search=true
hive.index.compact.file.ignore.hdfs=false
hive.index.compact.query.max.entries=10000000
hive.index.compact.query.max.size=10737418240
hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat
hive.insert.into.external.tables=true
hive.insert.into.multilevel.dirs=false
hive.int.timestamp.conversion.in.seconds=false
hive.internal.ss.authz.settings.applied.marker=true
hive.io.rcfile.column.number.conf=0
hive.io.rcfile.record.buffer.size=4194304
hive.io.rcfile.record.interval=2147483647
hive.io.rcfile.tolerate.corruptions=false
hive.io.sarg.cache.max.weight.mb=10
hive.jobname.length=50
hive.join.cache.size=25000
hive.join.emit.interval=1000
hive.lazysimple.extended_boolean_literal=false
hive.limit.optimize.enable=false
hive.limit.optimize.fetch.max=50000
hive.limit.optimize.limit.file=10
hive.limit.pushdown.memory.usage=0.1
hive.limit.query.max.table.partition=-1
hive.limit.row.max.size=100000
hive.llap.allow.permanent.fns=true
hive.llap.am.liveness.connection.sleep.between.retries.ms=2000ms
hive.llap.am.liveness.connection.timeout.ms=10000ms
hive.llap.am.use.fqdn=false
hive.llap.auto.allow.uber=false
hive.llap.auto.auth=false
hive.llap.auto.enforce.stats=true
hive.llap.auto.enforce.tree=true
hive.llap.auto.enforce.vectorized=true
hive.llap.auto.max.input.size=10737418240
hive.llap.auto.max.output.size=1073741824
hive.llap.cache.allow.synthetic.fileid=false
hive.llap.client.consistent.splits=false
hive.llap.daemon.acl=*
hive.llap.daemon.am-reporter.max.threads=4
hive.llap.daemon.am.liveness.heartbeat.interval.ms=10000ms
hive.llap.daemon.communicator.num.threads=10
hive.llap.daemon.delegation.token.lifetime=14d
hive.llap.daemon.download.permanent.fns=false
hive.llap.daemon.logger=query-routing
hive.llap.daemon.memory.per.instance.mb=4096
hive.llap.daemon.num.executors=4
hive.llap.daemon.num.file.cleaner.threads=1
hive.llap.daemon.output.service.max.pending.writes=8
hive.llap.daemon.output.service.port=15003
hive.llap.daemon.output.service.send.buffer.size=131072
hive.llap.daemon.output.stream.timeout=120s
hive.llap.daemon.rpc.num.handlers=5
hive.llap.daemon.rpc.port=0
hive.llap.daemon.service.refresh.interval.sec=60s
hive.llap.daemon.shuffle.dir.watcher.enabled=false
hive.llap.daemon.task.preemption.metrics.intervals=30,60,300
hive.llap.daemon.task.scheduler.enable.preemption=true
hive.llap.daemon.task.scheduler.wait.queue.size=10
hive.llap.daemon.vcpus.per.instance=4
hive.llap.daemon.wait.queue.comparator.class.name=org.apache.hadoop.hive.llap.daemon.impl.comparator.ShortestJobFirstComparator
hive.llap.daemon.web.port=15002
hive.llap.daemon.web.ssl=false
hive.llap.daemon.xmx.headroom=5%
hive.llap.daemon.yarn.container.mb=-1
hive.llap.daemon.yarn.shuffle.port=15551
hive.llap.enable.grace.join.in.llap=false
hive.llap.execution.mode=none
hive.llap.file.cleanup.delay.seconds=300s
hive.llap.hs2.coordinator.enabled=true
hive.llap.io.allocator.alloc.max=16Mb
hive.llap.io.allocator.alloc.min=256Kb
hive.llap.io.allocator.arena.count=8
hive.llap.io.allocator.direct=true
hive.llap.io.allocator.mmap=false
hive.llap.io.allocator.mmap.path=
    /tmp:
hive.llap.io.decoding.metrics.percentiles.intervals=30
hive.llap.io.encode.alloc.size=256Kb
hive.llap.io.encode.enabled=true
hive.llap.io.encode.formats=org.apache.hadoop.mapred.TextInputFormat,
hive.llap.io.encode.slice.lrr=true
hive.llap.io.encode.slice.row.count=100000
hive.llap.io.encode.vector.serde.async.enabled=true
hive.llap.io.encode.vector.serde.enabled=true
hive.llap.io.lrfu.lambda=0.01
hive.llap.io.memory.mode=cache
hive.llap.io.memory.size=1Gb
hive.llap.io.metadata.fraction=0.1
hive.llap.io.nonvector.wrapper.enabled=true
hive.llap.io.orc.time.counters=true
hive.llap.io.threadpool.size=10
hive.llap.io.use.fileid.path=
    true:
hive.llap.io.use.lrfu=true
hive.llap.management.acl=*
hive.llap.management.rpc.port=15004
hive.llap.object.cache.enabled=true
hive.llap.orc.gap.cache=true
hive.llap.remote.token.requires.signing=true
hive.llap.skip.compile.udf.check=false
hive.llap.task.communicator.connection.sleep.between.retries.ms=2000ms
hive.llap.task.communicator.connection.timeout.ms=16000ms
hive.llap.task.communicator.listener.thread-count=30
hive.llap.task.scheduler.locality.delay=0ms
hive.llap.task.scheduler.node.disable.backoff.factor=1.5
hive.llap.task.scheduler.node.reenable.max.timeout.ms=10000ms
hive.llap.task.scheduler.node.reenable.min.timeout.ms=200ms
hive.llap.task.scheduler.num.schedulable.tasks.per.node=0
hive.llap.task.scheduler.timeout.seconds=60s
hive.llap.validate.acls=true
hive.load.dynamic.partitions.thread=15
hive.localize.resource.num.wait.attempts=5
hive.localize.resource.wait.interval=5000
hive.lock.manager=org.apache.hadoop.hive.ql.lockmgr.zookeeper.ZooKeeperHiveLockManager
hive.lock.mapred.only.operation=false
hive.lock.numretries=100
hive.lock.sleep.between.retries=60
hive.lockmgr.zookeeper.default.partition.name=__HIVE_DEFAULT_ZOOKEEPER_PARTITION__
hive.log.every.n.records=0
hive.log.explain.output=false
hive.map.aggr=true
hive.map.aggr.hash.force.flush.memory.threshold=0.9
hive.map.aggr.hash.min.reduction=0.5
hive.map.aggr.hash.percentmemory=0.5
hive.map.groupby.sorted=true
hive.mapjoin.bucket.cache.size=100
hive.mapjoin.check.memory.rows=100000
hive.mapjoin.followby.gby.localtask.max.memory.usage=0.55
hive.mapjoin.followby.map.aggr.hash.percentmemory=0.3
hive.mapjoin.hybridgrace.bloomfilter=true
hive.mapjoin.hybridgrace.hashtable=true
hive.mapjoin.hybridgrace.memcheckfrequency=1024
hive.mapjoin.hybridgrace.minnumpartitions=16
hive.mapjoin.hybridgrace.minwbsize=524288
hive.mapjoin.localtask.max.memory.usage=0.9
hive.mapjoin.optimized.hashtable=true
hive.mapjoin.optimized.hashtable.probe.percent=0.5
hive.mapjoin.optimized.hashtable.wbsize=8388608
hive.mapjoin.smalltable.filesize=25000000
hive.mapper.cannot.span.multiple.partitions=false
hive.mapred.local.mem=0
hive.mapred.partitioner=org.apache.hadoop.hive.ql.io.DefaultHivePartitioner
hive.mapred.reduce.tasks.speculative.execution=true
hive.materializedview.fileformat=ORC
hive.materializedview.rewriting=false
hive.materializedview.serde=org.apache.hadoop.hive.ql.io.orc.OrcSerde
hive.max.open.txns=100000
hive.merge.cardinality.check=true
hive.merge.mapfiles=true
hive.merge.mapredfiles=false
hive.merge.nway.joins=true
hive.merge.orcfile.stripe.level=true
hive.merge.rcfile.block.level=true
hive.merge.size.per.task=256000000
hive.merge.smallfiles.avgsize=16000000
hive.merge.sparkfiles=false
hive.merge.tezfiles=false
hive.metadata.move.exported.metadata.to.trash=true
hive.metastore.aggregate.stats.cache.clean.until=0.8
hive.metastore.aggregate.stats.cache.enabled=true
hive.metastore.aggregate.stats.cache.fpp=0.01
hive.metastore.aggregate.stats.cache.max.full=0.9
hive.metastore.aggregate.stats.cache.max.partitions=10000
hive.metastore.aggregate.stats.cache.max.reader.wait=1000
hive.metastore.aggregate.stats.cache.max.variance=0.01
hive.metastore.aggregate.stats.cache.max.writer.wait=5000
hive.metastore.aggregate.stats.cache.size=10000
hive.metastore.aggregate.stats.cache.ttl=600
hive.metastore.archive.intermediate.archived=_INTERMEDIATE_ARCHIVED
hive.metastore.archive.intermediate.extracted=_INTERMEDIATE_EXTRACTED
hive.metastore.archive.intermediate.original=_INTERMEDIATE_ORIGINAL
hive.metastore.authorization.storage.check.externaltable.drop=true
hive.metastore.authorization.storage.checks=false
hive.metastore.batch.retrieve.max=300
hive.metastore.batch.retrieve.table.partition.max=1000
hive.metastore.cache.pinobjtypes=Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order
hive.metastore.client.capability.check=true
hive.metastore.client.connect.retry.delay=10
hive.metastore.client.drop.partitions.using.expressions=true
hive.metastore.client.socket.lifetime=0
hive.metastore.client.socket.timeout=3600
hive.metastore.connect.retries=3
hive.metastore.direct.sql.batch.size=0
hive.metastore.disallow.incompatible.col.type.changes=true
hive.metastore.dml.events=false
hive.metastore.event.clean.freq=0
hive.metastore.event.db.listener.timetolive=86400
hive.metastore.event.expiry.duration=0
hive.metastore.event.message.factory=org.apache.hadoop.hive.metastore.messaging.json.JSONMessageFactory
hive.metastore.execute.setugi=true
hive.metastore.expression.proxy=org.apache.hadoop.hive.ql.optimizer.ppr.PartitionExpressionForMetastore
hive.metastore.failure.retries=30
hive.metastore.fastpath=
    false:
hive.metastore.filter.hook=org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
hive.metastore.fs.handler.class=org.apache.hadoop.hive.metastore.HiveMetaStoreFsImpl
hive.metastore.fshandler.threads=15
hive.metastore.hbase.aggr.stats.cache.entries=10000
hive.metastore.hbase.aggr.stats.hbase.ttl=604800s
hive.metastore.hbase.aggr.stats.invalidator.frequency=5s
hive.metastore.hbase.aggr.stats.memory.ttl=60s
hive.metastore.hbase.aggregate.stats.cache.size=10000
hive.metastore.hbase.aggregate.stats.false.positive.probability=0.01
hive.metastore.hbase.aggregate.stats.max.partitions=10000
hive.metastore.hbase.aggregate.stats.max.variance=0.1
hive.metastore.hbase.cache.clean.until=0.8
hive.metastore.hbase.cache.max.full=0.9
hive.metastore.hbase.cache.max.reader.wait=1000ms
hive.metastore.hbase.cache.max.writer.wait=5000ms
hive.metastore.hbase.cache.ttl=600s
hive.metastore.hbase.catalog.cache.size=50000
hive.metastore.hbase.connection.class=org.apache.hadoop.hive.metastore.hbase.VanillaHBaseConnection
hive.metastore.hbase.file.metadata.threads=1
hive.metastore.initial.metadata.count.enabled=true
hive.metastore.integral.jdo.pushdown=false
hive.metastore.kerberos.principal=hive-metastore/_HOST@EXAMPLE.COM
hive.metastore.limit.partition.request=-1
hive.metastore.metrics.enabled=false
hive.metastore.orm.retrieveMapNullsAsEmptyStrings=false
hive.metastore.port=9083
hive.metastore.rawstore.impl=org.apache.hadoop.hive.metastore.ObjectStore
hive.metastore.sasl.enabled=false
hive.metastore.schema.verification=true
hive.metastore.schema.verification.record.version=false
hive.metastore.server.max.message.size=104857600
hive.metastore.server.max.threads=1000
hive.metastore.server.min.threads=200
hive.metastore.server.tcp.keepalive=true
hive.metastore.stats.ndv.densityfunction=false
hive.metastore.stats.ndv.tuner=0.0
hive.metastore.thrift.compact.protocol.enabled=false
hive.metastore.thrift.framed.transport.enabled=false
hive.metastore.try.direct.sql=true
hive.metastore.try.direct.sql.ddl=true
hive.metastore.txn.store.impl=org.apache.hadoop.hive.metastore.txn.CompactionTxnHandler
hive.metastore.use.SSL=false
hive.metastore.warehouse.dir=adl://connectedsystemsuatdl.azuredatalakestore.net/user/hive/warehouse
hive.msck.path.validation=
    throw:
hive.msck.repair.batch.size=0
hive.multi.insert.move.tasks.share.dependencies=false
hive.multigroupby.singlereducer=true
hive.mv.files.thread=15
hive.new.job.grouping.set.cardinality=30
hive.optimize.bucketingsorting=true
hive.optimize.bucketmapjoin=false
hive.optimize.bucketmapjoin.sortedmerge=false
hive.optimize.constant.propagation=true
hive.optimize.correlation=false
hive.optimize.cte.materialize.threshold=-1
hive.optimize.distinct.rewrite=true
hive.optimize.dynamic.partition.hashjoin=false
hive.optimize.filter.stats.reduction=false
hive.optimize.groupby=true
hive.optimize.index.autoupdate=false
hive.optimize.index.filter=false
hive.optimize.index.filter.compact.maxsize=-1
hive.optimize.index.filter.compact.minsize=5368709120
hive.optimize.index.groupby=false
hive.optimize.limittranspose=false
hive.optimize.limittranspose.reductionpercentage=1.0
hive.optimize.limittranspose.reductiontuples=0
hive.optimize.listbucketing=false
hive.optimize.metadataonly=false
hive.optimize.null.scan=true
hive.optimize.partition.columns.separate=true
hive.optimize.point.lookup=true
hive.optimize.point.lookup.min=31
hive.optimize.ppd=true
hive.optimize.ppd.storage=true
hive.optimize.ppd.windowing=true
hive.optimize.reducededuplication=true
hive.optimize.reducededuplication.min.reducer=4
hive.optimize.remove.identity.project=true
hive.optimize.sampling.orderby=false
hive.optimize.sampling.orderby.number=1000
hive.optimize.sampling.orderby.percent=0.1
hive.optimize.semijoin.conversion=true
hive.optimize.skewjoin=false
hive.optimize.skewjoin.compiletime=false
hive.optimize.sort.dynamic.partition=false
hive.optimize.union.remove=false
hive.orc.cache.stripe.details.mem.size=256Mb
hive.orc.cache.use.soft.references=false
hive.orc.compute.splits.num.threads=10
hive.orc.splits.allow.synthetic.fileid=true
hive.orc.splits.directory.batch.ms=0
hive.orc.splits.include.file.footer=false
hive.orc.splits.include.fileid=true
hive.orc.splits.ms.footer.cache.enabled=false
hive.orc.splits.ms.footer.cache.ppd.enabled=true
hive.order.columnalignment=true
hive.orderby.position.alias=true
hive.parquet.timestamp.skip.conversion=true
hive.ppd.recognizetransivity=true
hive.ppd.remove.duplicatefilters=true
hive.prewarm.enabled=false
hive.prewarm.numcontainers=10
hive.query.result.fileformat=SequenceFile
hive.query.timeout.seconds=0s
hive.querylog.enable.plan.progress=true
hive.querylog.location=/local_disk0/tmp/root
hive.querylog.plan.progress.interval=60000
hive.reorder.nway.joins=true
hive.repl.cm.enabled=false
hive.repl.cm.interval=3600s
hive.repl.cm.retain=24h
hive.repl.cmrootdir=/user/hive/cmroot/
hive.repl.rootdir=/user/hive/repl/
hive.repl.task.factory=org.apache.hive.hcatalog.api.repl.exim.EximReplicationTaskFactory
hive.resultset.use.unique.column.names=true
hive.rework.mapredwork=false
hive.rpc.query.plan=false
hive.sample.seednumber=0
hive.scratch.dir.permission=700
hive.scratchdir.lock=false
hive.script.auto.progress=false
hive.script.operator.env.blacklist=hive.txn.valid.txns,hive.script.operator.env.blacklist
hive.script.operator.id.env.var=HIVE_SCRIPT_OPERATOR_ID
hive.script.operator.truncate.env=false
hive.script.recordreader=org.apache.hadoop.hive.ql.exec.TextRecordReader
hive.script.recordwriter=org.apache.hadoop.hive.ql.exec.TextRecordWriter
hive.script.serde=org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
hive.security.authenticator.manager=org.apache.hadoop.hive.ql.security.HadoopDefaultAuthenticator
hive.security.authorization.createtable.owner.grants=INSERT,SELECT,UPDATE,DELETE
hive.security.authorization.enabled=false
hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactory
hive.security.authorization.sqlstd.confwhitelist=hive\.auto\..*|hive\.cbo\..*|hive\.convert\..*|hive\.exec\.dynamic\.partition.*|hive\.exec\.max\.dynamic\.partitions.*|hive\.exec\.compress\..*|hive\.exec\.infer\..*|hive\.exec\.mode.local\..*|hive\.exec\.orc\..*|hive\.exec\.parallel.*|hive\.explain\..*|hive\.fetch.task\..*|hive\.groupby\..*|hive\.hbase\..*|hive\.index\..*|hive\.index\..*|hive\.intermediate\..*|hive\.join\..*|hive\.limit\..*|hive\.log\..*|hive\.mapjoin\..*|hive\.merge\..*|hive\.optimize\..*|hive\.orc\..*|hive\.outerjoin\..*|hive\.parquet\..*|hive\.ppd\..*|hive\.prewarm\..*|hive\.server2\.thrift\.resultset\.default\.fetch\.size|hive\.server2\.proxy\.user|hive\.skewjoin\..*|hive\.smbjoin\..*|hive\.stats\..*|hive\.strict\..*|hive\.tez\..*|hive\.vectorized\..*|mapred\.map\..*|mapred\.reduce\..*|mapred\.output\.compression\.codec|mapred\.job\.queuename|mapred\.output\.compression\.type|mapred\.min\.split\.size|mapreduce\.job\.reduce\.slowstart\.completedmaps|mapreduce\.job\.queuename|mapreduce\.job\.tags|mapreduce\.input\.fileinputformat\.split\.minsize|mapreduce\.map\..*|mapreduce\.reduce\..*|mapreduce\.output\.fileoutputformat\.compress\.codec|mapreduce\.output\.fileoutputformat\.compress\.type|oozie\..*|tez\.am\..*|tez\.task\..*|tez\.runtime\..*|tez\.queue\.name|hive\.transpose\.aggr\.join|hive\.exec\.reducers\.bytes\.per\.reducer|hive\.client\.stats\.counters|hive\.exec\.default\.partition\.name|hive\.exec\.drop\.ignorenonexistent|hive\.counters\.group\.name|hive\.default\.fileformat\.managed|hive\.enforce\.bucketmapjoin|hive\.enforce\.sortmergebucketmapjoin|hive\.cache\.expr\.evaluation|hive\.query\.result\.fileformat|hive\.hashtable\.loadfactor|hive\.hashtable\.initialCapacity|hive\.ignore\.mapjoin\.hint|hive\.limit\.row\.max\.size|hive\.mapred\.mode|hive\.map\.aggr|hive\.compute\.query\.using\.stats|hive\.exec\.rowoffset|hive\.variable\.substitute|hive\.variable\.substitute\.depth|hive\.autogen\.columnalias\.prefix\.includefuncname|hive\.autogen\.columnalias\.prefix\.label|hive\.exec\.check\.crossproducts|hive\.cli\.tez\.session\.async|hive\.compat|hive\.exec\.concatenate\.check\.index|hive\.display\.partition\.cols\.separately|hive\.error\.on\.empty\.partition|hive\.execution\.engine|hive\.exec\.copyfile\.maxsize|hive\.exim\.uri\.scheme\.whitelist|hive\.file\.max\.footer|hive\.insert\.into\.multilevel\.dirs|hive\.localize\.resource\.num\.wait\.attempts|hive\.multi\.insert\.move\.tasks\.share\.dependencies|hive\.support\.quoted\.identifiers|hive\.resultset\.use\.unique\.column\.names|hive\.analyze\.stmt\.collect\.partlevel\.stats|hive\.exec\.schema\.evolution|hive\.server2\.logging\.operation\.level|hive\.server2\.thrift\.resultset\.serialize\.in\.tasks|hive\.support\.special\.characters\.tablename|hive\.exec\.job\.debug\.capture\.stacktraces|hive\.exec\.job\.debug\.timeout|hive\.llap\.io\.enabled|hive\.llap\.io\.use\.fileid\.path|hive\.llap\.daemon\.service\.hosts|hive\.llap\.execution\.mode|hive\.llap\.auto\.allow\.uber|hive\.llap\.auto\.enforce\.tree|hive\.llap\.auto\.enforce\.vectorized|hive\.llap\.auto\.enforce\.stats|hive\.llap\.auto\.max\.input\.size|hive\.llap\.auto\.max\.output\.size|hive\.llap\.skip\.compile\.udf\.check|hive\.llap\.client\.consistent\.splits|hive\.llap\.enable\.grace\.join\.in\.llap|hive\.llap\.allow\.permanent\.fns|hive\.exec\.max\.created\.files|hive\.exec\.reducers\.max|hive\.reorder\.nway\.joins|hive\.output\.file\.extension|hive\.exec\.show\.job\.failure\.debug\.info|hive\.exec\.tasklog\.debug\.timeout|hive\.query\.id
hive.security.authorization.task.factory=org.apache.hadoop.hive.ql.parse.authorization.HiveAuthorizationTaskFactoryImpl
hive.security.command.whitelist=set,reset,dfs,add,list,delete,reload,compile
hive.security.metastore.authenticator.manager=org.apache.hadoop.hive.ql.security.HadoopDefaultMetastoreAuthenticator
hive.security.metastore.authorization.auth.reads=true
hive.security.metastore.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.DefaultHiveMetastoreAuthorizationProvider
hive.serdes.using.metastore.for.schema=org.apache.hadoop.hive.ql.io.orc.OrcSerde,org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe,org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe,org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDe,org.apache.hadoop.hive.serde2.MetadataTypedColumnsetSerDe,org.apache.hadoop.hive.serde2.columnar.LazyBinaryColumnarSerDe,org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe,org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
hive.server.read.socket.timeout=10
hive.server.tcp.keepalive=true
hive.server2.allow.user.substitution=true
hive.server2.async.exec.async.compile=false
hive.server2.async.exec.keepalive.time=10
hive.server2.async.exec.shutdown.timeout=10
hive.server2.async.exec.threads=100
hive.server2.async.exec.wait.queue.size=100
hive.server2.authentication=NONE
hive.server2.authentication.ldap.groupClassKey=groupOfNames
hive.server2.authentication.ldap.groupMembershipKey=member
hive.server2.authentication.ldap.guidKey=uid
hive.server2.clear.dangling.scratchdir=false
hive.server2.clear.dangling.scratchdir.interval=1800s
hive.server2.close.session.on.disconnect=true
hive.server2.compile.lock.timeout=0s
hive.server2.enable.doAs=true
hive.server2.global.init.file.location=${env:HIVE_CONF_DIR}
hive.server2.idle.operation.timeout=7200000
hive.server2.idle.session.check.operation=true
hive.server2.idle.session.timeout=900000
hive.server2.in.place.progress=true
hive.server2.keystore.password=
hive.server2.keystore.path=
    /databricks/keys/jetty-ssl-driver-keystore.jks:
hive.server2.llap.concurrent.queries=-1
hive.server2.logging.operation.enabled=true
hive.server2.logging.operation.level=EXECUTION
hive.server2.logging.operation.log.location=/local_disk0/tmp/root/operation_logs
hive.server2.long.polling.timeout=5000
hive.server2.map.fair.scheduler.queue=true
hive.server2.max.start.attempts=30
hive.server2.metrics.enabled=false
hive.server2.parallel.ops.in.session=true
hive.server2.session.check.interval=60000
hive.server2.sleep.interval.between.start.attempts=60s
hive.server2.support.dynamic.service.discovery=false
hive.server2.table.type.mapping=CLASSIC
hive.server2.tez.initialize.default.sessions=false
hive.server2.tez.session.lifetime=162h
hive.server2.tez.session.lifetime.jitter=3h
hive.server2.tez.sessions.custom.queue.allowed=true
hive.server2.tez.sessions.init.threads=16
hive.server2.tez.sessions.per.default.queue=1
hive.server2.thrift.client.connect.retry.limit=1
hive.server2.thrift.client.password=anonymous
hive.server2.thrift.client.retry.delay.seconds=1s
hive.server2.thrift.client.retry.limit=1
hive.server2.thrift.client.user=anonymous
hive.server2.thrift.exponential.backoff.slot.length=100
hive.server2.thrift.http.cookie.auth.enabled=true
hive.server2.thrift.http.cookie.is.httponly=true
hive.server2.thrift.http.cookie.is.secure=true
hive.server2.thrift.http.cookie.max.age=86400
hive.server2.thrift.http.max.idle.time=1800000
hive.server2.thrift.http.path=
    cliservice:
hive.server2.thrift.http.port=10000
hive.server2.thrift.http.request.header.size=6144
hive.server2.thrift.http.response.header.size=6144
hive.server2.thrift.http.worker.keepalive.time=60
hive.server2.thrift.login.timeout=20
hive.server2.thrift.max.message.size=104857600
hive.server2.thrift.max.worker.threads=500
hive.server2.thrift.min.worker.threads=5
hive.server2.thrift.port=10000
hive.server2.thrift.resultset.default.fetch.size=1000
hive.server2.thrift.resultset.max.fetch.size=10000
hive.server2.thrift.resultset.serialize.in.tasks=false
hive.server2.thrift.sasl.qop=auth
hive.server2.thrift.worker.keepalive.time=60
hive.server2.transport.mode=http
hive.server2.use.SSL=true
hive.server2.webui.host=0.0.0.0
hive.server2.webui.max.historic.queries=25
hive.server2.webui.max.threads=50
hive.server2.webui.port=10002
hive.server2.webui.spnego.principal=HTTP/_HOST@EXAMPLE.COM
hive.server2.webui.use.spnego=false
hive.server2.webui.use.ssl=false
hive.server2.xsrf.filter.enabled=false
hive.server2.zookeeper.namespace=hiveserver2
hive.server2.zookeeper.publish.configs=true
hive.service.metrics.class=org.apache.hadoop.hive.common.metrics.metrics2.CodahaleMetrics
hive.service.metrics.file.frequency=5s
hive.service.metrics.file.location=/tmp/report.json
hive.service.metrics.hadoop2.component=hive
hive.service.metrics.hadoop2.frequency=30s
hive.service.metrics.reporter=JSON_FILE, JMX
hive.session.history.enabled=false
hive.session.id=5299c531-93ff-4c80-b9a4-d52a5bb3de16
hive.session.silent=false
hive.skewjoin.key=100000
hive.skewjoin.mapjoin.map.tasks=10000
hive.skewjoin.mapjoin.min.split=33554432
hive.smbjoin.cache.rows=10000
hive.spark.client.connect.timeout=1000
hive.spark.client.future.timeout=60
hive.spark.client.rpc.max.size=52428800
hive.spark.client.rpc.sasl.mechanisms=DIGEST-MD5
hive.spark.client.rpc.threads=8
hive.spark.client.secret.bits=256
hive.spark.client.server.connect.timeout=90000
hive.spark.dynamic.partition.pruning=false
hive.spark.dynamic.partition.pruning.max.data.size=104857600
hive.spark.exec.inplace.progress=true
hive.spark.job.monitor.timeout=60
hive.spark.use.file.size.for.mapjoin=false
hive.spark.use.groupby.shuffle=true
hive.spark.use.op.stats=true
hive.ssl.protocol.blacklist=SSLv2,SSLv3
hive.stageid.rearrange=none
hive.start.cleanup.scratchdir=false
hive.stats.atomic=false
hive.stats.autogather=false
hive.stats.collect.scancols=false
hive.stats.collect.tablekeys=false
hive.stats.column.autogather=false
hive.stats.dbclass=fs
hive.stats.deserialization.factor=1.0
hive.stats.fetch.column.stats=false
hive.stats.fetch.partition.stats=true
hive.stats.filter.in.factor=1.0
hive.stats.gather.num.threads=10
hive.stats.jdbc.timeout=30
hive.stats.join.factor=1.1
hive.stats.list.num.entries=10
hive.stats.map.num.entries=10
hive.stats.max.variable.length=100
hive.stats.ndv.error=20.0
hive.stats.reliable=false
hive.stats.retries.wait=3000
hive.strict.checks.bucketing=true
hive.strict.checks.cartesian.product=true
hive.strict.checks.large.query=false
hive.strict.checks.type.safety=true
hive.support.concurrency=false
hive.support.quoted.identifiers=column
hive.support.special.characters.tablename=true
hive.test.authz.sstd.hs2.mode=false
hive.test.fail.compaction=false
hive.test.fail.heartbeater=false
hive.test.mode=false
hive.test.mode.prefix=test_
hive.test.mode.samplefreq=32
hive.test.rollbacktxn=false
hive.tez.auto.reducer.parallelism=false
hive.tez.bigtable.minsize.semijoin.reduction=1000000
hive.tez.bloom.filter.factor=2.0
hive.tez.bucket.pruning=false
hive.tez.bucket.pruning.compat=true
hive.tez.container.max.java.heap.fraction=0.8
hive.tez.container.size=-1
hive.tez.cpu.vcores=-1
hive.tez.dynamic.partition.pruning=true
hive.tez.dynamic.partition.pruning.max.data.size=104857600
hive.tez.dynamic.partition.pruning.max.event.size=1048576
hive.tez.dynamic.semijoin.reduction=true
hive.tez.dynamic.semijoin.reduction.threshold=0.5
hive.tez.enable.memory.manager=true
hive.tez.exec.inplace.progress=true
hive.tez.exec.print.summary=false
hive.tez.hs2.user.access=true
hive.tez.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat
hive.tez.input.generate.consistent.splits=true
hive.tez.max.bloom.filter.entries=100000000
hive.tez.max.partition.factor=2.0
hive.tez.min.bloom.filter.entries=1000000
hive.tez.min.partition.factor=0.25
hive.tez.smb.number.waves=0.5
hive.tez.task.scale.memory.reserve-fraction.min=0.3
hive.tez.task.scale.memory.reserve.fraction=-1.0
hive.tez.task.scale.memory.reserve.fraction.max=0.5
hive.timedout.txn.reaper.interval=180s
hive.timedout.txn.reaper.start=100s
hive.transactional.events.mem=10000000
hive.transactional.table.scan=false
hive.transform.escape.input=false
hive.transpose.aggr.join=false
hive.txn.heartbeat.threadpool.size=5
hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DummyTxnManager
hive.txn.manager.dump.lock.state.on.acquire.timeout=false
hive.txn.max.open.batch=1000
hive.txn.operational.properties=0
hive.txn.strict.locking.mode=true
hive.txn.timeout=300
hive.typecheck.on.insert=true
hive.udtf.auto.progress=false
hive.unlock.numretries=10
hive.user.install.directory=/user/
hive.variable.substitute=true
hive.variable.substitute.depth=40
hive.vectorized.adaptor.usage.mode=all
hive.vectorized.execution.enabled=false
hive.vectorized.execution.mapjoin.minmax.enabled=false
hive.vectorized.execution.mapjoin.native.enabled=true
hive.vectorized.execution.mapjoin.native.fast.hashtable.enabled=false
hive.vectorized.execution.mapjoin.native.multikey.only.enabled=false
hive.vectorized.execution.mapjoin.overflow.repeated.threshold=-1
hive.vectorized.execution.reduce.enabled=true
hive.vectorized.execution.reduce.groupby.enabled=true
hive.vectorized.execution.reducesink.new.enabled=true
hive.vectorized.groupby.checkinterval=100000
hive.vectorized.groupby.flush.percent=0.1
hive.vectorized.groupby.maxentries=1000000
hive.vectorized.use.row.serde.deserialize=false
hive.vectorized.use.vector.serde.deserialize=true
hive.vectorized.use.vectorized.input.format=true
hive.warehouse.subdir.inherit.perms=false
hive.writeset.reaper.interval=60s
hive.zookeeper.clean.extra.nodes=false
hive.zookeeper.client.port=2181
hive.zookeeper.connection.basesleeptime=1000
hive.zookeeper.connection.max.retries=3
hive.zookeeper.namespace=hive_zookeeper_namespace
hive.zookeeper.session.timeout=1200000
io.compression.codec.bzip2.library=system-native
io.file.buffer.size=65536
io.map.index.interval=128
io.map.index.skip=0
io.mapfile.bloom.error.rate=0.005
io.mapfile.bloom.size=1048576
io.native.lib.available=true
io.seqfile.compress.blocksize=1000000
io.seqfile.lazydecompress=true
io.seqfile.local.dir=/local_disk0/tmp/io/local
io.seqfile.sorter.recordlimit=1000000
io.serializations=org.apache.hadoop.io.serializer.WritableSerialization,org.apache.hadoop.io.serializer.avro.AvroSpecificSerialization,org.apache.hadoop.io.serializer.avro.AvroReflectSerialization
io.skip.checksum.errors=false
ipc.client.connect.max.retries=10
ipc.client.connect.max.retries.on.timeouts=45
ipc.client.connect.retry.interval=1000
ipc.client.connect.timeout=20000
ipc.client.connection.maxidletime=10000
ipc.client.fallback-to-simple-auth-allowed=false
ipc.client.idlethreshold=4000
ipc.client.kill.max=10
ipc.client.ping=true
ipc.client.rpc-timeout.ms=0
ipc.maximum.data.length=67108864
ipc.ping.interval=60000
ipc.server.listen.queue.size=128
ipc.server.max.connections=0
javax.jdo.PersistenceManagerFactoryClass=org.datanucleus.api.jdo.JDOPersistenceManagerFactory
javax.jdo.option.ConnectionDriverName=org.postgresql.Driver
javax.jdo.option.ConnectionPassword=
javax.jdo.option.ConnectionURL=jdbc:postgresql://uathmsdb.trans.enterprisecsp.net:5432/hive_adls
javax.jdo.option.ConnectionUserName=tedbuat
javax.jdo.option.DetachAllOnCommit=true
javax.jdo.option.Multithreaded=true
javax.jdo.option.NonTransactionalRead=true
map.sort.class=org.apache.hadoop.util.QuickSort
mapred.child.java.opts=-Xmx200m
mapred.output.committer.class=com.databricks.backend.daemon.data.client.DirectOutputCommitter
mapreduce.am.max-attempts=2
mapreduce.app-submission.cross-platform=false
mapreduce.client.completion.pollinterval=5000
mapreduce.client.output.filter=FAILED
mapreduce.client.progressmonitor.pollinterval=1000
mapreduce.client.submit.file.replication=10
mapreduce.cluster.acls.enabled=false
mapreduce.cluster.local.dir=/local_disk0/tmp/mapred/local
mapreduce.cluster.temp.dir=/local_disk0/tmp/mapred/temp
mapreduce.fileoutputcommitter.algorithm.version=2
mapreduce.framework.name=local
mapreduce.ifile.readahead=true
mapreduce.ifile.readahead.bytes=4194304
mapreduce.input.fileinputformat.list-status.num-threads=1
mapreduce.input.fileinputformat.split.maxsize=256000000
mapreduce.input.fileinputformat.split.minsize=0
mapreduce.input.fileinputformat.split.minsize.per.node=1
mapreduce.input.fileinputformat.split.minsize.per.rack=1
mapreduce.input.lineinputformat.linespermap=1
mapreduce.job.acl-modify-job= 
mapreduce.job.acl-view-job= 
mapreduce.job.classloader=false
mapreduce.job.committer.setup.cleanup.needed=true
mapreduce.job.complete.cancel.delegation.tokens=true
mapreduce.job.counters.max=120
mapreduce.job.emit-timeline-data=false
mapreduce.job.end-notification.max.attempts=5
mapreduce.job.end-notification.max.retry.interval=5000
mapreduce.job.end-notification.retry.attempts=0
mapreduce.job.end-notification.retry.interval=1000
mapreduce.job.hdfs-servers=dbfs:///
mapreduce.job.jvm.numtasks=1
mapreduce.job.map.output.collector.class=org.apache.hadoop.mapred.MapTask$MapOutputBuffer
mapreduce.job.maps=2
mapreduce.job.max.split.locations=10
mapreduce.job.maxtaskfailures.per.tracker=3
mapreduce.job.queuename=default
mapreduce.job.reduce.shuffle.consumer.plugin.class=org.apache.hadoop.mapreduce.task.reduce.Shuffle
mapreduce.job.reduce.slowstart.completedmaps=0.05
mapreduce.job.reducer.preempt.delay.sec=0
mapreduce.job.reducer.unconditional-preempt.delay.sec=300
mapreduce.job.reduces=100
mapreduce.job.running.map.limit=0
mapreduce.job.running.reduce.limit=0
mapreduce.job.speculative.minimum-allowed-tasks=10
mapreduce.job.speculative.retry-after-no-speculate=1000
mapreduce.job.speculative.retry-after-speculate=15000
mapreduce.job.speculative.slowtaskthreshold=1.0
mapreduce.job.speculative.speculative-cap-running-tasks=0.1
mapreduce.job.speculative.speculative-cap-total-tasks=0.01
mapreduce.job.split.metainfo.maxsize=10000000
mapreduce.job.token.tracking.ids.enabled=false
mapreduce.job.ubertask.enable=false
mapreduce.job.ubertask.maxmaps=9
mapreduce.job.ubertask.maxreduces=1
mapreduce.job.userlog.retain.hours=24
mapreduce.jobhistory.address=0.0.0.0:10020
mapreduce.jobhistory.admin.acl=*
mapreduce.jobhistory.admin.address=0.0.0.0:10033
mapreduce.jobhistory.cleaner.enable=true
mapreduce.jobhistory.cleaner.interval-ms=86400000
mapreduce.jobhistory.client.thread-count=10
mapreduce.jobhistory.datestring.cache.size=200000
mapreduce.jobhistory.done-dir=/tmp/hadoop-yarn/staging/history/done
mapreduce.jobhistory.http.policy=HTTP_ONLY
mapreduce.jobhistory.intermediate-done-dir=/tmp/hadoop-yarn/staging/history/done_intermediate
mapreduce.jobhistory.joblist.cache.size=20000
mapreduce.jobhistory.keytab=/etc/security/keytab/jhs.service.keytab
mapreduce.jobhistory.loadedjobs.cache.size=5
mapreduce.jobhistory.max-age-ms=604800000
mapreduce.jobhistory.minicluster.fixed.ports=false
mapreduce.jobhistory.move.interval-ms=180000
mapreduce.jobhistory.move.thread-count=3
mapreduce.jobhistory.principal=jhs/_HOST@REALM.TLD
mapreduce.jobhistory.recovery.enable=false
mapreduce.jobhistory.recovery.store.class=org.apache.hadoop.mapreduce.v2.hs.HistoryServerFileSystemStateStoreService
mapreduce.jobhistory.recovery.store.fs.uri=/local_disk0/tmp/mapred/history/recoverystore
mapreduce.jobhistory.recovery.store.leveldb.path=
    /local_disk0/tmp/mapred/history/recoverystore:
mapreduce.jobhistory.webapp.address=0.0.0.0:19888
mapreduce.jobtracker.address=local
mapreduce.jobtracker.expire.trackers.interval=600000
mapreduce.jobtracker.handler.count=10
mapreduce.jobtracker.heartbeats.in.second=100
mapreduce.jobtracker.http.address=0.0.0.0:50030
mapreduce.jobtracker.instrumentation=org.apache.hadoop.mapred.JobTrackerMetricsInst
mapreduce.jobtracker.jobhistory.block.size=3145728
mapreduce.jobtracker.jobhistory.lru.cache.size=5
mapreduce.jobtracker.jobhistory.task.numberprogresssplits=12
mapreduce.jobtracker.maxtasks.perjob=-1
mapreduce.jobtracker.persist.jobstatus.active=true
mapreduce.jobtracker.persist.jobstatus.dir=/jobtracker/jobsInfo
mapreduce.jobtracker.persist.jobstatus.hours=1
mapreduce.jobtracker.restart.recover=false
mapreduce.jobtracker.retiredjobs.cache.size=1000
mapreduce.jobtracker.staging.root.dir=/local_disk0/tmp/mapred/staging
mapreduce.jobtracker.system.dir=/local_disk0/tmp/mapred/system
mapreduce.jobtracker.taskcache.levels=2
mapreduce.jobtracker.taskscheduler=org.apache.hadoop.mapred.JobQueueTaskScheduler
mapreduce.jobtracker.tasktracker.maxblacklists=4
mapreduce.local.clientfactory.class.name=org.apache.hadoop.mapred.LocalClientFactory
mapreduce.map.cpu.vcores=1
mapreduce.map.maxattempts=4
mapreduce.map.memory.mb=1024
mapreduce.map.output.compress=false
mapreduce.map.output.compress.codec=org.apache.hadoop.io.compress.DefaultCodec
mapreduce.map.skip.maxrecords=0
mapreduce.map.skip.proc.count.autoincr=true
mapreduce.map.sort.spill.percent=0.80
mapreduce.map.speculative=true
mapreduce.output.fileoutputformat.compress=false
mapreduce.output.fileoutputformat.compress.codec=org.apache.hadoop.io.compress.DefaultCodec
mapreduce.output.fileoutputformat.compress.type=RECORD
mapreduce.reduce.cpu.vcores=1
mapreduce.reduce.input.buffer.percent=0.0
mapreduce.reduce.markreset.buffer.percent=0.0
mapreduce.reduce.maxattempts=4
mapreduce.reduce.memory.mb=1024
mapreduce.reduce.merge.inmem.threshold=1000
mapreduce.reduce.shuffle.connect.timeout=180000
mapreduce.reduce.shuffle.fetch.retry.enabled=false
mapreduce.reduce.shuffle.fetch.retry.interval-ms=1000
mapreduce.reduce.shuffle.fetch.retry.timeout-ms=30000
mapreduce.reduce.shuffle.input.buffer.percent=0.70
mapreduce.reduce.shuffle.memory.limit.percent=0.25
mapreduce.reduce.shuffle.merge.percent=0.66
mapreduce.reduce.shuffle.parallelcopies=5
mapreduce.reduce.shuffle.read.timeout=180000
mapreduce.reduce.shuffle.retry-delay.max.ms=60000
mapreduce.reduce.skip.maxgroups=0
mapreduce.reduce.skip.proc.count.autoincr=true
mapreduce.reduce.speculative=true
mapreduce.shuffle.connection-keep-alive.enable=false
mapreduce.shuffle.connection-keep-alive.timeout=5
mapreduce.shuffle.max.connections=0
mapreduce.shuffle.max.threads=0
mapreduce.shuffle.port=13562
mapreduce.shuffle.ssl.enabled=false
mapreduce.shuffle.ssl.file.buffer.size=65536
mapreduce.shuffle.transfer.buffer.size=131072
mapreduce.task.combine.progress.records=10000
mapreduce.task.files.preserve.failedtasks=false
mapreduce.task.io.sort.factor=10
mapreduce.task.io.sort.mb=100
mapreduce.task.merge.progress.records=10000
mapreduce.task.profile=false
mapreduce.task.profile.map.params=-agentlib:hprof=cpu=samples,heap=sites,force=n,thread=y,verbose=n,file=%s
mapreduce.task.profile.maps=0-2
mapreduce.task.profile.params=-agentlib:hprof=cpu=samples,heap=sites,force=n,thread=y,verbose=n,file=%s
mapreduce.task.profile.reduce.params=-agentlib:hprof=cpu=samples,heap=sites,force=n,thread=y,verbose=n,file=%s
mapreduce.task.profile.reduces=0-2
mapreduce.task.skip.start.attempts=2
mapreduce.task.timeout=600000
mapreduce.task.userlog.limit.kb=0
mapreduce.tasktracker.dns.interface=default
mapreduce.tasktracker.dns.nameserver=default
mapreduce.tasktracker.healthchecker.interval=60000
mapreduce.tasktracker.healthchecker.script.timeout=600000
mapreduce.tasktracker.http.address=0.0.0.0:50060
mapreduce.tasktracker.http.threads=40
mapreduce.tasktracker.indexcache.mb=10
mapreduce.tasktracker.instrumentation=org.apache.hadoop.mapred.TaskTrackerMetricsInst
mapreduce.tasktracker.local.dir.minspacekill=0
mapreduce.tasktracker.local.dir.minspacestart=0
mapreduce.tasktracker.map.tasks.maximum=2
mapreduce.tasktracker.outofband.heartbeat=false
mapreduce.tasktracker.reduce.tasks.maximum=2
mapreduce.tasktracker.report.address=127.0.0.1:0
mapreduce.tasktracker.taskcontroller=org.apache.hadoop.mapred.DefaultTaskController
mapreduce.tasktracker.taskmemorymanager.monitoringinterval=5000
mapreduce.tasktracker.tasks.sleeptimebeforesigkill=5000
net.topology.impl=org.apache.hadoop.net.NetworkTopology
net.topology.node.switch.mapping.impl=org.apache.hadoop.net.ScriptBasedMapping
net.topology.script.number.args=100
nfs.allow.insecure.ports=true
nfs.dump.dir=/tmp/.hdfs-nfs
nfs.exports.allowed.hosts=* rw
nfs.mountd.port=4242
nfs.rtmax=1048576
nfs.server.port=2049
nfs.wtmax=1048576
parquet.memory.pool.ratio=0.5
rpc.metrics.quantile.enable=false
s3.blocksize=67108864
s3.bytes-per-checksum=512
s3.client-write-packet-size=65536
s3.replication=3
s3.stream-buffer-size=4096
s3native.blocksize=67108864
s3native.bytes-per-checksum=512
s3native.client-write-packet-size=65536
s3native.replication=3
s3native.stream-buffer-size=4096
spark.akka.frameSize=256
spark.app.id=app-20191008203218-0000
spark.app.name=Databricks Shell
spark.cleaner.referenceTracking.blocking=false
spark.databricks.acl.client=com.databricks.spark.sql.acl.client.SparkSqlAclClient
spark.databricks.acl.dfAclsEnabled=false
spark.databricks.acl.provider=com.databricks.sql.acl.ReflectionBackedAclProvider
spark.databricks.aggressiveWindowDownS=300
spark.databricks.cloudProvider=Azure
spark.databricks.clusterSource=JOB
spark.databricks.clusterUsageTags.autoTerminationMinutes=0
spark.databricks.clusterUsageTags.clusterAggressiveAutoscalingWindowDownSeconds=300
spark.databricks.clusterUsageTags.clusterAllTags=[{"key":"Vendor","value":"Databricks"},{"key":"Creator","value":"basheer.ahamed@nielsen.com"},{"key":"ClusterName","value":"job-65695-run-1"},{"key":"ClusterId","value":"1008-202845-vogue653"},{"key":"DatabricksEnvironment","value":"workerenv-5614106561397268"}]
spark.databricks.clusterUsageTags.clusterCreator=JobLauncher
spark.databricks.clusterUsageTags.clusterGeneration=0
spark.databricks.clusterUsageTags.clusterId=1008-202845-vogue653
spark.databricks.clusterUsageTags.clusterLogDeliveryEnabled=true
spark.databricks.clusterUsageTags.clusterLogDestination=dbfs:/mnt/te_mount/uat_logs/
spark.databricks.clusterUsageTags.clusterMaxWorkers=5
spark.databricks.clusterUsageTags.clusterMetastoreAccessType=RDS_DIRECT
spark.databricks.clusterUsageTags.clusterMinWorkers=1
spark.databricks.clusterUsageTags.clusterName=job-65695-run-1
spark.databricks.clusterUsageTags.clusterNoDriverDaemon=false
spark.databricks.clusterUsageTags.clusterNodeType=Standard_E16s_v3
spark.databricks.clusterUsageTags.clusterNumSshKeys=0
spark.databricks.clusterUsageTags.clusterOwnerOrgId=5614106561397268
spark.databricks.clusterUsageTags.clusterOwnerUserId=3464818780517260
spark.databricks.clusterUsageTags.clusterPinned=false
spark.databricks.clusterUsageTags.clusterPythonVersion=2
spark.databricks.clusterUsageTags.clusterResourceClass=default
spark.databricks.clusterUsageTags.clusterScalingType=autoscaling
spark.databricks.clusterUsageTags.clusterSku=STANDARD_SKU
spark.databricks.clusterUsageTags.clusterState=Pending
spark.databricks.clusterUsageTags.clusterStateMessage=Starting Spark
spark.databricks.clusterUsageTags.clusterTargetWorkers=1
spark.databricks.clusterUsageTags.clusterWorkers=1
spark.databricks.clusterUsageTags.containerType=LXC
spark.databricks.clusterUsageTags.driverContainerId=9206d3fcddb044288db66217141d2db8
spark.databricks.clusterUsageTags.driverContainerPrivateIp=10.139.64.31
spark.databricks.clusterUsageTags.driverInstanceId=45b042d078c048ac981c2cf65c0b17ea
spark.databricks.clusterUsageTags.driverInstancePrivateIp=10.139.0.30
spark.databricks.clusterUsageTags.driverNodeType=Standard_E16s_v3
spark.databricks.clusterUsageTags.driverPublicDns=20.186.33.64
spark.databricks.clusterUsageTags.enableCredentialPassthrough=false
spark.databricks.clusterUsageTags.enableDfAcls=false
spark.databricks.clusterUsageTags.enableElasticDisk=true
spark.databricks.clusterUsageTags.enableJdbcAutoStart=true
spark.databricks.clusterUsageTags.enableJobsAutostart=true
spark.databricks.clusterUsageTags.enableSqlAclsOnly=false
spark.databricks.clusterUsageTags.isSingleUserCluster=false
spark.databricks.clusterUsageTags.numPerClusterInitScriptsV2=0
spark.databricks.clusterUsageTags.sparkVersion=5.2.x-scala2.11
spark.databricks.clusterUsageTags.userProvidedRemoteVolumeCount=0
spark.databricks.clusterUsageTags.userProvidedRemoteVolumeSizeGb=0
spark.databricks.clusterUsageTags.userProvidedRemoteVolumeType=azure_disk_volume_type: PREMIUM_LRS

spark.databricks.clusterUsageTags.workerEnvironmentId=workerenv-5614106561397268
spark.databricks.credential.redactor=com.databricks.logging.secrets.CredentialRedactorProxyImpl
spark.databricks.delta.logStore.crossCloud.fatal=true
spark.databricks.delta.multiClusterWrites.enabled=true
spark.databricks.delta.preview.enabled=true
spark.databricks.driverNodeTypeId=Standard_E16s_v3
spark.databricks.eventLog.dir=eventlogs
spark.databricks.io.directoryCommit.enableLogicalDelete=false
spark.databricks.overrideDefaultCommitProtocol=org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol
spark.databricks.passthrough.adls.tokenProviderClassName=com.databricks.backend.daemon.data.client.adl.AdlCredentialContextTokenProvider
spark.databricks.passthrough.s3a.tokenProviderClassName=com.databricks.backend.daemon.driver.aws.AwsCredentialContextTokenProvider
spark.databricks.preemption.enabled=true
spark.databricks.r.cleanWorkspace=true
spark.databricks.redactor=com.databricks.spark.util.DatabricksSparkLogRedactorProxy
spark.databricks.session.share=false
spark.databricks.sparkContextId=617644649427700606
spark.databricks.sql.hive.msck.batchSize=200
spark.databricks.sql.hive.msck.parallelism=20
spark.databricks.tahoe.logStore.aws.class=com.databricks.tahoe.store.MultiClusterLogStore
spark.databricks.tahoe.logStore.azure.class=com.databricks.tahoe.store.AzureLogStore
spark.databricks.tahoe.logStore.class=com.databricks.tahoe.store.DelegatingLogStore
spark.databricks.workerNodeTypeId=Standard_E16s_v3
spark.driver.allowMultipleContexts=false
spark.driver.host=10.139.64.31
spark.driver.maxResultSize=0
spark.driver.port=40667
spark.driver.tempDirectory=/local_disk0/tmp
spark.eventLog.enabled=false
spark.executor.extraClassPath=
    /databricks/spark/dbconf/log4j/executor:
    /databricks/spark/dbconf/jets3t/:
    /databricks/spark/dbconf/hadoop:
    /databricks/hive/conf:
    /databricks/jars/api-base--api-base_java-spark_2.4_2.11_deploy.jar:
    /databricks/jars/api-base--api-base-spark_2.4_2.11_deploy.jar:
    /databricks/jars/api--rpc--rpc_parser-spark_2.4_2.11_deploy.jar:
    /databricks/jars/chauffeur-api--api--endpoints--endpoints-spark_2.4_2.11_deploy.jar:
    /databricks/jars/chauffeur-api--chauffeur-api-spark_2.4_2.11_deploy.jar:
    /databricks/jars/common--client--client-spark_2.4_2.11_deploy.jar:
    /databricks/jars/common--common-spark_2.4_2.11_deploy.jar:
    /databricks/jars/common--credentials--credentials-spark_2.4_2.11_deploy.jar:
    /databricks/jars/common--hadoop--hadoop-spark_2.4_2.11_deploy.jar:
    /databricks/jars/common--jetty--client--client-spark_2.4_2.11_deploy.jar:
    /databricks/jars/common--lazy--lazy-spark_2.4_2.11_deploy.jar:
    /databricks/jars/common--libcommon_resources.jar:
    /databricks/jars/common--path--path-spark_2.4_2.11_deploy.jar:
    /databricks/jars/common--rate-limiter--rate-limiter-spark_2.4_2.11_deploy.jar:
    /databricks/jars/common--storage--storage-spark_2.4_2.11_deploy.jar:
    /databricks/jars/daemon--data--client--client-spark_2.4_2.11_deploy.jar:
    /databricks/jars/daemon--data--client--conf--conf-spark_2.4_2.11_deploy.jar:
    /databricks/jars/daemon--data--client--utils-spark_2.4_2.11_deploy.jar:
    /databricks/jars/daemon--data--data-common--data-common-spark_2.4_2.11_deploy.jar:
    /databricks/jars/dbfs--utils--dbfs-utils-spark_2.4_2.11_deploy.jar:
    /databricks/jars/extern--acl--auth--auth-spark_2.4_2.11_deploy.jar:
    /databricks/jars/extern--extern-spark_2.4_2.11_deploy.jar:
    /databricks/jars/extern--libaws-regions.jar:
    /databricks/jars/----jackson_annotations_shaded--libjackson-annotations.jar:
    /databricks/jars/----jackson_core_shaded--libjackson-core.jar:
    /databricks/jars/----jackson_databind_shaded--libjackson-databind.jar:
    /databricks/jars/----jackson_datatype_joda_shaded--libjackson-datatype-joda.jar:
    /databricks/jars/jsonutil--jsonutil-spark_2.4_2.11_deploy.jar:
    /databricks/jars/logging--log4j-mod--log4j-mod-spark_2.4_2.11_deploy.jar:
    /databricks/jars/logging--utils--logging-utils-spark_2.4_2.11_deploy.jar:
    /databricks/jars/s3commit--client--client-spark_2.4_2.11_deploy.jar:
    /databricks/jars/s3commit--common--common-spark_2.4_2.11_deploy.jar:
    /databricks/jars/s3--s3-spark_2.4_2.11_deploy.jar:
    /databricks/jars/secret-manager--api--api-spark_2.4_2.11_deploy.jar:
    /databricks/jars/spark--common--spark-common-spark_2.4_2.11_deploy.jar:
    /databricks/jars/spark--conf-reader--conf-reader_lib-spark_2.4_2.11_deploy.jar:
    /databricks/jars/spark--dbutils--dbutils-api-spark_2.4_2.11_deploy.jar:
    /databricks/jars/spark--driver--antlr--parser-spark_2.4_2.11_deploy.jar:
    /databricks/jars/spark--driver--common--driver-common-spark_2.4_2.11_deploy.jar:
    /databricks/jars/spark--driver--display--display-spark_2.4_2.11_deploy.jar:
    /databricks/jars/spark--driver--driver-spark_2.4_2.11_deploy.jar:
    /databricks/jars/spark--driver--events-spark_2.4_2.11_deploy.jar:
    /databricks/jars/spark--driver--ml--ml-spark_2.4_2.11_deploy.jar:
    /databricks/jars/spark--driver--secret-redaction-spark_2.4_2.11_deploy.jar:
    /databricks/jars/spark--driver--spark--resources-resources.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--antlr--antlr--antlr__antlr__2.7.7.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--amazon-kinesis-client--com.amazonaws__amazon-kinesis-client__1.8.10.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-autoscaling--com.amazonaws__aws-java-sdk-autoscaling__1.11.313.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-cloudformation--com.amazonaws__aws-java-sdk-cloudformation__1.11.313.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-cloudfront--com.amazonaws__aws-java-sdk-cloudfront__1.11.313.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-cloudhsm--com.amazonaws__aws-java-sdk-cloudhsm__1.11.313.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-cloudsearch--com.amazonaws__aws-java-sdk-cloudsearch__1.11.313.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-cloudtrail--com.amazonaws__aws-java-sdk-cloudtrail__1.11.313.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-cloudwatch--com.amazonaws__aws-java-sdk-cloudwatch__1.11.313.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-cloudwatchmetrics--com.amazonaws__aws-java-sdk-cloudwatchmetrics__1.11.313.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-codedeploy--com.amazonaws__aws-java-sdk-codedeploy__1.11.313.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-cognitoidentity--com.amazonaws__aws-java-sdk-cognitoidentity__1.11.313.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-cognitosync--com.amazonaws__aws-java-sdk-cognitosync__1.11.313.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-config--com.amazonaws__aws-java-sdk-config__1.11.313.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-core--com.amazonaws__aws-java-sdk-core__1.11.313.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-datapipeline--com.amazonaws__aws-java-sdk-datapipeline__1.11.313.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-directconnect--com.amazonaws__aws-java-sdk-directconnect__1.11.313.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-directory--com.amazonaws__aws-java-sdk-directory__1.11.313.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-dynamodb--com.amazonaws__aws-java-sdk-dynamodb__1.11.313.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-ec2--com.amazonaws__aws-java-sdk-ec2__1.11.313.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-ecs--com.amazonaws__aws-java-sdk-ecs__1.11.313.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-efs--com.amazonaws__aws-java-sdk-efs__1.11.313.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-elasticache--com.amazonaws__aws-java-sdk-elasticache__1.11.313.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-elasticbeanstalk--com.amazonaws__aws-java-sdk-elasticbeanstalk__1.11.313.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-elasticloadbalancing--com.amazonaws__aws-java-sdk-elasticloadbalancing__1.11.313.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-elastictranscoder--com.amazonaws__aws-java-sdk-elastictranscoder__1.11.313.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-emr--com.amazonaws__aws-java-sdk-emr__1.11.313.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-glacier--com.amazonaws__aws-java-sdk-glacier__1.11.313.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-iam--com.amazonaws__aws-java-sdk-iam__1.11.313.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-importexport--com.amazonaws__aws-java-sdk-importexport__1.11.313.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-kinesis--com.amazonaws__aws-java-sdk-kinesis__1.11.313.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-kms--com.amazonaws__aws-java-sdk-kms__1.11.313.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-lambda--com.amazonaws__aws-java-sdk-lambda__1.11.313.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-logs--com.amazonaws__aws-java-sdk-logs__1.11.313.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-machinelearning--com.amazonaws__aws-java-sdk-machinelearning__1.11.313.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-opsworks--com.amazonaws__aws-java-sdk-opsworks__1.11.313.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-rds--com.amazonaws__aws-java-sdk-rds__1.11.313.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-redshift--com.amazonaws__aws-java-sdk-redshift__1.11.313.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-route53--com.amazonaws__aws-java-sdk-route53__1.11.313.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-s3--com.amazonaws__aws-java-sdk-s3__1.11.313.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-ses--com.amazonaws__aws-java-sdk-ses__1.11.313.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-simpledb--com.amazonaws__aws-java-sdk-simpledb__1.11.313.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-simpleworkflow--com.amazonaws__aws-java-sdk-simpleworkflow__1.11.313.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-sns--com.amazonaws__aws-java-sdk-sns__1.11.313.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-sqs--com.amazonaws__aws-java-sdk-sqs__1.11.313.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-ssm--com.amazonaws__aws-java-sdk-ssm__1.11.313.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-storagegateway--com.amazonaws__aws-java-sdk-storagegateway__1.11.313.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-sts--com.amazonaws__aws-java-sdk-sts__1.11.313.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-support--com.amazonaws__aws-java-sdk-support__1.11.313.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-swf-libraries--com.amazonaws__aws-java-sdk-swf-libraries__1.11.22.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-workspaces--com.amazonaws__aws-java-sdk-workspaces__1.11.313.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--jmespath-java--com.amazonaws__jmespath-java__1.11.313.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.carrotsearch--hppc--com.carrotsearch__hppc__0.7.2.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.chuusai--shapeless_2.11--com.chuusai__shapeless_2.11__2.3.2.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.clearspring.analytics--stream--com.clearspring.analytics__stream__2.7.0.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.databricks--dbml-local_2.11--com.databricks__dbml-local_2.11__0.5.0-db8-spark2.4.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.databricks--dbml-local_2.11-tests--com.databricks__dbml-local_2.11-tests__0.5.0-db8-spark2.4.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.databricks--jets3t--com.databricks__jets3t__0.7.1-0.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.databricks--Rserve--com.databricks__Rserve__1.8-3.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.databricks.scalapb--compilerplugin_2.11--com.databricks.scalapb__compilerplugin_2.11__0.4.15-9.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.databricks.scalapb--scalapb-runtime_2.11--com.databricks.scalapb__scalapb-runtime_2.11__0.4.15-9.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.esotericsoftware--kryo-shaded--com.esotericsoftware__kryo-shaded__4.0.2.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.esotericsoftware--minlog--com.esotericsoftware__minlog__1.3.0.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.fasterxml--classmate--com.fasterxml__classmate__1.0.0.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.fasterxml.jackson.core--jackson-annotations--com.fasterxml.jackson.core__jackson-annotations__2.6.7.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.fasterxml.jackson.core--jackson-core--com.fasterxml.jackson.core__jackson-core__2.6.7.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.fasterxml.jackson.core--jackson-databind--com.fasterxml.jackson.core__jackson-databind__2.6.7.1.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.fasterxml.jackson.dataformat--jackson-dataformat-cbor--com.fasterxml.jackson.dataformat__jackson-dataformat-cbor__2.6.7.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.fasterxml.jackson.datatype--jackson-datatype-joda--com.fasterxml.jackson.datatype__jackson-datatype-joda__2.6.7.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.fasterxml.jackson.module--jackson-module-paranamer--com.fasterxml.jackson.module__jackson-module-paranamer__2.6.7.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.fasterxml.jackson.module--jackson-module-scala_2.11--com.fasterxml.jackson.module__jackson-module-scala_2.11__2.6.7.1.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.github.fommil--jniloader--com.github.fommil__jniloader__1.1.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.github.fommil.netlib--core--com.github.fommil.netlib__core__1.1.2.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.github.fommil.netlib--native_ref-java--com.github.fommil.netlib__native_ref-java__1.1.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.github.fommil.netlib--native_ref-java-natives--com.github.fommil.netlib__native_ref-java-natives__1.1.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.github.fommil.netlib--native_system-java--com.github.fommil.netlib__native_system-java__1.1.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.github.fommil.netlib--native_system-java-natives--com.github.fommil.netlib__native_system-java-natives__1.1.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.github.fommil.netlib--netlib-native_ref-linux-x86_64-natives--com.github.fommil.netlib__netlib-native_ref-linux-x86_64-natives__1.1.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.github.fommil.netlib--netlib-native_system-linux-x86_64-natives--com.github.fommil.netlib__netlib-native_system-linux-x86_64-natives__1.1.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.github.luben--zstd-jni--com.github.luben__zstd-jni__1.3.2-2.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.github.rwl--jtransforms--com.github.rwl__jtransforms__2.4.0.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.google.code.findbugs--jsr305--com.google.code.findbugs__jsr305__2.0.1.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.google.code.gson--gson--com.google.code.gson__gson__2.2.4.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.googlecode.javaewah--JavaEWAH--com.googlecode.javaewah__JavaEWAH__0.3.2.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.google.guava--guava--com.google.guava__guava__15.0.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.google.protobuf--protobuf-java--com.google.protobuf__protobuf-java__2.6.1.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.h2database--h2--com.h2database__h2__1.3.174.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.jcraft--jsch--com.jcraft__jsch__0.1.50.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.jolbox--bonecp--com.jolbox__bonecp__0.8.0.RELEASE.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.microsoft.azure--azure-data-lake-store-sdk--com.microsoft.azure__azure-data-lake-store-sdk__2.2.8.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.microsoft.sqlserver--mssql-jdbc--com.microsoft.sqlserver__mssql-jdbc__6.2.2.jre8.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--commons-beanutils--commons-beanutils--commons-beanutils__commons-beanutils__1.7.0.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--commons-beanutils--commons-beanutils-core--commons-beanutils__commons-beanutils-core__1.8.0.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--commons-cli--commons-cli--commons-cli__commons-cli__1.2.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--commons-codec--commons-codec--commons-codec__commons-codec__1.10.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--commons-collections--commons-collections--commons-collections__commons-collections__3.2.2.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--commons-configuration--commons-configuration--commons-configuration__commons-configuration__1.6.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--commons-dbcp--commons-dbcp--commons-dbcp__commons-dbcp__1.4.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--commons-digester--commons-digester--commons-digester__commons-digester__1.8.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--commons-httpclient--commons-httpclient--commons-httpclient__commons-httpclient__3.1.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--commons-io--commons-io--commons-io__commons-io__2.4.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--commons-lang--commons-lang--commons-lang__commons-lang__2.6.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--commons-logging--commons-logging--commons-logging__commons-logging__1.1.3.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--commons-net--commons-net--commons-net__commons-net__3.1.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--commons-pool--commons-pool--commons-pool__commons-pool__1.5.4.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.ning--compress-lzf--com.ning__compress-lzf__1.0.3.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.sun.mail--javax.mail--com.sun.mail__javax.mail__1.5.2.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.thoughtworks.paranamer--paranamer--com.thoughtworks.paranamer__paranamer__2.8.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.trueaccord.lenses--lenses_2.11--com.trueaccord.lenses__lenses_2.11__0.3.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.twitter--chill_2.11--com.twitter__chill_2.11__0.9.3.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.twitter--chill-java--com.twitter__chill-java__0.9.3.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.twitter--parquet-hadoop-bundle--com.twitter__parquet-hadoop-bundle__1.6.0.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.twitter--util-app_2.11--com.twitter__util-app_2.11__6.23.0.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.twitter--util-core_2.11--com.twitter__util-core_2.11__6.23.0.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.twitter--util-jvm_2.11--com.twitter__util-jvm_2.11__6.23.0.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.typesafe--config--com.typesafe__config__1.2.1.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.typesafe.scala-logging--scala-logging-api_2.11--com.typesafe.scala-logging__scala-logging-api_2.11__2.1.2.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.typesafe.scala-logging--scala-logging-slf4j_2.11--com.typesafe.scala-logging__scala-logging-slf4j_2.11__2.1.2.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.univocity--univocity-parsers--com.univocity__univocity-parsers__2.7.3.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.vlkan--flatbuffers--com.vlkan__flatbuffers__1.2.0-3f79e055.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.zaxxer--HikariCP--com.zaxxer__HikariCP__3.1.0.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--info.ganglia.gmetric4j--gmetric4j--info.ganglia.gmetric4j__gmetric4j__1.0.7.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--io.airlift--aircompressor--io.airlift__aircompressor__0.10.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--io.dropwizard.metrics--metrics-core--io.dropwizard.metrics__metrics-core__3.1.5.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--io.dropwizard.metrics--metrics-ganglia--io.dropwizard.metrics__metrics-ganglia__3.1.5.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--io.dropwizard.metrics--metrics-graphite--io.dropwizard.metrics__metrics-graphite__3.1.5.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--io.dropwizard.metrics--metrics-healthchecks--io.dropwizard.metrics__metrics-healthchecks__3.1.5.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--io.dropwizard.metrics--metrics-jetty9--io.dropwizard.metrics__metrics-jetty9__3.1.5.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--io.dropwizard.metrics--metrics-json--io.dropwizard.metrics__metrics-json__3.1.5.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--io.dropwizard.metrics--metrics-jvm--io.dropwizard.metrics__metrics-jvm__3.1.5.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--io.dropwizard.metrics--metrics-log4j--io.dropwizard.metrics__metrics-log4j__3.1.5.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--io.dropwizard.metrics--metrics-servlets--io.dropwizard.metrics__metrics-servlets__3.1.5.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--io.netty--netty-all--io.netty__netty-all__4.1.17.Final.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--io.netty--netty--io.netty__netty__3.9.9.Final.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--io.prometheus.jmx--collector--io.prometheus.jmx__collector__0.7.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--io.prometheus--simpleclient_common--io.prometheus__simpleclient_common__0.0.16.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--io.prometheus--simpleclient_dropwizard--io.prometheus__simpleclient_dropwizard__0.0.16.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--io.prometheus--simpleclient--io.prometheus__simpleclient__0.0.16.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--io.prometheus--simpleclient_servlet--io.prometheus__simpleclient_servlet__0.0.16.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--javax.activation--activation--javax.activation__activation__1.1.1.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--javax.annotation--javax.annotation-api--javax.annotation__javax.annotation-api__1.2.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--javax.el--javax.el-api--javax.el__javax.el-api__2.2.4.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--javax.jdo--jdo-api--javax.jdo__jdo-api__3.0.1.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--javax.servlet--javax.servlet-api--javax.servlet__javax.servlet-api__3.1.0.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--javax.servlet.jsp--jsp-api--javax.servlet.jsp__jsp-api__2.1.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--javax.transaction--jta--javax.transaction__jta__1.1.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--javax.validation--validation-api--javax.validation__validation-api__1.1.0.Final.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--javax.ws.rs--javax.ws.rs-api--javax.ws.rs__javax.ws.rs-api__2.0.1.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--javax.xml.bind--jaxb-api--javax.xml.bind__jaxb-api__2.2.2.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--javax.xml.stream--stax-api--javax.xml.stream__stax-api__1.0-2.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--javolution--javolution--javolution__javolution__5.5.1.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--jline--jline--jline__jline__2.11.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--joda-time--joda-time--joda-time__joda-time__2.9.3.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--log4j--apache-log4j-extras--log4j__apache-log4j-extras__1.2.17.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--log4j--log4j--log4j__log4j__1.2.17.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--net.hydromatic--eigenbase-properties--net.hydromatic__eigenbase-properties__1.1.5.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--net.razorvine--pyrolite--net.razorvine__pyrolite__4.13.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--net.sf.jpam--jpam--net.sf.jpam__jpam__1.1.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--net.sf.opencsv--opencsv--net.sf.opencsv__opencsv__2.3.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--net.sf.supercsv--super-csv--net.sf.supercsv__super-csv__2.2.0.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--net.snowflake--snowflake-ingest-sdk--net.snowflake__snowflake-ingest-sdk__0.9.5.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--net.snowflake--snowflake-jdbc--net.snowflake__snowflake-jdbc__3.6.15.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--net.snowflake--spark-snowflake_2.11--net.snowflake__spark-snowflake_2.11__2.4.10-spark_2.4.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--net.sourceforge.f2j--arpack_combined_all--net.sourceforge.f2j__arpack_combined_all__0.1.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.acplt--oncrpc--org.acplt__oncrpc__1.0.7.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.antlr--antlr4-runtime--org.antlr__antlr4-runtime__4.7.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.antlr--antlr-runtime--org.antlr__antlr-runtime__3.4.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.antlr--ST4--org.antlr__ST4__4.0.4.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.antlr--stringtemplate--org.antlr__stringtemplate__3.2.1.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.apache.ant--ant-jsch--org.apache.ant__ant-jsch__1.9.2.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.apache.ant--ant-launcher--org.apache.ant__ant-launcher__1.9.2.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.apache.ant--ant--org.apache.ant__ant__1.9.2.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.apache.arrow--arrow-format--org.apache.arrow__arrow-format__0.10.0.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.apache.arrow--arrow-memory--org.apache.arrow__arrow-memory__0.10.0.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.apache.arrow--arrow-vector--org.apache.arrow__arrow-vector__0.10.0.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.apache.avro--avro-ipc--org.apache.avro__avro-ipc__1.8.2.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.apache.avro--avro-mapred-hadoop2--org.apache.avro__avro-mapred-hadoop2__1.8.2.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.apache.avro--avro--org.apache.avro__avro__1.8.2.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.apache.calcite--calcite-avatica--org.apache.calcite__calcite-avatica__1.2.0-incubating.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.apache.calcite--calcite-core--org.apache.calcite__calcite-core__1.2.0-incubating.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.apache.calcite--calcite-linq4j--org.apache.calcite__calcite-linq4j__1.2.0-incubating.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.apache.commons--commons-compress--org.apache.commons__commons-compress__1.8.1.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.apache.commons--commons-crypto--org.apache.commons__commons-crypto__1.0.0.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.apache.commons--commons-lang3--org.apache.commons__commons-lang3__3.5.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.apache.commons--commons-math3--org.apache.commons__commons-math3__3.4.1.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.apache.curator--curator-client--org.apache.curator__curator-client__2.7.1.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.apache.curator--curator-framework--org.apache.curator__curator-framework__2.7.1.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.apache.curator--curator-recipes--org.apache.curator__curator-recipes__2.7.1.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.apache.derby--derby--org.apache.derby__derby__10.12.1.1.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.apache.directory.api--api-asn1-api--org.apache.directory.api__api-asn1-api__1.0.0-M20.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.apache.directory.api--api-util--org.apache.directory.api__api-util__1.0.0-M20.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.apache.directory.server--apacheds-i18n--org.apache.directory.server__apacheds-i18n__2.0.0-M15.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.apache.directory.server--apacheds-kerberos-codec--org.apache.directory.server__apacheds-kerberos-codec__2.0.0-M15.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.apache.hadoop--hadoop-annotations--org.apache.hadoop__hadoop-annotations__2.7.3.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.apache.hadoop--hadoop-auth--org.apache.hadoop__hadoop-auth__2.7.3.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.apache.hadoop--hadoop-client--org.apache.hadoop__hadoop-client__2.7.3.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.apache.hadoop--hadoop-common--org.apache.hadoop__hadoop-common__2.7.3.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.apache.hadoop--hadoop-hdfs--org.apache.hadoop__hadoop-hdfs__2.7.3.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.apache.hadoop--hadoop-mapreduce-client-app--org.apache.hadoop__hadoop-mapreduce-client-app__2.7.3.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.apache.hadoop--hadoop-mapreduce-client-common--org.apache.hadoop__hadoop-mapreduce-client-common__2.7.3.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.apache.hadoop--hadoop-mapreduce-client-core--org.apache.hadoop__hadoop-mapreduce-client-core__2.7.3.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.apache.hadoop--hadoop-mapreduce-client-jobclient--org.apache.hadoop__hadoop-mapreduce-client-jobclient__2.7.3.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.apache.hadoop--hadoop-mapreduce-client-shuffle--org.apache.hadoop__hadoop-mapreduce-client-shuffle__2.7.3.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.apache.hadoop--hadoop-yarn-api--org.apache.hadoop__hadoop-yarn-api__2.7.3.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.apache.hadoop--hadoop-yarn-client--org.apache.hadoop__hadoop-yarn-client__2.7.3.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.apache.hadoop--hadoop-yarn-common--org.apache.hadoop__hadoop-yarn-common__2.7.3.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.apache.hadoop--hadoop-yarn-server-common--org.apache.hadoop__hadoop-yarn-server-common__2.7.3.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.apache.htrace--htrace-core--org.apache.htrace__htrace-core__3.1.0-incubating.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.apache.httpcomponents--httpclient--org.apache.httpcomponents__httpclient__4.5.4.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.apache.httpcomponents--httpcore--org.apache.httpcomponents__httpcore__4.4.8.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.apache.ivy--ivy--org.apache.ivy__ivy__2.4.0.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.apache.orc--orc-core-nohive--org.apache.orc__orc-core-nohive__1.5.2.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.apache.orc--orc-mapreduce-nohive--org.apache.orc__orc-mapreduce-nohive__1.5.2.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.apache.orc--orc-shims--org.apache.orc__orc-shims__1.5.2.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.apache.parquet--parquet-column--org.apache.parquet__parquet-column__1.10.1-databricks3.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.apache.parquet--parquet-common--org.apache.parquet__parquet-common__1.10.1-databricks3.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.apache.parquet--parquet-encoding--org.apache.parquet__parquet-encoding__1.10.1-databricks3.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.apache.parquet--parquet-format--org.apache.parquet__parquet-format__2.4.0.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.apache.parquet--parquet-hadoop--org.apache.parquet__parquet-hadoop__1.10.1-databricks3.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.apache.parquet--parquet-jackson--org.apache.parquet__parquet-jackson__1.10.1-databricks3.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.apache.thrift--libfb303--org.apache.thrift__libfb303__0.9.3.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.apache.thrift--libthrift--org.apache.thrift__libthrift__0.9.3.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.apache.xbean--xbean-asm6-shaded--org.apache.xbean__xbean-asm6-shaded__4.8.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.apache.zookeeper--zookeeper--org.apache.zookeeper__zookeeper__3.4.6.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.codehaus.jackson--jackson-core-asl--org.codehaus.jackson__jackson-core-asl__1.9.13.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.codehaus.jackson--jackson-jaxrs--org.codehaus.jackson__jackson-jaxrs__1.9.13.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.codehaus.jackson--jackson-mapper-asl--org.codehaus.jackson__jackson-mapper-asl__1.9.13.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.codehaus.jackson--jackson-xc--org.codehaus.jackson__jackson-xc__1.9.13.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.codehaus.janino--commons-compiler--org.codehaus.janino__commons-compiler__3.0.10.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.codehaus.janino--janino--org.codehaus.janino__janino__3.0.10.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.datanucleus--datanucleus-api-jdo--org.datanucleus__datanucleus-api-jdo__3.2.6.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.datanucleus--datanucleus-core--org.datanucleus__datanucleus-core__3.2.10.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.datanucleus--datanucleus-rdbms--org.datanucleus__datanucleus-rdbms__3.2.9.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-client--org.eclipse.jetty__jetty-client__9.3.20.v20170531.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-continuation--org.eclipse.jetty__jetty-continuation__9.3.20.v20170531.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-http--org.eclipse.jetty__jetty-http__9.3.20.v20170531.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-io--org.eclipse.jetty__jetty-io__9.3.20.v20170531.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-jndi--org.eclipse.jetty__jetty-jndi__9.3.20.v20170531.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-plus--org.eclipse.jetty__jetty-plus__9.3.20.v20170531.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-proxy--org.eclipse.jetty__jetty-proxy__9.3.20.v20170531.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-security--org.eclipse.jetty__jetty-security__9.3.20.v20170531.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-server--org.eclipse.jetty__jetty-server__9.3.20.v20170531.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-servlet--org.eclipse.jetty__jetty-servlet__9.3.20.v20170531.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-servlets--org.eclipse.jetty__jetty-servlets__9.3.20.v20170531.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-util--org.eclipse.jetty__jetty-util__9.3.20.v20170531.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-webapp--org.eclipse.jetty__jetty-webapp__9.3.20.v20170531.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-xml--org.eclipse.jetty__jetty-xml__9.3.20.v20170531.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.fusesource.leveldbjni--leveldbjni-all--org.fusesource.leveldbjni__leveldbjni-all__1.8.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.glassfish.hk2.external--aopalliance-repackaged--org.glassfish.hk2.external__aopalliance-repackaged__2.4.0-b34.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.glassfish.hk2.external--javax.inject--org.glassfish.hk2.external__javax.inject__2.4.0-b34.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.glassfish.hk2--hk2-api--org.glassfish.hk2__hk2-api__2.4.0-b34.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.glassfish.hk2--hk2-locator--org.glassfish.hk2__hk2-locator__2.4.0-b34.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.glassfish.hk2--hk2-utils--org.glassfish.hk2__hk2-utils__2.4.0-b34.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.glassfish.hk2--osgi-resource-locator--org.glassfish.hk2__osgi-resource-locator__1.0.1.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.glassfish.jersey.bundles.repackaged--jersey-guava--org.glassfish.jersey.bundles.repackaged__jersey-guava__2.22.2.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.glassfish.jersey.containers--jersey-container-servlet-core--org.glassfish.jersey.containers__jersey-container-servlet-core__2.22.2.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.glassfish.jersey.containers--jersey-container-servlet--org.glassfish.jersey.containers__jersey-container-servlet__2.22.2.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.glassfish.jersey.core--jersey-client--org.glassfish.jersey.core__jersey-client__2.22.2.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.glassfish.jersey.core--jersey-common--org.glassfish.jersey.core__jersey-common__2.22.2.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.glassfish.jersey.core--jersey-server--org.glassfish.jersey.core__jersey-server__2.22.2.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.glassfish.jersey.media--jersey-media-jaxb--org.glassfish.jersey.media__jersey-media-jaxb__2.22.2.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.hibernate--hibernate-validator--org.hibernate__hibernate-validator__5.1.1.Final.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.iq80.snappy--snappy--org.iq80.snappy__snappy__0.2.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.javassist--javassist--org.javassist__javassist__3.18.1-GA.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.jboss.logging--jboss-logging--org.jboss.logging__jboss-logging__3.1.3.GA.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.jdbi--jdbi--org.jdbi__jdbi__2.63.1.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.joda--joda-convert--org.joda__joda-convert__1.7.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.jodd--jodd-core--org.jodd__jodd-core__3.5.2.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.json4s--json4s-ast_2.11--org.json4s__json4s-ast_2.11__3.5.3.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.json4s--json4s-core_2.11--org.json4s__json4s-core_2.11__3.5.3.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.json4s--json4s-jackson_2.11--org.json4s__json4s-jackson_2.11__3.5.3.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.json4s--json4s-scalap_2.11--org.json4s__json4s-scalap_2.11__3.5.3.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.lz4--lz4-java--org.lz4__lz4-java__1.4.0.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.mariadb.jdbc--mariadb-java-client--org.mariadb.jdbc__mariadb-java-client__2.1.2.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.mockito--mockito-all--org.mockito__mockito-all__1.9.5.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.objenesis--objenesis--org.objenesis__objenesis__2.5.1.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.postgresql--postgresql--org.postgresql__postgresql__42.1.4.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.roaringbitmap--RoaringBitmap--org.roaringbitmap__RoaringBitmap__0.5.11.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.rocksdb--rocksdbjni--org.rocksdb__rocksdbjni__5.2.1.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.rosuda.REngine--REngine--org.rosuda.REngine__REngine__2.1.0.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.scalacheck--scalacheck_2.11--org.scalacheck__scalacheck_2.11__1.12.5.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.scalactic--scalactic_2.11--org.scalactic__scalactic_2.11__3.0.3.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.scala-lang.modules--scala-parser-combinators_2.11--org.scala-lang.modules__scala-parser-combinators_2.11__1.0.2.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.scala-lang.modules--scala-xml_2.11--org.scala-lang.modules__scala-xml_2.11__1.0.5.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.scala-lang--scala-compiler_2.11--org.scala-lang__scala-compiler__2.11.8.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.scala-lang--scala-library_2.11--org.scala-lang__scala-library__2.11.8.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.scala-lang--scala-reflect_2.11--org.scala-lang__scala-reflect__2.11.8.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.scalanlp--breeze_2.11--org.scalanlp__breeze_2.11__0.13.2.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.scalanlp--breeze-macros_2.11--org.scalanlp__breeze-macros_2.11__0.13.2.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.scala-sbt--test-interface--org.scala-sbt__test-interface__1.0.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.scalatest--scalatest_2.11--org.scalatest__scalatest_2.11__3.0.3.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.slf4j--jcl-over-slf4j--org.slf4j__jcl-over-slf4j__1.7.16.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.slf4j--jul-to-slf4j--org.slf4j__jul-to-slf4j__1.7.16.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.slf4j--slf4j-api--org.slf4j__slf4j-api__1.7.16.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.slf4j--slf4j-log4j12--org.slf4j__slf4j-log4j12__1.7.16.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.spark-project.hive--hive-beeline--org.spark-project.hive__hive-beeline__1.2.1.spark2.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.spark-project.hive--hive-cli--org.spark-project.hive__hive-cli__1.2.1.spark2.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.spark-project.hive--hive-exec--org.spark-project.hive__hive-exec__1.2.1.spark2.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.spark-project.hive--hive-jdbc--org.spark-project.hive__hive-jdbc__1.2.1.spark2.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.spark-project.hive--hive-metastore--org.spark-project.hive__hive-metastore__1.2.1.spark2.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.spark-project.spark--unused--org.spark-project.spark__unused__1.0.0.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.spire-math--spire_2.11--org.spire-math__spire_2.11__0.13.0.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.spire-math--spire-macros_2.11--org.spire-math__spire-macros_2.11__0.13.0.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.springframework--spring-core--org.springframework__spring-core__4.1.4.RELEASE.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.springframework--spring-test--org.springframework__spring-test__4.1.4.RELEASE.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.tukaani--xz--org.tukaani__xz__1.5.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.typelevel--machinist_2.11--org.typelevel__machinist_2.11__0.6.1.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.typelevel--macro-compat_2.11--org.typelevel__macro-compat_2.11__1.1.1.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.xerial.snappy--snappy-java--org.xerial.snappy__snappy-java__1.1.7.1.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.xerial--sqlite-jdbc--org.xerial__sqlite-jdbc__3.8.11.2.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.yaml--snakeyaml--org.yaml__snakeyaml__1.16.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--oro--oro--oro__oro__2.0.8.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--software.amazon.ion--ion-java--software.amazon.ion__ion-java__1.0.2.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--stax--stax-api--stax__stax-api__1.0.1.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--xmlenc--xmlenc--xmlenc__xmlenc__0.52.jar:
    /databricks/jars/spark--sql-extension--sql-extension-spark_2.4_2.11_deploy.jar:
    /databricks/jars/spark--versions--2.4--avro_2.11_deploy_shaded.jar:
    /databricks/jars/spark--versions--2.4--catalyst_2.11_deploy.jar:
    /databricks/jars/spark--versions--2.4--core_2.11_deploy.jar:
    /databricks/jars/spark--versions--2.4--ganglia-lgpl_2.11_deploy.jar:
    /databricks/jars/spark--versions--2.4--graphx_2.11_deploy.jar:
    /databricks/jars/spark--versions--2.4--hive_2.11_deploy_shaded.jar:
    /databricks/jars/spark--versions--2.4--hive-thriftserver_2.11_deploy.jar:
    /databricks/jars/spark--versions--2.4--io.confluent__common-utils__4.0.0_shaded-for-avro.jar:
    /databricks/jars/spark--versions--2.4--io.confluent__kafka-schema-registry-client__4.0.0_shaded-for-avro.jar:
    /databricks/jars/spark--versions--2.4--kafka_2.11_only_kafka08_shaded.jar:
    /databricks/jars/spark--versions--2.4--kafka-clients_only_kafka08_shaded.jar:
    /databricks/jars/spark--versions--2.4--kafka-clients_only_shaded.jar:
    /databricks/jars/spark--versions--2.4--libspark-sql-parser-compiled.jar:
    /databricks/jars/spark--versions--2.4--metrics-core_only_kafka08_shaded.jar:
    /databricks/jars/spark--versions--2.4--mllib_2.11_deploy_shaded.jar:
    /databricks/jars/spark--versions--2.4--mllib-local_2.11_deploy.jar:
    /databricks/jars/spark--versions--2.4--org.apache.commons__commons-pool2__2.5.0_shaded-for-hive.jar:
    /databricks/jars/spark--versions--2.4--org.jpmml__pmml-model__1.2.15_shaded-for-mllib.jar:
    /databricks/jars/spark--versions--2.4--org.jpmml__pmml-schema__1.2.15_shaded-for-mllib.jar:
    /databricks/jars/spark--versions--2.4--py4j_2.11_deploy.jar:
    /databricks/jars/spark--versions--2.4--redshift_2.11_deploy.jar:
    /databricks/jars/spark--versions--2.4--repl_2.11_deploy.jar:
    /databricks/jars/spark--versions--2.4--shim_2.11_deploy.jar:
    /databricks/jars/spark--versions--2.4--spark_2.11_deploy.jar:
    /databricks/jars/spark--versions--2.4--spark-2.4-avro-resources-jar-resources.jar:
    /databricks/jars/spark--versions--2.4--spark-2.4-core-resources-jar-resources.jar:
    /databricks/jars/spark--versions--2.4--spark-2.4-hive-resources-jar-resources.jar:
    /databricks/jars/spark--versions--2.4--spark-2.4-mllib-resources-jar-resources.jar:
    /databricks/jars/spark--versions--2.4--spark-2.4-spark-sql-aws-resources-jar-resources.jar:
    /databricks/jars/spark--versions--2.4--spark-2.4-spark-sql-azure-resources-jar-resources.jar:
    /databricks/jars/spark--versions--2.4--spark-2.4-spark-sql-kafka-0-10-resources-jar-resources.jar:
    /databricks/jars/spark--versions--2.4--spark-2.4-spark-sql-kafka-0-8-resources-jar-resources.jar:
    /databricks/jars/spark--versions--2.4--spark-2.4-sql-resources-jar-resources.jar:
    /databricks/jars/spark--versions--2.4--spark-2.4-streaming-resources-jar-resources.jar:
    /databricks/jars/spark--versions--2.4--spark-sql-aws_2.11_deploy.jar:
    /databricks/jars/spark--versions--2.4--spark-sql-azure_2.11_deploy.jar:
    /databricks/jars/spark--versions--2.4--spark-sql-kafka-0-10_2.11_deploy_shaded.jar:
    /databricks/jars/spark--versions--2.4--spark-sql-kafka-0-8_2.11_deploy_shaded.jar:
    /databricks/jars/spark--versions--2.4--spark-sql-notification_2.11_deploy.jar:
    /databricks/jars/spark--versions--2.4--sql_2.11_deploy.jar:
    /databricks/jars/spark--versions--2.4--sqldw_2.11_deploy.jar:
    /databricks/jars/spark--versions--2.4--streaming_2.11_deploy.jar:
    /databricks/jars/spark--versions--2.4--tags_2.11_deploy.jar:
    /databricks/jars/third_party--azure--com.fasterxml.jackson.core__jackson-core__2.7.2_shaded.jar:
    /databricks/jars/third_party--azure--com.microsoft.azure__azure-keyvault-core__1.0.0_shaded.jar:
    /databricks/jars/third_party--azure--com.microsoft.azure__azure-storage__5.2.0_shaded.jar:
    /databricks/jars/third_party--azure--org.apache.commons__commons-lang3__3.4_shaded.jar:
    /databricks/jars/third_party--datalake--datalake-spark_2.4_2.11_deploy.jar:
    /databricks/jars/third_party--hadoop-azure-2.7.3-abfs--aopalliance__aopalliance__1.0_2.11_shaded_20180625_3682417.jar:
    /databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.fasterxml.jackson.core__jackson-annotations__2.7.0_2.11_shaded_20180625_3682417.jar:
    /databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.fasterxml.jackson.core__jackson-core__2.7.2_2.11_shaded_20180625_3682417.jar:
    /databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.fasterxml.jackson.core__jackson-core__2.7.2_2.11_shaded_20180920_b33d810.jar:
    /databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.fasterxml.jackson.core__jackson-databind__2.7.2_2.11_shaded_20180625_3682417.jar:
    /databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.fasterxml.jackson.datatype__jackson-datatype-joda__2.7.2_2.11_shaded_20180625_3682417.jar:
    /databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.google.code.findbugs__jsr305__1.3.9_2.11_shaded_20180920_b33d810.jar:
    /databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.google.guava__guava__11.0.2_2.11_shaded_20180920_b33d810.jar:
    /databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.google.guava__guava__16.0_2.11_shaded_20180625_3682417.jar:
    /databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.google.inject__guice__3.0_2.11_shaded_20180625_3682417.jar:
    /databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.microsoft.azure__azure-annotations__1.2.0_2.11_shaded_20180625_3682417.jar:
    /databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.microsoft.azure__azure-keyvault-core__1.0.0_2.11_shaded_20180920_b33d810.jar:
    /databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.microsoft.azure__azure-storage__7.0.0_2.11_shaded_20180920_b33d810.jar:
    /databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.microsoft.rest__client-runtime__1.1.0_2.11_shaded_20180625_3682417.jar:
    /databricks/jars/third_party--hadoop-azure-2.7.3-abfs--commons-codec__commons-codec__1.9_2.11_shaded_20180625_3682417.jar:
    /databricks/jars/third_party--hadoop-azure-2.7.3-abfs--commons-codec__commons-codec__1.9_2.11_shaded_20180920_b33d810.jar:
    /databricks/jars/third_party--hadoop-azure-2.7.3-abfs--commons-logging__commons-logging__1.2_2.11_shaded_20180625_3682417.jar:
    /databricks/jars/third_party--hadoop-azure-2.7.3-abfs--commons-logging__commons-logging__1.2_2.11_shaded_20180920_b33d810.jar:
    /databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.squareup.okhttp3__logging-interceptor__3.3.1_2.11_shaded_20180625_3682417.jar:
    /databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.squareup.okhttp3__okhttp__3.3.1_2.11_shaded_20180625_3682417.jar:
    /databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.squareup.okhttp3__okhttp-urlconnection__3.3.1_2.11_shaded_20180625_3682417.jar:
    /databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.squareup.okio__okio__1.8.0_2.11_shaded_20180625_3682417.jar:
    /databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.squareup.retrofit2__adapter-rxjava__2.1.0_2.11_shaded_20180625_3682417.jar:
    /databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.squareup.retrofit2__converter-jackson__2.1.0_2.11_shaded_20180625_3682417.jar:
    /databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.squareup.retrofit2__retrofit__2.1.0_2.11_shaded_20180625_3682417.jar:
    /databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.sun.xml.bind__jaxb-impl__2.2.3-1_2.11_shaded_20180625_3682417.jar:
    /databricks/jars/third_party--hadoop-azure-2.7.3-abfs--hadoop-azure-2.7.3-abfs-external-20180625_3682417-spark_2.4_2.11_deploy_shaded.jar:
    /databricks/jars/third_party--hadoop-azure-2.7.3-abfs--hadoop-azure-2.7.3-abfs-external-20180920_b33d810-spark_2.4_2.11_deploy_shaded.jar:
    /databricks/jars/third_party--hadoop-azure-2.7.3-abfs--io.netty__netty-all__4.0.52.Final_2.11_shaded_20180625_3682417.jar:
    /databricks/jars/third_party--hadoop-azure-2.7.3-abfs--io.reactivex__rxjava__1.2.4_2.11_shaded_20180625_3682417.jar:
    /databricks/jars/third_party--hadoop-azure-2.7.3-abfs--javax.activation__activation__1.1_2.11_shaded_20180625_3682417.jar:
    /databricks/jars/third_party--hadoop-azure-2.7.3-abfs--javax.inject__javax.inject__1_2.11_shaded_20180625_3682417.jar:
    /databricks/jars/third_party--hadoop-azure-2.7.3-abfs--javax.xml.bind__jaxb-api__2.2.2_2.11_shaded_20180625_3682417.jar:
    /databricks/jars/third_party--hadoop-azure-2.7.3-abfs--javax.xml.stream__stax-api__1.0-2_2.11_shaded_20180625_3682417.jar:
    /databricks/jars/third_party--hadoop-azure-2.7.3-abfs--joda-time__joda-time__2.4_2.11_shaded_20180625_3682417.jar:
    /databricks/jars/third_party--hadoop-azure-2.7.3-abfs--org.apache.htrace__htrace-core__3.1.0-incubating_2.11_shaded_20180625_3682417.jar:
    /databricks/jars/third_party--hadoop-azure-2.7.3-abfs--org.apache.httpcomponents__httpclient__4.5.2_2.11_shaded_20180625_3682417.jar:
    /databricks/jars/third_party--hadoop-azure-2.7.3-abfs--org.apache.httpcomponents__httpclient__4.5.2_2.11_shaded_20180920_b33d810.jar:
    /databricks/jars/third_party--hadoop-azure-2.7.3-abfs--org.apache.httpcomponents__httpcore__4.4.4_2.11_shaded_20180625_3682417.jar:
    /databricks/jars/third_party--hadoop-azure-2.7.3-abfs--org.apache.httpcomponents__httpcore__4.4.4_2.11_shaded_20180920_b33d810.jar:
    /databricks/jars/third_party--jackson--guava_only_shaded.jar:
    /databricks/jars/third_party--jackson--jackson-module-scala-shaded_2.11_deploy.jar:
    /databricks/jars/third_party--jackson--jsr305_only_shaded.jar:
    /databricks/jars/third_party--jackson--paranamer_only_shaded.jar:
    /databricks/jars/third_party--jetty8-shaded-client--databricks-patched-jetty-client-jar_shaded.jar:
    /databricks/jars/third_party--jetty8-shaded-client--databricks-patched-jetty-http-jar_shaded.jar:
    /databricks/jars/third_party--jetty8-shaded-client--jetty-io_shaded.jar:
    /databricks/jars/third_party--jetty8-shaded-client--jetty-jmx_shaded.jar:
    /databricks/jars/third_party--jetty8-shaded-client--jetty-util_shaded.jar:
    /databricks/jars/utils--process_utils-spark_2.4_2.11_deploy.jar:
    /databricks/jars/workflow--workflow-spark_2.4_2.11_deploy.jar:
spark.executor.extraJavaOptions=-Djava.io.tmpdir=/local_disk0/tmp -XX:ReservedCodeCacheSize=256m -XX:+UseCodeCacheFlushing -Djava.security.properties=/databricks/spark/dbconf/java/extra.security -XX:+PrintFlagsFinal -XX:+PrintGCDateStamps -verbose:gc -XX:+PrintGCDetails -Xss4m -Djavax.xml.datatype.DatatypeFactory=com.sun.org.apache.xerces.internal.jaxp.datatype.DatatypeFactoryImpl -Djavax.xml.parsers.DocumentBuilderFactory=com.sun.org.apache.xerces.internal.jaxp.DocumentBuilderFactoryImpl -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -Djavax.xml.validation.SchemaFactory:http://www.w3.org/2001/XMLSchema=com.sun.org.apache.xerces.internal.jaxp.validation.XMLSchemaFactory -Dorg.xml.sax.driver=com.sun.org.apache.xerces.internal.parsers.SAXParser -Dorg.w3c.dom.DOMImplementationSourceList=com.sun.org.apache.xerces.internal.dom.DOMXSImplementationSourceImpl -Ddatabricks.serviceName=spark-executor-1
spark.executor.id=driver
spark.executor.memory=97871m
spark.executor.tempDirectory=/local_disk0/tmp
spark.extraListeners=com.databricks.backend.daemon.driver.DBCEventLoggingListener
spark.files.fetchFailure.unRegisterOutputOnHost=true
spark.files.overwrite=true
spark.files.useFetchCache=false
spark.hadoop.databricks.dbfs.client.version=v2
spark.hadoop.databricks.s3commit.client.sslTrustAll=false
spark.hadoop.datanucleus.connectionPool.maxIdle=1
spark.hadoop.datanucleus.connectionPool.maxPoolSize=30
spark.hadoop.datanucleus.connectionPool.minPoolSize=15
spark.hadoop.dfs.adls.oauth2.access.token.provider.type=ClientCredential
spark.hadoop.dfs.adls.oauth2.client.id=003106c3-ff8c-4c62-a060-24b8b32ea512
spark.hadoop.dfs.adls.oauth2.credential=viUHaEfNg8cHQRJSRNMzpIv1T6goiUg+c9R/XSbYBFA=
spark.hadoop.dfs.adls.oauth2.refresh.url=https://login.microsoftonline.com/6ac7a1f4-5fb1-4153-bb4f-12d2020a1f7d/oauth2/token
spark.hadoop.fs.abfs.impl=shaded.databricks.v20180920_b33d810.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem
spark.hadoop.fs.abfs.impl.disable.cache=true
spark.hadoop.fs.abfss.impl=shaded.databricks.v20180920_b33d810.org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem
spark.hadoop.fs.abfss.impl.disable.cache=true
spark.hadoop.fs.adl.impl=com.databricks.adl.AdlFileSystem
spark.hadoop.fs.adl.impl.disable.cache=false
spark.hadoop.fs.azure.skip.metrics=true
spark.hadoop.fs.s3.impl=com.databricks.s3a.S3AFileSystem
spark.hadoop.fs.s3a.connection.maximum=200
spark.hadoop.fs.s3a.fast.upload=true
spark.hadoop.fs.s3a.fast.upload.default=true
spark.hadoop.fs.s3a.impl=com.databricks.s3a.S3AFileSystem
spark.hadoop.fs.s3a.multipart.size=10485760
spark.hadoop.fs.s3a.multipart.threshold=104857600
spark.hadoop.fs.s3a.threads.max=136
spark.hadoop.fs.s3n.impl=com.databricks.s3a.S3AFileSystem
spark.hadoop.fs.wasb.impl=shaded.databricks.org.apache.hadoop.fs.azure.NativeAzureFileSystem
spark.hadoop.fs.wasb.impl.disable.cache=true
spark.hadoop.fs.wasbs.impl=shaded.databricks.org.apache.hadoop.fs.azure.NativeAzureFileSystem
spark.hadoop.fs.wasbs.impl.disable.cache=true
spark.hadoop.hive.server2.idle.operation.timeout=7200000
spark.hadoop.hive.server2.idle.session.timeout=900000
spark.hadoop.hive.server2.keystore.password=gb1gQqZ9ZIHS
spark.hadoop.hive.server2.keystore.path=
    /databricks/keys/jetty-ssl-driver-keystore.jks:
spark.hadoop.hive.server2.session.check.interval=60000
spark.hadoop.hive.server2.thrift.http.port=10000
spark.hadoop.hive.server2.transport.mode=http
spark.hadoop.hive.server2.use.SSL=true
spark.hadoop.hive.warehouse.subdir.inherit.perms=false
spark.hadoop.javax.jdo.option.ConnectionDriverName=org.postgresql.Driver
spark.hadoop.javax.jdo.option.ConnectionPassword=560I890@fd80709
spark.hadoop.javax.jdo.option.ConnectionURL=jdbc:postgresql://uathmsdb.trans.enterprisecsp.net:5432/hive_adls
spark.hadoop.javax.jdo.option.ConnectionUserName=tedbuat
spark.hadoop.mapred.output.committer.class=com.databricks.backend.daemon.data.client.DirectOutputCommitter
spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version=2
spark.hadoop.parquet.memory.pool.ratio=0.5
spark.hadoop.spark.sql.parquet.output.committer.class=org.apache.spark.sql.parquet.DirectParquetOutputCommitter
spark.hadoop.spark.sql.sources.outputCommitterClass=com.databricks.backend.daemon.data.client.MapReduceDirectOutputCommitter
spark.hadoop.spark.thriftserver.customHeadersToProperties=X-Databricks-User-Token:spark.databricks.token,X-Databricks-Api-Url:spark.databricks.api.url
spark.home=/databricks/spark
spark.logConf=true
spark.master=spark://10.139.64.31:7077
spark.metrics.conf=/databricks/spark/conf/metrics.properties
spark.r.backendConnectionTimeout=604800
spark.r.numRBackendThreads=1
spark.r.sql.derby.temp.dir=/tmp/RtmprDP5MD
spark.rdd.compress=true
spark.repl.class.outputDir=/local_disk0/tmp/repl/spark-617644649427700606-2741f5b8-99ac-4643-b73d-fe451ffd09c9
spark.repl.class.uri=spark://10.139.64.31:40667/classes
spark.scheduler.listenerbus.eventqueue.size=20000
spark.scheduler.mode=FAIR
spark.serializer.objectStreamReset=100
spark.shuffle.manager=SORT
spark.shuffle.memoryFraction=0.2
spark.shuffle.reduceLocality.enabled=false
spark.shuffle.service.enabled=true
spark.shuffle.service.port=4048
spark.sparkr.use.daemon=false
spark.speculation=false
spark.speculation.multiplier=3
spark.speculation.quantile=0.9
spark.sql.allowMultipleContexts=false
spark.sql.catalogImplementation=hive
spark.sql.hive.convertCTAS=true
spark.sql.hive.convertMetastoreParquet=true
spark.sql.hive.metastore.jars=maven
spark.sql.hive.metastore.sharedPrefixes=org.mariadb.jdbc,com.mysql.jdbc,org.postgresql,com.microsoft.sqlserver,microsoft.sql.DateTimeOffset,microsoft.sql.Types,com.databricks,com.codahale,com.fasterxml.jackson,shaded.databricks
spark.sql.hive.metastore.version=2.3.0
spark.sql.parquet.cacheMetadata=true
spark.sql.parquet.compression.codec=snappy
spark.sql.parquet.filterPushdown=false
spark.sql.parquet.output.committer.class=org.apache.spark.sql.parquet.DirectParquetOutputCommitter
spark.sql.shuffle.partitions=2000
spark.sql.sources.commitProtocolClass=com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol
spark.sql.sources.outputCommitterClass=com.databricks.backend.daemon.data.client.MapReduceDirectOutputCommitter
spark.sql.streaming.checkpointFileManagerClass=com.databricks.spark.sql.streaming.DatabricksCheckpointFileManager
spark.sql.ui.retainedExecutions=100
spark.sql.warehouse.dir=adl://connectedsystemsuatdl.azuredatalakestore.net/user/hive/warehouse
spark.storage.blockManagerTimeoutIntervalMs=300000
spark.storage.memoryFraction=0.5
spark.streaming.driver.writeAheadLog.allowBatching=true
spark.streaming.driver.writeAheadLog.closeFileAfterWrite=true
spark.task.maxFailures=10
spark.task.reaper.enabled=true
spark.task.reaper.killTimeout=60s
spark.thriftserver.customHeadersToProperties=X-Databricks-User-Token:spark.databricks.token,X-Databricks-Api-Url:spark.databricks.api.url
spark.ui.port=46241
spark.worker.cleanup.enabled=false
stream.stderr.reporter.enabled=true
stream.stderr.reporter.prefix=reporter:
tfile.fs.input.buffer.size=262144
tfile.fs.output.buffer.size=262144
tfile.io.chunk.size=1048576
yarn.acl.enable=false
yarn.admin.acl=*
yarn.am.liveness-monitor.expiry-interval-ms=600000
yarn.app.mapreduce.am.command-opts=-Xmx1024m
yarn.app.mapreduce.am.container.log.backups=0
yarn.app.mapreduce.am.container.log.limit.kb=0
yarn.app.mapreduce.am.containerlauncher.threadpool-initial-size=10
yarn.app.mapreduce.am.hard-kill-timeout-ms=10000
yarn.app.mapreduce.am.job.committer.cancel-timeout=60000
yarn.app.mapreduce.am.job.committer.commit-window=10000
yarn.app.mapreduce.am.job.task.listener.thread-count=30
yarn.app.mapreduce.am.resource.cpu-vcores=1
yarn.app.mapreduce.am.resource.mb=1536
yarn.app.mapreduce.am.scheduler.heartbeat.interval-ms=1000
yarn.app.mapreduce.am.staging-dir=/tmp/hadoop-yarn/staging
yarn.app.mapreduce.client-am.ipc.max-retries=3
yarn.app.mapreduce.client-am.ipc.max-retries-on-timeouts=3
yarn.app.mapreduce.client.job.max-retries=0
yarn.app.mapreduce.client.job.retry-interval=2000
yarn.app.mapreduce.client.max-retries=3
yarn.app.mapreduce.shuffle.log.backups=0
yarn.app.mapreduce.shuffle.log.limit.kb=0
yarn.app.mapreduce.shuffle.log.separate=true
yarn.app.mapreduce.task.container.log.backups=0
yarn.bin.path=
    yarn:
yarn.client.application-client-protocol.poll-interval-ms=200
yarn.client.failover-proxy-provider=org.apache.hadoop.yarn.client.ConfiguredRMFailoverProxyProvider
yarn.client.failover-retries=0
yarn.client.failover-retries-on-socket-timeouts=0
yarn.client.max-cached-nodemanagers-proxies=0
yarn.client.nodemanager-client-async.thread-pool-max-size=500
yarn.client.nodemanager-connect.max-wait-ms=180000
yarn.client.nodemanager-connect.retry-interval-ms=10000
yarn.dispatcher.drain-events.timeout=300000
yarn.fail-fast=false
yarn.http.policy=HTTP_ONLY
yarn.ipc.rpc.class=org.apache.hadoop.yarn.ipc.HadoopYarnProtoRPC
yarn.log-aggregation-enable=false
yarn.log-aggregation.retain-check-interval-seconds=-1
yarn.log-aggregation.retain-seconds=-1
yarn.nm.liveness-monitor.expiry-interval-ms=600000
yarn.nodemanager.address=0.0.0.0:0
yarn.nodemanager.admin-env=MALLOC_ARENA_MAX=$MALLOC_ARENA_MAX
yarn.nodemanager.aux-services.mapreduce_shuffle.class=org.apache.hadoop.mapred.ShuffleHandler
yarn.nodemanager.container-executor.class=org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor
yarn.nodemanager.container-manager.thread-count=20
yarn.nodemanager.container-metrics.unregister-delay-ms=10000
yarn.nodemanager.container-monitor.interval-ms=3000
yarn.nodemanager.container-monitor.procfs-tree.smaps-based-rss.enabled=false
yarn.nodemanager.delete.debug-delay-sec=0
yarn.nodemanager.delete.thread-count=4
yarn.nodemanager.disk-health-checker.interval-ms=120000
yarn.nodemanager.disk-health-checker.max-disk-utilization-per-disk-percentage=90.0
yarn.nodemanager.disk-health-checker.min-free-space-per-disk-mb=0
yarn.nodemanager.disk-health-checker.min-healthy-disks=0.25
yarn.nodemanager.docker-container-executor.exec-name=/usr/bin/docker
yarn.nodemanager.env-whitelist=JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,HADOOP_YARN_HOME
yarn.nodemanager.health-checker.interval-ms=600000
yarn.nodemanager.health-checker.script.timeout-ms=1200000
yarn.nodemanager.hostname=0.0.0.0
yarn.nodemanager.keytab=/etc/krb5.keytab
yarn.nodemanager.linux-container-executor.cgroups.hierarchy=/hadoop-yarn
yarn.nodemanager.linux-container-executor.cgroups.mount=false
yarn.nodemanager.linux-container-executor.cgroups.strict-resource-usage=false
yarn.nodemanager.linux-container-executor.nonsecure-mode.limit-users=true
yarn.nodemanager.linux-container-executor.nonsecure-mode.local-user=nobody
yarn.nodemanager.linux-container-executor.nonsecure-mode.user-pattern=^[_.A-Za-z0-9][-@_.A-Za-z0-9]{0,255}?[$]?$
yarn.nodemanager.linux-container-executor.resources-handler.class=org.apache.hadoop.yarn.server.nodemanager.util.DefaultLCEResourcesHandler
yarn.nodemanager.local-cache.max-files-per-directory=8192
yarn.nodemanager.local-dirs=/local_disk0/tmp/nm-local-dir
yarn.nodemanager.localizer.address=0.0.0.0:8040
yarn.nodemanager.localizer.cache.cleanup.interval-ms=600000
yarn.nodemanager.localizer.cache.target-size-mb=10240
yarn.nodemanager.localizer.client.thread-count=5
yarn.nodemanager.localizer.fetch.thread-count=4
yarn.nodemanager.log-aggregation.compression-type=none
yarn.nodemanager.log-aggregation.roll-monitoring-interval-seconds=-1
yarn.nodemanager.log-dirs=${yarn.log.dir}/userlogs
yarn.nodemanager.log.retain-seconds=10800
yarn.nodemanager.pmem-check-enabled=true
yarn.nodemanager.process-kill-wait.ms=2000
yarn.nodemanager.recovery.compaction-interval-secs=3600
yarn.nodemanager.recovery.dir=/local_disk0/tmp/yarn-nm-recovery
yarn.nodemanager.recovery.enabled=false
yarn.nodemanager.remote-app-log-dir=/tmp/logs
yarn.nodemanager.remote-app-log-dir-suffix=logs
yarn.nodemanager.resource.cpu-vcores=8
yarn.nodemanager.resource.memory-mb=8192
yarn.nodemanager.resource.percentage-physical-cpu-limit=100
yarn.nodemanager.resourcemanager.minimum.version=NONE
yarn.nodemanager.sleep-delay-before-sigkill.ms=250
yarn.nodemanager.vmem-check-enabled=true
yarn.nodemanager.vmem-pmem-ratio=2.1
yarn.nodemanager.webapp.address=0.0.0.0:8042
yarn.nodemanager.webapp.cross-origin.enabled=false
yarn.nodemanager.windows-container.cpu-limit.enabled=false
yarn.nodemanager.windows-container.memory-limit.enabled=false
yarn.resourcemanager.address=0.0.0.0:8032
yarn.resourcemanager.admin.address=0.0.0.0:8033
yarn.resourcemanager.admin.client.thread-count=1
yarn.resourcemanager.am-rm-tokens.master-key-rolling-interval-secs=86400
yarn.resourcemanager.am.max-attempts=2
yarn.resourcemanager.amlauncher.thread-count=50
yarn.resourcemanager.client.thread-count=50
yarn.resourcemanager.configuration.provider-class=org.apache.hadoop.yarn.LocalConfigurationProvider
yarn.resourcemanager.connect.max-wait.ms=900000
yarn.resourcemanager.connect.retry-interval.ms=30000
yarn.resourcemanager.container-tokens.master-key-rolling-interval-secs=86400
yarn.resourcemanager.container.liveness-monitor.interval-ms=600000
yarn.resourcemanager.delayed.delegation-token.removal-interval-ms=30000
yarn.resourcemanager.fail-fast=false
yarn.resourcemanager.fs.state-store.num-retries=0
yarn.resourcemanager.fs.state-store.retry-interval-ms=1000
yarn.resourcemanager.fs.state-store.retry-policy-spec=2000, 500
yarn.resourcemanager.fs.state-store.uri=/local_disk0/tmp/yarn/system/rmstore
yarn.resourcemanager.ha.automatic-failover.embedded=true
yarn.resourcemanager.ha.automatic-failover.enabled=true
yarn.resourcemanager.ha.automatic-failover.zk-base-path=
    /yarn-leader-election:
yarn.resourcemanager.ha.enabled=false
yarn.resourcemanager.hostname=0.0.0.0
yarn.resourcemanager.keytab=/etc/krb5.keytab
yarn.resourcemanager.leveldb-state-store.compaction-interval-secs=3600
yarn.resourcemanager.leveldb-state-store.path=
    /local_disk0/tmp/yarn/system/rmstore:
yarn.resourcemanager.max-completed-applications=10000
yarn.resourcemanager.nodemanager-connect-retries=10
yarn.resourcemanager.nodemanager.minimum.version=NONE
yarn.resourcemanager.nodemanagers.heartbeat-interval-ms=1000
yarn.resourcemanager.proxy-user-privileges.enabled=false
yarn.resourcemanager.recovery.enabled=false
yarn.resourcemanager.resource-tracker.address=0.0.0.0:8031
yarn.resourcemanager.resource-tracker.client.thread-count=50
yarn.resourcemanager.scheduler.address=0.0.0.0:8030
yarn.resourcemanager.scheduler.class=org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler
yarn.resourcemanager.scheduler.client.thread-count=50
yarn.resourcemanager.scheduler.monitor.enable=false
yarn.resourcemanager.scheduler.monitor.policies=org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy
yarn.resourcemanager.state-store.max-completed-applications=10000
yarn.resourcemanager.store.class=org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore
yarn.resourcemanager.system-metrics-publisher.dispatcher.pool-size=10
yarn.resourcemanager.system-metrics-publisher.enabled=false
yarn.resourcemanager.webapp.address=0.0.0.0:8088
yarn.resourcemanager.webapp.cross-origin.enabled=false
yarn.resourcemanager.webapp.delegation-token-auth-filter.enabled=true
yarn.resourcemanager.webapp.https.address=0.0.0.0:8090
yarn.resourcemanager.work-preserving-recovery.enabled=true
yarn.resourcemanager.work-preserving-recovery.scheduling-wait-ms=10000
yarn.resourcemanager.zk-acl=world:anyone:rwcda
yarn.resourcemanager.zk-num-retries=1000
yarn.resourcemanager.zk-retry-interval-ms=1000
yarn.resourcemanager.zk-state-store.parent-path=
    /rmstore:
yarn.resourcemanager.zk-timeout-ms=10000
yarn.scheduler.maximum-allocation-mb=8192
yarn.scheduler.maximum-allocation-vcores=32
yarn.scheduler.minimum-allocation-mb=1024
yarn.scheduler.minimum-allocation-vcores=1
yarn.sharedcache.admin.address=0.0.0.0:8047
yarn.sharedcache.admin.thread-count=1
yarn.sharedcache.app-checker.class=org.apache.hadoop.yarn.server.sharedcachemanager.RemoteAppChecker
yarn.sharedcache.checksum.algo.impl=org.apache.hadoop.yarn.sharedcache.ChecksumSHA256Impl
yarn.sharedcache.cleaner.initial-delay-mins=10
yarn.sharedcache.cleaner.period-mins=1440
yarn.sharedcache.cleaner.resource-sleep-ms=0
yarn.sharedcache.client-server.address=0.0.0.0:8045
yarn.sharedcache.client-server.thread-count=50
yarn.sharedcache.enabled=false
yarn.sharedcache.nested-level=3
yarn.sharedcache.nm.uploader.replication.factor=10
yarn.sharedcache.nm.uploader.thread-count=20
yarn.sharedcache.root-dir=/sharedcache
yarn.sharedcache.store.class=org.apache.hadoop.yarn.server.sharedcachemanager.store.InMemorySCMStore
yarn.sharedcache.store.in-memory.check-period-mins=720
yarn.sharedcache.store.in-memory.initial-delay-mins=10
yarn.sharedcache.store.in-memory.staleness-period-mins=10080
yarn.sharedcache.uploader.server.address=0.0.0.0:8046
yarn.sharedcache.uploader.server.thread-count=50
yarn.sharedcache.webapp.address=0.0.0.0:8788
yarn.timeline-service.address=0.0.0.0:10200
yarn.timeline-service.client.best-effort=false
yarn.timeline-service.client.max-retries=30
yarn.timeline-service.client.retry-interval-ms=1000
yarn.timeline-service.enabled=false
yarn.timeline-service.generic-application-history.max-applications=10000
yarn.timeline-service.handler-thread-count=10
yarn.timeline-service.hostname=0.0.0.0
yarn.timeline-service.http-authentication.simple.anonymous.allowed=true
yarn.timeline-service.http-authentication.type=simple
yarn.timeline-service.keytab=/etc/krb5.keytab
yarn.timeline-service.leveldb-state-store.path=
    /local_disk0/tmp/yarn/timeline:
yarn.timeline-service.leveldb-timeline-store.path=
    /local_disk0/tmp/yarn/timeline:
yarn.timeline-service.leveldb-timeline-store.read-cache-size=104857600
yarn.timeline-service.leveldb-timeline-store.start-time-read-cache-size=10000
yarn.timeline-service.leveldb-timeline-store.start-time-write-cache-size=10000
yarn.timeline-service.leveldb-timeline-store.ttl-interval-ms=300000
yarn.timeline-service.recovery.enabled=false
yarn.timeline-service.state-store-class=org.apache.hadoop.yarn.server.timeline.recovery.LeveldbTimelineStateStore
yarn.timeline-service.store-class=org.apache.hadoop.yarn.server.timeline.LeveldbTimelineStore
yarn.timeline-service.ttl-enable=true
yarn.timeline-service.ttl-ms=604800000
yarn.timeline-service.webapp.address=0.0.0.0:8188
yarn.timeline-service.webapp.https.address=0.0.0.0:8190
END========"new HiveConf()"========

19/10/08 20:34:21 WARN ObjectStore: Failed to get database pqm_tyson_1000233_e2e, returning NoSuchObjectException
19/10/08 20:34:21 DEBUG HttpTransport: HTTPRequest,Failed,cReqId:f9f54677-8e8c-4729-9c6a-27e9acf234e4.0,lat:7,err:HTTP404(FileNotFoundException),Reqlen:0,Resplen:265,token_ns:6900,sReqId:b8682999-dd4b-405b-8f20-fabd791b81cf,path:/user/hive/warehouse/PQM_TYSON_1000233_E2E.db,qp:op=GETFILESTATUS&tooid=true&api-version=2017-08-01
19/10/08 20:34:21 ERROR ScalaDriverLocal: User Code Stack Trace: 
org.apache.spark.sql.AnalysisException: Path does not exist: wasbs://teuatcontainer01@teuatstorage.blob.core.windows.net/PQM_TYSON_1000233_E2E/MRKT_DIM.csv;
	at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary$1.apply(DataSource.scala:612)
	at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary$1.apply(DataSource.scala:595)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
	at scala.collection.immutable.List.foreach(List.scala:381)
	at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)
	at scala.collection.immutable.List.flatMap(List.scala:344)
	at org.apache.spark.sql.execution.datasources.DataSource.org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary(DataSource.scala:595)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:390)
	at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:298)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:279)
	at org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:707)
	at org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:551)
	at com.nielsen.te.v3.core.executors.FileToDimension.executeStep(FileToDimension.scala:30)
	at com.nielsen.te.v3.executor.LibraryStepExecutor.executeStep(StepExecutor.scala:56)
	at com.nielsen.te.v3.executor.StepProcessor$$anonfun$stepProcessor$1.apply(StepProcessor.scala:129)
	at com.nielsen.te.v3.executor.StepProcessor$$anonfun$stepProcessor$1.apply(StepProcessor.scala:127)
	at scala.collection.immutable.List.foreach(List.scala:381)
	at com.nielsen.te.v3.executor.StepProcessor$.stepProcessor(StepProcessor.scala:127)
	at com.nielsen.te.v3.TEv3Main$.appMain(TEv3Main.scala:47)
	at com.nielsen.te.v3.TEv3Main$.main(TEv3Main.scala:51)
	at line17fda24c80ec4bf79383a3970a4ca76725.$read$$iw$$iw$$iw$$iw$$iw$$iw.<init>(command--1:1)
	at line17fda24c80ec4bf79383a3970a4ca76725.$read$$iw$$iw$$iw$$iw$$iw.<init>(command--1:44)
	at line17fda24c80ec4bf79383a3970a4ca76725.$read$$iw$$iw$$iw$$iw.<init>(command--1:46)
	at line17fda24c80ec4bf79383a3970a4ca76725.$read$$iw$$iw$$iw.<init>(command--1:48)
	at line17fda24c80ec4bf79383a3970a4ca76725.$read$$iw$$iw.<init>(command--1:50)
	at line17fda24c80ec4bf79383a3970a4ca76725.$read$$iw.<init>(command--1:52)
	at line17fda24c80ec4bf79383a3970a4ca76725.$read.<init>(command--1:54)
	at line17fda24c80ec4bf79383a3970a4ca76725.$read$.<init>(command--1:58)
	at line17fda24c80ec4bf79383a3970a4ca76725.$read$.<clinit>(command--1)
	at line17fda24c80ec4bf79383a3970a4ca76725.$eval$.$print$lzycompute(<notebook>:7)
	at line17fda24c80ec4bf79383a3970a4ca76725.$eval$.$print(<notebook>:6)
	at line17fda24c80ec4bf79383a3970a4ca76725.$eval.$print(<notebook>)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:786)
	at scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:1047)
	at scala.tools.nsc.interpreter.IMain$WrappedRequest$$anonfun$loadAndRunReq$1.apply(IMain.scala:638)
	at scala.tools.nsc.interpreter.IMain$WrappedRequest$$anonfun$loadAndRunReq$1.apply(IMain.scala:637)
	at scala.reflect.internal.util.ScalaClassLoader$class.asContext(ScalaClassLoader.scala:31)
	at scala.reflect.internal.util.AbstractFileClassLoader.asContext(AbstractFileClassLoader.scala:19)
	at scala.tools.nsc.interpreter.IMain$WrappedRequest.loadAndRunReq(IMain.scala:637)
	at scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:569)
	at scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:565)
	at com.databricks.backend.daemon.driver.DriverILoop.execute(DriverILoop.scala:199)
	at com.databricks.backend.daemon.driver.ScalaDriverLocal$$anonfun$repl$1.apply$mcV$sp(ScalaDriverLocal.scala:189)
	at com.databricks.backend.daemon.driver.ScalaDriverLocal$$anonfun$repl$1.apply(ScalaDriverLocal.scala:189)
	at com.databricks.backend.daemon.driver.ScalaDriverLocal$$anonfun$repl$1.apply(ScalaDriverLocal.scala:189)
	at com.databricks.backend.daemon.driver.DriverLocal$TrapExitInternal$.trapExit(DriverLocal.scala:587)
	at com.databricks.backend.daemon.driver.DriverLocal$TrapExit$.apply(DriverLocal.scala:542)
	at com.databricks.backend.daemon.driver.ScalaDriverLocal.repl(ScalaDriverLocal.scala:189)
	at com.databricks.backend.daemon.driver.DriverLocal$$anonfun$execute$7.apply(DriverLocal.scala:324)
	at com.databricks.backend.daemon.driver.DriverLocal$$anonfun$execute$7.apply(DriverLocal.scala:304)
	at com.databricks.logging.UsageLogging$$anonfun$withAttributionContext$1.apply(UsageLogging.scala:235)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at com.databricks.logging.UsageLogging$class.withAttributionContext(UsageLogging.scala:230)
	at com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:45)
	at com.databricks.logging.UsageLogging$class.withAttributionTags(UsageLogging.scala:268)
	at com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:45)
	at com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:304)
	at com.databricks.backend.daemon.driver.DriverWrapper$$anonfun$tryExecutingCommand$2.apply(DriverWrapper.scala:589)
	at com.databricks.backend.daemon.driver.DriverWrapper$$anonfun$tryExecutingCommand$2.apply(DriverWrapper.scala:589)
	at scala.util.Try$.apply(Try.scala:192)
	at com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:584)
	at com.databricks.backend.daemon.driver.DriverWrapper.getCommandOutputAndError(DriverWrapper.scala:475)
	at com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:542)
	at com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:381)
	at com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:328)
	at com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:215)
	at java.lang.Thread.run(Thread.java:748)
19/10/08 20:34:24 ERROR TaskSchedulerImpl: Lost executor 0 on 10.139.64.33: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
