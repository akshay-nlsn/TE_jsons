19/10/16 18:26:30 INFO StaticConf$: DB_HOME: /databricks
19/10/16 18:26:30 INFO DriverDaemon$: ========== driver starting up ==========
19/10/16 18:26:30 INFO DriverDaemon$: Java: Oracle Corporation 1.8.0_191
19/10/16 18:26:30 INFO DriverDaemon$: OS: Linux/amd64 4.15.0-1050-azure
19/10/16 18:26:30 INFO DriverDaemon$: CWD: /databricks/driver
19/10/16 18:26:30 INFO DriverDaemon$: Mem: Max: 97.3G loaded GCs: PS Scavenge, PS MarkSweep
19/10/16 18:26:30 INFO DriverDaemon$: Logging multibyte characters: âœ“
19/10/16 18:26:30 INFO DriverDaemon$: 'publicFile' appender in root logger: class com.databricks.logging.RedactionRollingFileAppender
19/10/16 18:26:30 INFO DriverDaemon$: 'org.apache.log4j.Appender' appender in root logger: class com.codahale.metrics.log4j.InstrumentedAppender
19/10/16 18:26:30 INFO DriverDaemon$: 'null' appender in root logger: class com.databricks.logging.RequestTracker
19/10/16 18:26:30 INFO DriverDaemon$: == Modules:
19/10/16 18:26:31 INFO DriverDaemon$: Starting prometheus metrics log export timer
19/10/16 18:26:31 INFO DriverDaemon$: Universe Git Hash: 7201322412393895429516835306b6ef047d0688
19/10/16 18:26:31 INFO DriverDaemon$: Spark Git Hash: a937effdb77c94e1293f1fa405f29834568a066a
19/10/16 18:26:31 WARN RunHelpers$: Missing tag isolation client: java.util.NoSuchElementException: key not found: TagDefinition(clientType,The client type for a request, used for isolating resources for the request.)
19/10/16 18:26:31 INFO DatabricksILoop$: Creating throwaway interpreter
19/10/16 18:26:31 INFO SparkConfUtils$: Customize spark config according to file /tmp/custom-spark.conf
19/10/16 18:26:31 INFO SparkConfUtils$: new spark config: spark.sql.warehouse.dir -> adl://connectedsystemsuatdl.azuredatalakestore.net/user/hive/warehouse
19/10/16 18:26:31 INFO SparkConfUtils$: new spark config: spark.hadoop.javax.jdo.option.ConnectionPassword -> 560I890@fd80709
19/10/16 18:26:31 INFO SparkConfUtils$: new spark config: spark.hadoop.dfs.adls.oauth2.credential -> viUHaEfNg8cHQRJSRNMzpIv1T6goiUg+c9R/XSbYBFA=
19/10/16 18:26:31 INFO SparkConfUtils$: new spark config: spark.hadoop.javax.jdo.option.ConnectionDriverName -> org.postgresql.Driver
19/10/16 18:26:31 INFO SparkConfUtils$: new spark config: spark.databricks.acl.dfAclsEnabled -> false
19/10/16 18:26:31 INFO SparkConfUtils$: new spark config: spark.driver.tempDirectory -> /local_disk0/tmp
19/10/16 18:26:31 INFO SparkConfUtils$: new spark config: spark.hadoop.datanucleus.connectionPool.maxIdle -> 1
19/10/16 18:26:31 INFO SparkConfUtils$: new spark config: spark.hadoop.dfs.adls.oauth2.client.id -> 003106c3-ff8c-4c62-a060-24b8b32ea512
19/10/16 18:26:31 INFO SparkConfUtils$: new spark config: spark.hadoop.dfs.adls.oauth2.refresh.url -> https://login.microsoftonline.com/6ac7a1f4-5fb1-4153-bb4f-12d2020a1f7d/oauth2/token
19/10/16 18:26:31 INFO SparkConfUtils$: new spark config: spark.executor.tempDirectory -> /local_disk0/tmp
19/10/16 18:26:31 INFO SparkConfUtils$: new spark config: spark.databricks.delta.preview.enabled -> true
19/10/16 18:26:31 INFO SparkConfUtils$: new spark config: spark.task.maxFailures -> 10
19/10/16 18:26:31 INFO SparkConfUtils$: new spark config: spark.sql.hive.metastore.jars -> maven
19/10/16 18:26:31 INFO SparkConfUtils$: new spark config: spark.databricks.aggressiveWindowDownS -> 300
19/10/16 18:26:31 INFO SparkConfUtils$: new spark config: spark.hadoop.javax.jdo.option.ConnectionUserName -> tedbuat
19/10/16 18:26:31 INFO SparkConfUtils$: new spark config: spark.databricks.sql.hive.msck.batchSize -> 200
19/10/16 18:26:31 INFO SparkConfUtils$: new spark config: spark.hadoop.javax.jdo.option.ConnectionURL -> jdbc:postgresql://uathmsdb.trans.enterprisecsp.net:5432/hive_adls
19/10/16 18:26:31 INFO SparkConfUtils$: new spark config: datanucleus.fixedDatastore -> false
19/10/16 18:26:31 INFO SparkConfUtils$: new spark config: spark.databricks.sql.hive.msck.parallelism -> 20
19/10/16 18:26:31 INFO SparkConfUtils$: new spark config: spark.hadoop.datanucleus.connectionPool.maxPoolSize -> 30
19/10/16 18:26:31 INFO SparkConfUtils$: new spark config: spark.hadoop.datanucleus.connectionPool.minPoolSize -> 15
19/10/16 18:26:31 INFO SparkConfUtils$: new spark config: spark.sql.parquet.filterPushdown -> false
19/10/16 18:26:31 INFO SparkConfUtils$: new spark config: spark.hadoop.dfs.adls.oauth2.access.token.provider.type -> ClientCredential
19/10/16 18:26:31 INFO SparkConfUtils$: new spark config: spark.sql.shuffle.partitions -> 2000
19/10/16 18:26:31 INFO SparkConfUtils$: new spark config: spark.driver.maxResultSize -> 0
19/10/16 18:26:31 INFO SparkConfUtils$: new spark config: fs.azure.account.key.teuatsa.blob.core.windows.net -> IsrxMnnzLeO9N44beU4eMw0LaiKJdi1hHmKZFUiPa8nFNKZAFyhH0Hv/UCoC4qhgcdq63zwTRGFhUKV9d4ZsBw==
19/10/16 18:26:31 INFO SparkConfUtils$: new spark config: datanucleus.autoCreateSchema -> true
19/10/16 18:26:31 INFO SparkConfUtils$: new spark config: spark.hadoop.fs.adl.impl.disable.cache -> false
19/10/16 18:26:31 INFO SparkConfUtils$: new spark config: spark.sql.hive.metastore.version -> 2.3.0
19/10/16 18:26:31 WARN MetastoreMonitor$: Uri scheme postgresql is not supported.
19/10/16 18:26:31 WARN MetastoreMonitor$: Unexpected partial metastore configuration (userOpt=Some(tedbuat), pw.isDefined=true, uriOpt=None)
19/10/16 18:26:31 INFO MetastoreMonitor$: Generic external metastore configurd (config=jdbc:postgresql://uathmsdb.trans.enterprisecsp.net:5432/hive_adls)
19/10/16 18:26:31 INFO MetastoreMonitor: Not monitoring ExternalGenericMetastore
19/10/16 18:26:31 INFO JettyClient$: Creating new HttpClient with SSLContextFactory=None,maxRequestHeaderSize=65536, namePrefix=Some(DriverDaemon), idleTimeout=2 hours
19/10/16 18:26:31 INFO DriverCorral: Creating the driver context
19/10/16 18:26:31 INFO DatabricksILoop$: Class Server Dir: /local_disk0/tmp/repl/spark-1665401093835382800-d383f379-cd90-4f0a-b537-047f02636013
19/10/16 18:26:31 INFO SparkConfUtils$: Customize spark config according to file /tmp/custom-spark.conf
19/10/16 18:26:31 INFO SparkConfUtils$: new spark config: spark.sql.warehouse.dir -> adl://connectedsystemsuatdl.azuredatalakestore.net/user/hive/warehouse
19/10/16 18:26:31 INFO SparkConfUtils$: new spark config: spark.hadoop.javax.jdo.option.ConnectionPassword -> 560I890@fd80709
19/10/16 18:26:31 INFO SparkConfUtils$: new spark config: spark.hadoop.dfs.adls.oauth2.credential -> viUHaEfNg8cHQRJSRNMzpIv1T6goiUg+c9R/XSbYBFA=
19/10/16 18:26:31 INFO SparkConfUtils$: new spark config: spark.hadoop.javax.jdo.option.ConnectionDriverName -> org.postgresql.Driver
19/10/16 18:26:31 INFO SparkConfUtils$: new spark config: spark.databricks.acl.dfAclsEnabled -> false
19/10/16 18:26:31 INFO SparkConfUtils$: new spark config: spark.driver.tempDirectory -> /local_disk0/tmp
19/10/16 18:26:31 INFO SparkConfUtils$: new spark config: spark.hadoop.datanucleus.connectionPool.maxIdle -> 1
19/10/16 18:26:31 INFO SparkConfUtils$: new spark config: spark.hadoop.dfs.adls.oauth2.client.id -> 003106c3-ff8c-4c62-a060-24b8b32ea512
19/10/16 18:26:31 INFO SparkConfUtils$: new spark config: spark.hadoop.dfs.adls.oauth2.refresh.url -> https://login.microsoftonline.com/6ac7a1f4-5fb1-4153-bb4f-12d2020a1f7d/oauth2/token
19/10/16 18:26:31 INFO SparkConfUtils$: new spark config: spark.executor.tempDirectory -> /local_disk0/tmp
19/10/16 18:26:31 INFO SparkConfUtils$: new spark config: spark.databricks.delta.preview.enabled -> true
19/10/16 18:26:31 INFO SparkConfUtils$: new spark config: spark.task.maxFailures -> 10
19/10/16 18:26:31 INFO SparkConfUtils$: new spark config: spark.sql.hive.metastore.jars -> maven
19/10/16 18:26:31 INFO SparkConfUtils$: new spark config: spark.databricks.aggressiveWindowDownS -> 300
19/10/16 18:26:31 INFO SparkConfUtils$: new spark config: spark.hadoop.javax.jdo.option.ConnectionUserName -> tedbuat
19/10/16 18:26:31 INFO SparkConfUtils$: new spark config: spark.databricks.sql.hive.msck.batchSize -> 200
19/10/16 18:26:31 INFO SparkConfUtils$: new spark config: spark.hadoop.javax.jdo.option.ConnectionURL -> jdbc:postgresql://uathmsdb.trans.enterprisecsp.net:5432/hive_adls
19/10/16 18:26:31 INFO SparkConfUtils$: new spark config: datanucleus.fixedDatastore -> false
19/10/16 18:26:31 INFO SparkConfUtils$: new spark config: spark.databricks.sql.hive.msck.parallelism -> 20
19/10/16 18:26:31 INFO SparkConfUtils$: new spark config: spark.hadoop.datanucleus.connectionPool.maxPoolSize -> 30
19/10/16 18:26:31 INFO SparkConfUtils$: new spark config: spark.hadoop.datanucleus.connectionPool.minPoolSize -> 15
19/10/16 18:26:31 INFO SparkConfUtils$: new spark config: spark.sql.parquet.filterPushdown -> false
19/10/16 18:26:31 INFO SparkConfUtils$: new spark config: spark.hadoop.dfs.adls.oauth2.access.token.provider.type -> ClientCredential
19/10/16 18:26:31 INFO SparkConfUtils$: new spark config: spark.sql.shuffle.partitions -> 2000
19/10/16 18:26:31 INFO SparkConfUtils$: new spark config: spark.driver.maxResultSize -> 0
19/10/16 18:26:31 INFO SparkConfUtils$: new spark config: fs.azure.account.key.teuatsa.blob.core.windows.net -> IsrxMnnzLeO9N44beU4eMw0LaiKJdi1hHmKZFUiPa8nFNKZAFyhH0Hv/UCoC4qhgcdq63zwTRGFhUKV9d4ZsBw==
19/10/16 18:26:31 INFO SparkConfUtils$: new spark config: datanucleus.autoCreateSchema -> true
19/10/16 18:26:31 INFO SparkConfUtils$: new spark config: spark.hadoop.fs.adl.impl.disable.cache -> false
19/10/16 18:26:31 INFO SparkConfUtils$: new spark config: spark.sql.hive.metastore.version -> 2.3.0
19/10/16 18:26:31 WARN SparkConf: The configuration key 'spark.scheduler.listenerbus.eventqueue.size' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.scheduler.listenerbus.eventqueue.capacity' instead.
19/10/16 18:26:31 WARN SparkConf: The configuration key 'spark.akka.frameSize' has been deprecated as of Spark 1.6 and may be removed in the future. Please use the new key 'spark.rpc.message.maxSize' instead.
19/10/16 18:26:31 INFO SparkContext: Running Spark version 2.4.0
19/10/16 18:26:32 WARN SparkConf: Detected deprecated memory fraction settings: [spark.storage.memoryFraction, spark.shuffle.memoryFraction]. As of Spark 1.6, execution and storage memory management are unified. All memory fractions used in the old model are now deprecated and no longer read. If you wish to use the old memory management, you may explicitly enable `spark.memory.useLegacyMode` (not recommended).
19/10/16 18:26:32 INFO SparkContext: Submitted application: Databricks Shell
19/10/16 18:26:32 INFO SparkContext: Spark configuration:
datanucleus.autoCreateSchema=true
datanucleus.fixedDatastore=false
eventLog.rolloverIntervalSeconds=3600
fs.azure.account.key.teuatsa.blob.core.windows.net=IsrxMnnzLeO9N44beU4eMw0LaiKJdi1hHmKZFUiPa8nFNKZAFyhH0Hv/UCoC4qhgcdq63zwTRGFhUKV9d4ZsBw==
spark.akka.frameSize=256
spark.app.name=Databricks Shell
spark.cleaner.referenceTracking.blocking=false
spark.databricks.acl.client=com.databricks.spark.sql.acl.client.SparkSqlAclClient
spark.databricks.acl.dfAclsEnabled=false
spark.databricks.acl.provider=com.databricks.sql.acl.ReflectionBackedAclProvider
spark.databricks.aggressiveWindowDownS=300
spark.databricks.cloudProvider=Azure
spark.databricks.clusterSource=JOB
spark.databricks.clusterUsageTags.autoTerminationMinutes=0
spark.databricks.clusterUsageTags.clusterAggressiveAutoscalingWindowDownSeconds=300
spark.databricks.clusterUsageTags.clusterAllTags=[{"key":"Vendor","value":"Databricks"},{"key":"Creator","value":"basheer.ahamed@nielsen.com"},{"key":"ClusterName","value":"job-67852-run-1"},{"key":"ClusterId","value":"1016-182237-curvy914"},{"key":"DatabricksEnvironment","value":"workerenv-5614106561397268"}]
spark.databricks.clusterUsageTags.clusterCreator=JobLauncher
spark.databricks.clusterUsageTags.clusterGeneration=0
spark.databricks.clusterUsageTags.clusterId=1016-182237-curvy914
spark.databricks.clusterUsageTags.clusterLogDeliveryEnabled=true
spark.databricks.clusterUsageTags.clusterLogDestination=dbfs:/mnt/te_mount/uat_logs/
spark.databricks.clusterUsageTags.clusterMaxWorkers=5
spark.databricks.clusterUsageTags.clusterMetastoreAccessType=RDS_DIRECT
spark.databricks.clusterUsageTags.clusterMinWorkers=1
spark.databricks.clusterUsageTags.clusterName=job-67852-run-1
spark.databricks.clusterUsageTags.clusterNoDriverDaemon=false
spark.databricks.clusterUsageTags.clusterNodeType=Standard_E16s_v3
spark.databricks.clusterUsageTags.clusterNumSshKeys=0
spark.databricks.clusterUsageTags.clusterOwnerOrgId=5614106561397268
spark.databricks.clusterUsageTags.clusterOwnerUserId=*********(redacted)
spark.databricks.clusterUsageTags.clusterPinned=false
spark.databricks.clusterUsageTags.clusterPythonVersion=2
spark.databricks.clusterUsageTags.clusterResourceClass=default
spark.databricks.clusterUsageTags.clusterScalingType=autoscaling
spark.databricks.clusterUsageTags.clusterSku=STANDARD_SKU
spark.databricks.clusterUsageTags.clusterState=Pending
spark.databricks.clusterUsageTags.clusterStateMessage=Starting Spark
spark.databricks.clusterUsageTags.clusterTargetWorkers=1
spark.databricks.clusterUsageTags.clusterWorkers=1
spark.databricks.clusterUsageTags.containerType=LXC
spark.databricks.clusterUsageTags.driverContainerId=ccdfd0f301a84238ba2f33c2219a0f05
spark.databricks.clusterUsageTags.driverContainerPrivateIp=10.139.64.182
spark.databricks.clusterUsageTags.driverInstanceId=5424d279de114d129f1901ceb18591f4
spark.databricks.clusterUsageTags.driverInstancePrivateIp=10.139.0.182
spark.databricks.clusterUsageTags.driverNodeType=Standard_E16s_v3
spark.databricks.clusterUsageTags.driverPublicDns=52.232.160.41
spark.databricks.clusterUsageTags.enableCredentialPassthrough=*********(redacted)
spark.databricks.clusterUsageTags.enableDfAcls=false
spark.databricks.clusterUsageTags.enableElasticDisk=true
spark.databricks.clusterUsageTags.enableJdbcAutoStart=true
spark.databricks.clusterUsageTags.enableJobsAutostart=true
spark.databricks.clusterUsageTags.enableLocalDiskEncryption=false
spark.databricks.clusterUsageTags.enableSqlAclsOnly=false
spark.databricks.clusterUsageTags.hailEnabled=false
spark.databricks.clusterUsageTags.isSingleUserCluster=*********(redacted)
spark.databricks.clusterUsageTags.numPerClusterInitScriptsV2=0
spark.databricks.clusterUsageTags.sparkVersion=5.2.x-scala2.11
spark.databricks.clusterUsageTags.userProvidedRemoteVolumeCount=*********(redacted)
spark.databricks.clusterUsageTags.userProvidedRemoteVolumeSizeGb=*********(redacted)
spark.databricks.clusterUsageTags.userProvidedRemoteVolumeType=*********(redacted)
spark.databricks.clusterUsageTags.workerEnvironmentId=workerenv-5614106561397268
spark.databricks.credential.redactor=*********(redacted)
spark.databricks.delta.logStore.crossCloud.fatal=true
spark.databricks.delta.multiClusterWrites.enabled=true
spark.databricks.delta.preview.enabled=true
spark.databricks.driverNodeTypeId=Standard_E16s_v3
spark.databricks.eventLog.dir=eventlogs
spark.databricks.io.directoryCommit.enableLogicalDelete=false
spark.databricks.overrideDefaultCommitProtocol=org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol
spark.databricks.passthrough.adls.tokenProviderClassName=*********(redacted)
spark.databricks.passthrough.s3a.tokenProviderClassName=*********(redacted)
spark.databricks.preemption.enabled=true
spark.databricks.r.cleanWorkspace=true
spark.databricks.redactor=com.databricks.spark.util.DatabricksSparkLogRedactorProxy
spark.databricks.session.share=false
spark.databricks.sparkContextId=1665401093835382800
spark.databricks.sql.hive.msck.batchSize=200
spark.databricks.sql.hive.msck.parallelism=20
spark.databricks.tahoe.logStore.aws.class=com.databricks.tahoe.store.MultiClusterLogStore
spark.databricks.tahoe.logStore.azure.class=com.databricks.tahoe.store.AzureLogStore
spark.databricks.tahoe.logStore.class=com.databricks.tahoe.store.DelegatingLogStore
spark.databricks.workerNodeTypeId=Standard_E16s_v3
spark.driver.allowMultipleContexts=false
spark.driver.maxResultSize=0
spark.driver.tempDirectory=/local_disk0/tmp
spark.eventLog.enabled=false
spark.executor.extraClassPath=*********(redacted)
spark.executor.extraJavaOptions=-Djava.io.tmpdir=/local_disk0/tmp -XX:ReservedCodeCacheSize=256m -XX:+UseCodeCacheFlushing -Djava.security.properties=/databricks/spark/dbconf/java/extra.security -XX:+PrintFlagsFinal -XX:+PrintGCDateStamps -verbose:gc -XX:+PrintGCDetails -Xss4m -Djavax.xml.datatype.DatatypeFactory=com.sun.org.apache.xerces.internal.jaxp.datatype.DatatypeFactoryImpl -Djavax.xml.parsers.DocumentBuilderFactory=com.sun.org.apache.xerces.internal.jaxp.DocumentBuilderFactoryImpl -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -Djavax.xml.validation.SchemaFactory:http://www.w3.org/2001/XMLSchema=com.sun.org.apache.xerces.internal.jaxp.validation.XMLSchemaFactory -Dorg.xml.sax.driver=com.sun.org.apache.xerces.internal.parsers.SAXParser -Dorg.w3c.dom.DOMImplementationSourceList=com.sun.org.apache.xerces.internal.dom.DOMXSImplementationSourceImpl -Ddatabricks.serviceName=spark-executor-1
spark.executor.memory=97871m
spark.executor.tempDirectory=/local_disk0/tmp
spark.extraListeners=com.databricks.backend.daemon.driver.DBCEventLoggingListener
spark.files.fetchFailure.unRegisterOutputOnHost=true
spark.files.overwrite=true
spark.files.useFetchCache=false
spark.hadoop.databricks.dbfs.client.version=v2
spark.hadoop.databricks.s3commit.client.sslTrustAll=false
spark.hadoop.datanucleus.connectionPool.maxIdle=1
spark.hadoop.datanucleus.connectionPool.maxPoolSize=30
spark.hadoop.datanucleus.connectionPool.minPoolSize=15
spark.hadoop.dfs.adls.oauth2.access.token.provider.type=*********(redacted)
spark.hadoop.dfs.adls.oauth2.client.id=*********(redacted)
spark.hadoop.dfs.adls.oauth2.credential=*********(redacted)
spark.hadoop.dfs.adls.oauth2.refresh.url=*********(redacted)
spark.hadoop.fs.abfs.impl=shaded.databricks.v20180920_b33d810.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem
spark.hadoop.fs.abfs.impl.disable.cache=true
spark.hadoop.fs.abfss.impl=shaded.databricks.v20180920_b33d810.org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem
spark.hadoop.fs.abfss.impl.disable.cache=true
spark.hadoop.fs.adl.impl=com.databricks.adl.AdlFileSystem
spark.hadoop.fs.adl.impl.disable.cache=false
spark.hadoop.fs.azure.skip.metrics=true
spark.hadoop.fs.s3.impl=com.databricks.s3a.S3AFileSystem
spark.hadoop.fs.s3a.connection.maximum=200
spark.hadoop.fs.s3a.fast.upload=true
spark.hadoop.fs.s3a.fast.upload.default=true
spark.hadoop.fs.s3a.impl=com.databricks.s3a.S3AFileSystem
spark.hadoop.fs.s3a.multipart.size=10485760
spark.hadoop.fs.s3a.multipart.threshold=104857600
spark.hadoop.fs.s3a.threads.max=136
spark.hadoop.fs.s3n.impl=com.databricks.s3a.S3AFileSystem
spark.hadoop.fs.wasb.impl=shaded.databricks.org.apache.hadoop.fs.azure.NativeAzureFileSystem
spark.hadoop.fs.wasb.impl.disable.cache=true
spark.hadoop.fs.wasbs.impl=shaded.databricks.org.apache.hadoop.fs.azure.NativeAzureFileSystem
spark.hadoop.fs.wasbs.impl.disable.cache=true
spark.hadoop.hive.server2.idle.operation.timeout=7200000
spark.hadoop.hive.server2.idle.session.timeout=900000
spark.hadoop.hive.server2.keystore.password=*********(redacted)
spark.hadoop.hive.server2.keystore.path=/databricks/keys/jetty-ssl-driver-keystore.jks
spark.hadoop.hive.server2.session.check.interval=60000
spark.hadoop.hive.server2.thrift.http.port=10000
spark.hadoop.hive.server2.transport.mode=http
spark.hadoop.hive.server2.use.SSL=true
spark.hadoop.hive.warehouse.subdir.inherit.perms=false
spark.hadoop.javax.jdo.option.ConnectionDriverName=org.postgresql.Driver
spark.hadoop.javax.jdo.option.ConnectionPassword=*********(redacted)
spark.hadoop.javax.jdo.option.ConnectionURL=*********(redacted)
spark.hadoop.javax.jdo.option.ConnectionUserName=*********(redacted)
spark.hadoop.mapred.output.committer.class=com.databricks.backend.daemon.data.client.DirectOutputCommitter
spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version=2
spark.hadoop.parquet.memory.pool.ratio=0.5
spark.hadoop.spark.sql.parquet.output.committer.class=org.apache.spark.sql.parquet.DirectParquetOutputCommitter
spark.hadoop.spark.sql.sources.outputCommitterClass=com.databricks.backend.daemon.data.client.MapReduceDirectOutputCommitter
spark.hadoop.spark.thriftserver.customHeadersToProperties=*********(redacted)
spark.home=/databricks/spark
spark.logConf=true
spark.master=spark://10.139.64.182:7077
spark.metrics.conf=/databricks/spark/conf/metrics.properties
spark.r.backendConnectionTimeout=604800
spark.r.numRBackendThreads=1
spark.rdd.compress=true
spark.repl.class.outputDir=/local_disk0/tmp/repl/spark-1665401093835382800-d383f379-cd90-4f0a-b537-047f02636013
spark.scheduler.listenerbus.eventqueue.size=20000
spark.scheduler.mode=FAIR
spark.serializer.objectStreamReset=100
spark.shuffle.manager=SORT
spark.shuffle.memoryFraction=0.2
spark.shuffle.reduceLocality.enabled=false
spark.shuffle.service.enabled=true
spark.shuffle.service.port=4048
spark.sparkr.use.daemon=false
spark.speculation=false
spark.speculation.multiplier=3
spark.speculation.quantile=0.9
spark.sql.allowMultipleContexts=false
spark.sql.hive.convertCTAS=true
spark.sql.hive.convertMetastoreParquet=true
spark.sql.hive.metastore.jars=maven
spark.sql.hive.metastore.sharedPrefixes=org.mariadb.jdbc,com.mysql.jdbc,org.postgresql,com.microsoft.sqlserver,microsoft.sql.DateTimeOffset,microsoft.sql.Types,com.databricks,com.codahale,com.fasterxml.jackson,shaded.databricks
spark.sql.hive.metastore.version=2.3.0
spark.sql.parquet.cacheMetadata=true
spark.sql.parquet.compression.codec=snappy
spark.sql.parquet.filterPushdown=false
spark.sql.shuffle.partitions=2000
spark.sql.sources.commitProtocolClass=com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol
spark.sql.streaming.checkpointFileManagerClass=com.databricks.spark.sql.streaming.DatabricksCheckpointFileManager
spark.sql.ui.retainedExecutions=100
spark.sql.warehouse.dir=*********(redacted)
spark.storage.blockManagerTimeoutIntervalMs=300000
spark.storage.memoryFraction=0.5
spark.streaming.driver.writeAheadLog.allowBatching=true
spark.streaming.driver.writeAheadLog.closeFileAfterWrite=true
spark.task.maxFailures=10
spark.task.reaper.enabled=true
spark.task.reaper.killTimeout=60s
spark.ui.port=44095
spark.worker.cleanup.enabled=false
19/10/16 18:26:32 INFO SecurityManager: Changing view acls to: root
19/10/16 18:26:32 INFO SecurityManager: Changing modify acls to: root
19/10/16 18:26:32 INFO SecurityManager: Changing view acls groups to: 
19/10/16 18:26:32 INFO SecurityManager: Changing modify acls groups to: 
19/10/16 18:26:32 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
19/10/16 18:26:32 INFO Utils: Successfully started service 'sparkDriver' on port 33603.
19/10/16 18:26:32 INFO SparkEnv: Registering MapOutputTracker
19/10/16 18:26:32 INFO SparkEnv: Registering BlockManagerMaster
19/10/16 18:26:32 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
19/10/16 18:26:32 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
19/10/16 18:26:32 INFO DiskBlockManager: Created local directory at /local_disk0/blockmgr-914b4aed-df38-475f-a9eb-3a400a0c9972
19/10/16 18:26:32 INFO MemoryStore: MemoryStore started with capacity 54.2 GB
19/10/16 18:26:32 INFO SparkEnv: Registering OutputCommitCoordinator
19/10/16 18:26:32 WARN MetricsSystem: Using default name SparkStatusTracker for source because neither spark.metrics.namespace nor spark.app.id is set.
19/10/16 18:26:33 INFO log: Logging initialized @4617ms
19/10/16 18:26:33 INFO Server: jetty-9.3.20.v20170531
19/10/16 18:26:33 INFO Server: Started @4744ms
19/10/16 18:26:33 INFO AbstractConnector: Started ServerConnector@3c50ad4b{HTTP/1.1,[http/1.1]}{10.139.64.182:44095}
19/10/16 18:26:33 INFO Utils: Successfully started service 'SparkUI' on port 44095.
19/10/16 18:26:33 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@2ab26378{/jobs,null,AVAILABLE,@Spark}
19/10/16 18:26:33 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@2f0ed952{/jobs/json,null,AVAILABLE,@Spark}
19/10/16 18:26:33 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@6f044c58{/jobs/job,null,AVAILABLE,@Spark}
19/10/16 18:26:33 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@c017175{/jobs/job/json,null,AVAILABLE,@Spark}
19/10/16 18:26:33 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@559cedee{/stages,null,AVAILABLE,@Spark}
19/10/16 18:26:33 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@18371d89{/stages/json,null,AVAILABLE,@Spark}
19/10/16 18:26:33 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@4f3faa70{/stages/stage,null,AVAILABLE,@Spark}
19/10/16 18:26:33 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@5a8816cc{/stages/stage/json,null,AVAILABLE,@Spark}
19/10/16 18:26:33 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@68565bc7{/stages/pool,null,AVAILABLE,@Spark}
19/10/16 18:26:33 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@37baddde{/stages/pool/json,null,AVAILABLE,@Spark}
19/10/16 18:26:33 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@5115f590{/storage,null,AVAILABLE,@Spark}
19/10/16 18:26:33 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@4b31a708{/storage/json,null,AVAILABLE,@Spark}
19/10/16 18:26:33 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@11e355ca{/storage/rdd,null,AVAILABLE,@Spark}
19/10/16 18:26:33 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@70d63e05{/storage/rdd/json,null,AVAILABLE,@Spark}
19/10/16 18:26:33 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@e11ecfa{/environment,null,AVAILABLE,@Spark}
19/10/16 18:26:33 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@485e13d7{/environment/json,null,AVAILABLE,@Spark}
19/10/16 18:26:33 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@43984213{/executors,null,AVAILABLE,@Spark}
19/10/16 18:26:33 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@2ff7a73{/executors/json,null,AVAILABLE,@Spark}
19/10/16 18:26:33 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@38830ea{/executors/threadDump,null,AVAILABLE,@Spark}
19/10/16 18:26:33 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@3b705be7{/executors/threadDump/json,null,AVAILABLE,@Spark}
19/10/16 18:26:33 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@3a790e40{/executors/heapHistogram,null,AVAILABLE,@Spark}
19/10/16 18:26:33 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@43acd79e{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
19/10/16 18:26:33 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@1e1b061{/static,null,AVAILABLE,@Spark}
19/10/16 18:26:33 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@59fbb34{/,null,AVAILABLE,@Spark}
19/10/16 18:26:33 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@1b6924cb{/api,null,AVAILABLE,@Spark}
19/10/16 18:26:33 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@3ebe4ccc{/jobs/job/kill,null,AVAILABLE,@Spark}
19/10/16 18:26:33 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@6ed043d3{/stages/stage/kill,null,AVAILABLE,@Spark}
19/10/16 18:26:33 INFO SparkUI: Bound SparkUI to 10.139.64.182, and started at http://10.139.64.182:44095
19/10/16 18:26:33 WARN FairSchedulableBuilder: Fair Scheduler configuration file not found so jobs will be scheduled in FIFO order. To use fair scheduling, configure pools in fairscheduler.xml or set spark.scheduler.allocation.file to a file that contains the configuration.
19/10/16 18:26:33 INFO FairSchedulableBuilder: Created default pool: default, schedulingMode: FIFO, minShare: 0, weight: 1
19/10/16 18:26:33 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://10.139.64.182:7077...
19/10/16 18:26:33 INFO TransportClientFactory: Successfully created connection to /10.139.64.182:7077 after 43 ms (0 ms spent in bootstraps)
19/10/16 18:26:33 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20191016182633-0000
19/10/16 18:26:33 INFO TaskSchedulerImpl: Task preemption enabled.
19/10/16 18:26:33 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46123.
19/10/16 18:26:33 INFO NettyBlockTransferService: Server created on 10.139.64.182:46123
19/10/16 18:26:33 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20191016182633-0000/0 on worker-20191016182632-10.139.64.183-38329 (10.139.64.183:38329) with 16 core(s)
19/10/16 18:26:33 INFO StandaloneSchedulerBackend: Granted executor ID app-20191016182633-0000/0 on hostPort 10.139.64.183:38329 with 16 core(s), 95.6 GB RAM
19/10/16 18:26:33 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
19/10/16 18:26:33 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.139.64.182, 46123, None)
19/10/16 18:26:33 INFO BlockManagerMasterEndpoint: Registering block manager 10.139.64.182:46123 with 54.2 GB RAM, BlockManagerId(driver, 10.139.64.182, 46123, None)
19/10/16 18:26:33 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.139.64.182, 46123, None)
19/10/16 18:26:33 INFO BlockManager: external shuffle service port = 4048
19/10/16 18:26:33 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.139.64.182, 46123, None)
19/10/16 18:26:34 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20191016182633-0000/0 is now RUNNING
19/10/16 18:26:34 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@70721c12{/metrics/json,null,AVAILABLE,@Spark}
19/10/16 18:26:34 INFO DBCEventLoggingListener: Initializing DBCEventLoggingListener
19/10/16 18:26:34 INFO DBCEventLoggingListener: Logging events to eventlogs/1665401093835382800/eventlog
19/10/16 18:26:34 INFO SparkContext: Registered listener com.databricks.backend.daemon.driver.DBCEventLoggingListener
19/10/16 18:26:34 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
19/10/16 18:26:34 INFO SparkContext: Loading Spark Service RPC Server
19/10/16 18:26:34 INFO SparkServiceRPCServer: Spark Service RPC Server is disabled.
19/10/16 18:26:34 INFO DatabricksILoop$: Successfully registered spark metrics in Prometheus registry
19/10/16 18:26:34 INFO DatabricksILoop$: Successfully initialized SparkContext
19/10/16 18:26:34 INFO SharedState: Scheduler stats enabled.
19/10/16 18:26:34 INFO SharedState: loading hive config file: file:/databricks/hive/conf/hive-site.xml
19/10/16 18:26:34 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('adl://connectedsystemsuatdl.azuredatalakestore.net/user/hive/warehouse').
19/10/16 18:26:34 INFO SharedState: Warehouse path is 'adl://connectedsystemsuatdl.azuredatalakestore.net/user/hive/warehouse'.
19/10/16 18:26:34 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@25ffd826{/SQL,null,AVAILABLE,@Spark}
19/10/16 18:26:34 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@29896529{/SQL/json,null,AVAILABLE,@Spark}
19/10/16 18:26:34 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@78910096{/SQL/execution,null,AVAILABLE,@Spark}
19/10/16 18:26:34 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@39dec536{/SQL/execution/json,null,AVAILABLE,@Spark}
19/10/16 18:26:34 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@684e8c9d{/static/sql,null,AVAILABLE,@Spark}
19/10/16 18:26:34 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@61bb1e4d{/storage/iocache,null,AVAILABLE,@Spark}
19/10/16 18:26:34 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@5c53f292{/storage/iocache/json,null,AVAILABLE,@Spark}
19/10/16 18:26:34 INFO LogStore: LogStore class: class com.databricks.tahoe.store.DelegatingLogStore
19/10/16 18:26:35 INFO DatabricksILoop$: Finished creating throwaway interpreter
19/10/16 18:26:35 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
19/10/16 18:26:36 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.139.64.183:43294) with ID 0
19/10/16 18:26:36 INFO BlockManagerMasterEndpoint: Registering block manager 10.139.64.183:34813 with 50.8 GB RAM, BlockManagerId(0, 10.139.64.183, 34813, None)
19/10/16 18:26:37 INFO HiveUtils: Initializing execution hive, version 1.2.1
19/10/16 18:26:37 INFO HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
19/10/16 18:26:37 INFO ObjectStore: ObjectStore, initialize called
19/10/16 18:26:37 INFO Persistence: Property spark.hadoop.datanucleus.connectionPool.maxIdle unknown - will be ignored
19/10/16 18:26:37 INFO Persistence: Property spark.hadoop.datanucleus.connectionPool.maxPoolSize unknown - will be ignored
19/10/16 18:26:37 INFO Persistence: Property spark.hadoop.datanucleus.connectionPool.minPoolSize unknown - will be ignored
19/10/16 18:26:37 INFO Persistence: Property spark.hadoop.javax.jdo.option.ConnectionDriverName unknown - will be ignored
19/10/16 18:26:37 INFO Persistence: Property spark.hadoop.javax.jdo.option.ConnectionURL unknown - will be ignored
19/10/16 18:26:37 INFO Persistence: Property spark.hadoop.javax.jdo.option.ConnectionUserName unknown - will be ignored
19/10/16 18:26:37 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
19/10/16 18:26:37 INFO Persistence: Property spark.hadoop.javax.jdo.option.ConnectionPassword unknown - will be ignored
19/10/16 18:26:37 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
19/10/16 18:26:37 INFO Persistence: Property datanucleus.schema.autoCreateAll unknown - will be ignored
19/10/16 18:26:39 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
19/10/16 18:26:41 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
19/10/16 18:26:41 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
19/10/16 18:26:41 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
19/10/16 18:26:41 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
19/10/16 18:26:41 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
19/10/16 18:26:41 INFO ObjectStore: Initialized ObjectStore
19/10/16 18:26:41 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0
19/10/16 18:26:42 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
19/10/16 18:26:42 INFO HiveMetaStore: Added admin role in metastore
19/10/16 18:26:42 INFO HiveMetaStore: Added public role in metastore
19/10/16 18:26:42 INFO HiveMetaStore: No user is added in admin role, since config is empty
19/10/16 18:26:42 INFO HiveMetaStore: 0: get_all_databases
19/10/16 18:26:42 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_all_databases	
19/10/16 18:26:42 INFO HiveMetaStore: 0: get_functions: db=default pat=*
19/10/16 18:26:42 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
19/10/16 18:26:42 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MResourceUri" is tagged as "embedded-only" so does not have its own datastore table.
19/10/16 18:26:42 INFO JettyClient$: Creating new HttpClient with SSLContextFactory=None,maxRequestHeaderSize=65536, namePrefix=Some(DBFSV2), idleTimeout=2 hours
19/10/16 18:26:43 WARN EC2MetadataUtils: Unable to retrieve the requested metadata.
19/10/16 18:26:43 INFO deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum
19/10/16 18:26:43 INFO DatabricksFileSystemV2Factory: Creating wasbs file system for wasbs://root@dbstorage5cunvudtvyhjm.blob.core.windows.net
19/10/16 18:26:43 INFO DBFS: Initialized DBFS with DBFSV2 as the delegate.
19/10/16 18:26:43 INFO SessionState: Created local directory: /local_disk0/tmp/root
19/10/16 18:26:43 INFO SessionState: Created local directory: /local_disk0/tmp/ced865ae-36b1-469c-bded-8ce9a02c96ab_resources
19/10/16 18:26:44 INFO SessionState: Created HDFS directory: /tmp/hive/root/ced865ae-36b1-469c-bded-8ce9a02c96ab
19/10/16 18:26:44 INFO SessionState: Created local directory: /local_disk0/tmp/root/ced865ae-36b1-469c-bded-8ce9a02c96ab
19/10/16 18:26:44 INFO SessionState: Created HDFS directory: /tmp/hive/root/ced865ae-36b1-469c-bded-8ce9a02c96ab/_tmp_space.db
19/10/16 18:26:44 INFO HiveClientImpl: Warehouse location for Hive client (version 1.2.2) is adl://connectedsystemsuatdl.azuredatalakestore.net/user/hive/warehouse
19/10/16 18:26:44 INFO SessionManager: Operation log root directory is created: /local_disk0/tmp/root/operation_logs
19/10/16 18:26:44 INFO SessionManager: HiveServer2: Background operation thread pool size: 100
19/10/16 18:26:44 INFO SessionManager: HiveServer2: Background operation thread wait queue size: 100
19/10/16 18:26:44 INFO SessionManager: HiveServer2: Background operation thread keepalive time: 10 seconds
19/10/16 18:26:44 INFO AbstractService: Service:OperationManager is inited.
19/10/16 18:26:44 INFO AbstractService: Service:SessionManager is inited.
19/10/16 18:26:44 INFO AbstractService: Service: CLIService is inited.
19/10/16 18:26:44 INFO AbstractService: Service:ThriftHttpCLIService is inited.
19/10/16 18:26:44 INFO AbstractService: Service: HiveServer2 is inited.
19/10/16 18:26:44 INFO AbstractService: Service:OperationManager is started.
19/10/16 18:26:44 INFO AbstractService: Service:SessionManager is started.
19/10/16 18:26:44 INFO AbstractService: Service:CLIService is started.
19/10/16 18:26:44 INFO ObjectStore: ObjectStore, initialize called
19/10/16 18:26:44 INFO Query: Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
19/10/16 18:26:44 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
19/10/16 18:26:44 INFO ObjectStore: Initialized ObjectStore
19/10/16 18:26:44 INFO HiveMetaStore: 0: get_databases: default
19/10/16 18:26:44 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_databases: default	
19/10/16 18:26:44 INFO HiveMetaStore: 0: Shutting down the object store...
19/10/16 18:26:44 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=Shutting down the object store...	
19/10/16 18:26:44 INFO HiveMetaStore: 0: Metastore shutdown complete.
19/10/16 18:26:44 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=Metastore shutdown complete.	
19/10/16 18:26:44 INFO AbstractService: Service:ThriftHttpCLIService is started.
19/10/16 18:26:44 INFO AbstractService: Service:HiveServer2 is started.
19/10/16 18:26:44 INFO ThriftCLIService: HTTP Server SSL: adding excluded protocols: [SSLv2, SSLv3]
19/10/16 18:26:44 INFO ThriftCLIService: HTTP Server SSL: SslContextFactory.getExcludeProtocols = [SSL, SSLv2, SSLv2Hello, SSLv3]
19/10/16 18:26:44 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@5cef5fc9{/sqlserver,null,AVAILABLE,@Spark}
19/10/16 18:26:44 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@b867869{/sqlserver/json,null,AVAILABLE,@Spark}
19/10/16 18:26:44 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@6d5fea64{/sqlserver/session,null,AVAILABLE,@Spark}
19/10/16 18:26:44 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@4a7427f9{/sqlserver/session/json,null,AVAILABLE,@Spark}
19/10/16 18:26:44 INFO DriverDaemon: Starting driver daemon...
19/10/16 18:26:44 INFO SparkConfUtils$: Customize spark config according to file /tmp/custom-spark.conf
19/10/16 18:26:44 INFO SparkConfUtils$: new spark config: spark.sql.warehouse.dir -> adl://connectedsystemsuatdl.azuredatalakestore.net/user/hive/warehouse
19/10/16 18:26:44 INFO SparkConfUtils$: new spark config: spark.hadoop.javax.jdo.option.ConnectionPassword -> 560I890@fd80709
19/10/16 18:26:44 INFO SparkConfUtils$: new spark config: spark.hadoop.dfs.adls.oauth2.credential -> viUHaEfNg8cHQRJSRNMzpIv1T6goiUg+c9R/XSbYBFA=
19/10/16 18:26:44 INFO SparkConfUtils$: new spark config: spark.hadoop.javax.jdo.option.ConnectionDriverName -> org.postgresql.Driver
19/10/16 18:26:44 INFO SparkConfUtils$: new spark config: spark.databricks.acl.dfAclsEnabled -> false
19/10/16 18:26:44 INFO SparkConfUtils$: new spark config: spark.driver.tempDirectory -> /local_disk0/tmp
19/10/16 18:26:44 INFO SparkConfUtils$: new spark config: spark.hadoop.datanucleus.connectionPool.maxIdle -> 1
19/10/16 18:26:44 INFO SparkConfUtils$: new spark config: spark.hadoop.dfs.adls.oauth2.client.id -> 003106c3-ff8c-4c62-a060-24b8b32ea512
19/10/16 18:26:44 INFO SparkConfUtils$: new spark config: spark.hadoop.dfs.adls.oauth2.refresh.url -> https://login.microsoftonline.com/6ac7a1f4-5fb1-4153-bb4f-12d2020a1f7d/oauth2/token
19/10/16 18:26:44 INFO SparkConfUtils$: new spark config: spark.executor.tempDirectory -> /local_disk0/tmp
19/10/16 18:26:44 INFO SparkConfUtils$: new spark config: spark.databricks.delta.preview.enabled -> true
19/10/16 18:26:44 INFO SparkConfUtils$: new spark config: spark.task.maxFailures -> 10
19/10/16 18:26:44 INFO SparkConfUtils$: new spark config: spark.sql.hive.metastore.jars -> maven
19/10/16 18:26:44 INFO SparkConfUtils$: new spark config: spark.databricks.aggressiveWindowDownS -> 300
19/10/16 18:26:44 INFO SparkConfUtils$: new spark config: spark.hadoop.javax.jdo.option.ConnectionUserName -> tedbuat
19/10/16 18:26:44 INFO SparkConfUtils$: new spark config: spark.databricks.sql.hive.msck.batchSize -> 200
19/10/16 18:26:44 INFO SparkConfUtils$: new spark config: spark.hadoop.javax.jdo.option.ConnectionURL -> jdbc:postgresql://uathmsdb.trans.enterprisecsp.net:5432/hive_adls
19/10/16 18:26:44 INFO SparkConfUtils$: new spark config: datanucleus.fixedDatastore -> false
19/10/16 18:26:44 INFO SparkConfUtils$: new spark config: spark.databricks.sql.hive.msck.parallelism -> 20
19/10/16 18:26:44 INFO SparkConfUtils$: new spark config: spark.hadoop.datanucleus.connectionPool.maxPoolSize -> 30
19/10/16 18:26:44 INFO SparkConfUtils$: new spark config: spark.hadoop.datanucleus.connectionPool.minPoolSize -> 15
19/10/16 18:26:44 INFO SparkConfUtils$: new spark config: spark.sql.parquet.filterPushdown -> false
19/10/16 18:26:44 INFO SparkConfUtils$: new spark config: spark.hadoop.dfs.adls.oauth2.access.token.provider.type -> ClientCredential
19/10/16 18:26:44 INFO SparkConfUtils$: new spark config: spark.sql.shuffle.partitions -> 2000
19/10/16 18:26:44 INFO SparkConfUtils$: new spark config: spark.driver.maxResultSize -> 0
19/10/16 18:26:44 INFO SparkConfUtils$: new spark config: fs.azure.account.key.teuatsa.blob.core.windows.net -> IsrxMnnzLeO9N44beU4eMw0LaiKJdi1hHmKZFUiPa8nFNKZAFyhH0Hv/UCoC4qhgcdq63zwTRGFhUKV9d4ZsBw==
19/10/16 18:26:44 INFO SparkConfUtils$: new spark config: datanucleus.autoCreateSchema -> true
19/10/16 18:26:44 INFO SparkConfUtils$: new spark config: spark.hadoop.fs.adl.impl.disable.cache -> false
19/10/16 18:26:44 INFO SparkConfUtils$: new spark config: spark.sql.hive.metastore.version -> 2.3.0
19/10/16 18:26:44 WARN SparkConf: The configuration key 'spark.scheduler.listenerbus.eventqueue.size' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.scheduler.listenerbus.eventqueue.capacity' instead.
19/10/16 18:26:44 WARN SparkConf: The configuration key 'spark.akka.frameSize' has been deprecated as of Spark 1.6 and may be removed in the future. Please use the new key 'spark.rpc.message.maxSize' instead.
19/10/16 18:26:44 INFO DriverDaemon$$anon$1: Message out thread ready
19/10/16 18:26:44 INFO Server: jetty-9.3.20.v20170531
19/10/16 18:26:44 INFO AbstractConnector: Started ServerConnector@3ec6f3f1{HTTP/1.1,[http/1.1]}{0.0.0.0:6061}
19/10/16 18:26:44 INFO Server: Started @15904ms
19/10/16 18:26:44 INFO DriverDaemon: Driver daemon started.
19/10/16 18:26:44 INFO Server: jetty-9.3.20.v20170531
19/10/16 18:26:44 WARN SecurityHandler: ServletContext@o.e.j.s.ServletContextHandler@12ed21b8{/,null,STARTING} has uncovered http methods for path: /*
19/10/16 18:26:44 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@12ed21b8{/,null,AVAILABLE}
19/10/16 18:26:44 INFO SslContextFactory: x509=X509@15067e61(1,h=[databrickscloud.com],w=[]) for SslContextFactory@44c51cce(file:///databricks/keys/jetty-ssl-driver-keystore.jks,null)
19/10/16 18:26:44 INFO AbstractConnector: Started ServerConnector@3eafa094{SSL,[ssl, http/1.1]}{0.0.0.0:10000}
19/10/16 18:26:44 INFO Server: Started @16041ms
19/10/16 18:26:44 INFO ThriftCLIService: Started ThriftHttpCLIService in https mode on port 10000 path=/cliservice/* with 5...500 worker threads
19/10/16 18:26:45 INFO DriverCorral: Loading the root classloader
19/10/16 18:26:45 INFO DriverCorral: Starting repl ReplId-d665e-8c490-826d7
19/10/16 18:26:45 INFO DriverCorral: Starting repl ReplId-3b992-2b775-e808d-9
19/10/16 18:26:45 INFO SQLDriverWrapper: setupRepl:ReplId-d665e-8c490-826d7: finished to load
19/10/16 18:26:45 INFO SQLDriverWrapper: setupRepl:ReplId-3b992-2b775-e808d-9: finished to load
19/10/16 18:26:45 INFO DriverCorral: Starting repl ReplId-2c802-70b60-fd726-0
19/10/16 18:26:45 INFO SQLDriverWrapper: setupRepl:ReplId-2c802-70b60-fd726-0: finished to load
19/10/16 18:26:45 INFO DriverCorral: Starting repl ReplId-6f4bb-b6470-7fb04-2
19/10/16 18:26:45 INFO SQLDriverWrapper: setupRepl:ReplId-6f4bb-b6470-7fb04-2: finished to load
19/10/16 18:26:45 INFO DriverCorral: Starting repl ReplId-ea1f7-af58d-6467a
19/10/16 18:26:45 INFO SQLDriverWrapper: setupRepl:ReplId-ea1f7-af58d-6467a: finished to load
19/10/16 18:26:45 INFO DriverCorral: Starting repl ReplId-4a914-4924d-b1e83
19/10/16 18:26:45 INFO RDriverLocal: 1. RDriverLocal.85642a15-7773-42bb-9815-998e829d34c5: object created with for ReplId-4a914-4924d-b1e83.
19/10/16 18:26:45 INFO RDriverLocal: 2. RDriverLocal.85642a15-7773-42bb-9815-998e829d34c5: initializing ...
19/10/16 18:26:45 INFO RDriverLocal: 3. RDriverLocal.85642a15-7773-42bb-9815-998e829d34c5: started RBackend thread on port 42665
19/10/16 18:26:45 INFO RDriverLocal: 4. RDriverLocal.85642a15-7773-42bb-9815-998e829d34c5: waiting for SparkR to be installed ...
19/10/16 18:26:51 INFO RDriverLocal$: SparkR installation completed.
19/10/16 18:26:51 INFO RDriverLocal: 5. RDriverLocal.85642a15-7773-42bb-9815-998e829d34c5: launching R process ...
19/10/16 18:26:51 INFO RDriverLocal: 6. RDriverLocal.85642a15-7773-42bb-9815-998e829d34c5: cgroup isolation disabled, not placing R process in REPL cgroup.
19/10/16 18:26:51 INFO RDriverLocal: 7. RDriverLocal.85642a15-7773-42bb-9815-998e829d34c5: starting R process on port 47138 (attempt 1) ...
19/10/16 18:26:52 INFO RDriverLocal: 8. RDriverLocal.85642a15-7773-42bb-9815-998e829d34c5: R process started with RServe listening on port 47138.
19/10/16 18:26:52 INFO RDriverLocal: 9. RDriverLocal.85642a15-7773-42bb-9815-998e829d34c5: setting up BufferedStreamThread with bufferSize: 100.
19/10/16 18:26:53 INFO RDriverLocal: 10. RDriverLocal.85642a15-7773-42bb-9815-998e829d34c5: starting interpreter to talk to R process ...
19/10/16 18:26:54 WARN SparkContext: Using an existing SparkContext; some configuration may not take effect.
19/10/16 18:26:54 INFO RDriverLocal: 11. RDriverLocal.85642a15-7773-42bb-9815-998e829d34c5: R interpretter is connected.
19/10/16 18:26:54 INFO RDriverWrapper: setupRepl:ReplId-4a914-4924d-b1e83: finished to load
19/10/16 18:26:57 INFO LibraryDownloadManager: Downloading a library that was not in the cache: UploadedLibrary(org/rogach/scallop_2.11-3.3.1.jar,dbfs:/FileStore/jars/maven/org/rogach/scallop_2.11-3.3.1.jar,scala)
19/10/16 18:26:57 INFO LibraryDownloadManager: Downloaded library org/rogach/scallop_2.11-3.3.1.jar as local file /local_disk0/tmp/addedFile6883373321297042917org_rogach_scallop_2_11_3_3_1-ecc25.jar
19/10/16 18:26:57 INFO SharedDriverContext: Adding libraries that haven't been added before: Set(/local_disk0/tmp/addedFile6883373321297042917org_rogach_scallop_2_11_3_3_1-ecc25.jar)
19/10/16 18:26:57 INFO SparkContext: Added file /local_disk0/tmp/addedFile6883373321297042917org_rogach_scallop_2_11_3_3_1-ecc25.jar at spark://10.139.64.182:33603/files/addedFile6883373321297042917org_rogach_scallop_2_11_3_3_1-ecc25.jar with timestamp 1571250417952
19/10/16 18:26:57 INFO Utils: Copying /local_disk0/tmp/addedFile6883373321297042917org_rogach_scallop_2_11_3_3_1-ecc25.jar to /local_disk0/spark-1c8a2322-0b98-4da4-8056-97823a6fb0ca/userFiles-fc630912-4dbf-4b5d-ba91-5df8acdf08e6/addedFile6883373321297042917org_rogach_scallop_2_11_3_3_1-ecc25.jar
19/10/16 18:26:57 INFO SparkContext: Added JAR /local_disk0/tmp/addedFile6883373321297042917org_rogach_scallop_2_11_3_3_1-ecc25.jar at spark://10.139.64.182:33603/jars/addedFile6883373321297042917org_rogach_scallop_2_11_3_3_1-ecc25.jar with timestamp 1571250417966
19/10/16 18:26:57 INFO LibraryDownloadManager: Downloading a library that was not in the cache: UploadedLibrary(com/typesafe/config-1.3.1.jar,dbfs:/FileStore/jars/maven/com/typesafe/config-1.3.1.jar,scala)
19/10/16 18:26:58 INFO LibraryDownloadManager: Downloaded library com/typesafe/config-1.3.1.jar as local file /local_disk0/tmp/addedFile5162712074542881962com_typesafe_config_1_3_1-e3e42.jar
19/10/16 18:26:58 INFO SharedDriverContext: Adding libraries that haven't been added before: Set(/local_disk0/tmp/addedFile5162712074542881962com_typesafe_config_1_3_1-e3e42.jar)
19/10/16 18:26:58 INFO SparkContext: Added file /local_disk0/tmp/addedFile5162712074542881962com_typesafe_config_1_3_1-e3e42.jar at spark://10.139.64.182:33603/files/addedFile5162712074542881962com_typesafe_config_1_3_1-e3e42.jar with timestamp 1571250418138
19/10/16 18:26:58 INFO Utils: Copying /local_disk0/tmp/addedFile5162712074542881962com_typesafe_config_1_3_1-e3e42.jar to /local_disk0/spark-1c8a2322-0b98-4da4-8056-97823a6fb0ca/userFiles-fc630912-4dbf-4b5d-ba91-5df8acdf08e6/addedFile5162712074542881962com_typesafe_config_1_3_1-e3e42.jar
19/10/16 18:26:58 INFO SparkContext: Added JAR /local_disk0/tmp/addedFile5162712074542881962com_typesafe_config_1_3_1-e3e42.jar at spark://10.139.64.182:33603/jars/addedFile5162712074542881962com_typesafe_config_1_3_1-e3e42.jar with timestamp 1571250418145
19/10/16 18:26:58 INFO LibraryDownloadManager: Downloading a library that was not in the cache: UploadedLibrary(xml-apis/xml-apis-1.0.b2.jar,dbfs:/FileStore/jars/maven/xml-apis/xml-apis-1.0.b2.jar,scala)
19/10/16 18:26:58 INFO LibraryDownloadManager: Downloaded library xml-apis/xml-apis-1.0.b2.jar as local file /local_disk0/tmp/addedFile8624879695200027925xml_apis_xml_apis_1_0_b2-167e6.jar
19/10/16 18:26:58 INFO SharedDriverContext: Adding libraries that haven't been added before: Set(/local_disk0/tmp/addedFile8624879695200027925xml_apis_xml_apis_1_0_b2-167e6.jar)
19/10/16 18:26:58 INFO SparkContext: Added file /local_disk0/tmp/addedFile8624879695200027925xml_apis_xml_apis_1_0_b2-167e6.jar at spark://10.139.64.182:33603/files/addedFile8624879695200027925xml_apis_xml_apis_1_0_b2-167e6.jar with timestamp 1571250418235
19/10/16 18:26:58 INFO Utils: Copying /local_disk0/tmp/addedFile8624879695200027925xml_apis_xml_apis_1_0_b2-167e6.jar to /local_disk0/spark-1c8a2322-0b98-4da4-8056-97823a6fb0ca/userFiles-fc630912-4dbf-4b5d-ba91-5df8acdf08e6/addedFile8624879695200027925xml_apis_xml_apis_1_0_b2-167e6.jar
19/10/16 18:26:58 INFO SparkContext: Added JAR /local_disk0/tmp/addedFile8624879695200027925xml_apis_xml_apis_1_0_b2-167e6.jar at spark://10.139.64.182:33603/jars/addedFile8624879695200027925xml_apis_xml_apis_1_0_b2-167e6.jar with timestamp 1571250418245
19/10/16 18:26:58 INFO LibraryDownloadManager: Downloading a library that was not in the cache: UploadedLibrary(dom4j/dom4j-1.6.1.jar,dbfs:/FileStore/jars/maven/dom4j/dom4j-1.6.1.jar,scala)
19/10/16 18:26:58 INFO LibraryDownloadManager: Downloaded library dom4j/dom4j-1.6.1.jar as local file /local_disk0/tmp/addedFile236936972251169783dom4j_dom4j_1_6_1-3b35e.jar
19/10/16 18:26:58 INFO SharedDriverContext: Adding libraries that haven't been added before: Set(/local_disk0/tmp/addedFile236936972251169783dom4j_dom4j_1_6_1-3b35e.jar)
19/10/16 18:26:58 INFO SparkContext: Added file /local_disk0/tmp/addedFile236936972251169783dom4j_dom4j_1_6_1-3b35e.jar at spark://10.139.64.182:33603/files/addedFile236936972251169783dom4j_dom4j_1_6_1-3b35e.jar with timestamp 1571250418346
19/10/16 18:26:58 INFO Utils: Copying /local_disk0/tmp/addedFile236936972251169783dom4j_dom4j_1_6_1-3b35e.jar to /local_disk0/spark-1c8a2322-0b98-4da4-8056-97823a6fb0ca/userFiles-fc630912-4dbf-4b5d-ba91-5df8acdf08e6/addedFile236936972251169783dom4j_dom4j_1_6_1-3b35e.jar
19/10/16 18:26:58 INFO SparkContext: Added JAR /local_disk0/tmp/addedFile236936972251169783dom4j_dom4j_1_6_1-3b35e.jar at spark://10.139.64.182:33603/jars/addedFile236936972251169783dom4j_dom4j_1_6_1-3b35e.jar with timestamp 1571250418356
19/10/16 18:26:58 INFO LibraryDownloadManager: Downloading a library that was not in the cache: UploadedLibrary(dbfs:/FileStore/saak9001/pqm_core_models-1.0.jar,dbfs:/FileStore/saak9001/pqm_core_models-1.0.jar,scala)
19/10/16 18:26:58 INFO LibraryDownloadManager: Downloaded library dbfs:/FileStore/saak9001/pqm_core_models-1.0.jar as local file /local_disk0/tmp/addedFile2365597673424539553dbfs__FileStore_saak9001_pqm_core_models_1_0-b0ccd.jar
19/10/16 18:26:58 INFO SharedDriverContext: Adding libraries that haven't been added before: Set(/local_disk0/tmp/addedFile2365597673424539553dbfs__FileStore_saak9001_pqm_core_models_1_0-b0ccd.jar)
19/10/16 18:26:58 INFO SparkContext: Added file /local_disk0/tmp/addedFile2365597673424539553dbfs__FileStore_saak9001_pqm_core_models_1_0-b0ccd.jar at spark://10.139.64.182:33603/files/addedFile2365597673424539553dbfs__FileStore_saak9001_pqm_core_models_1_0-b0ccd.jar with timestamp 1571250418573
19/10/16 18:26:58 INFO Utils: Copying /local_disk0/tmp/addedFile2365597673424539553dbfs__FileStore_saak9001_pqm_core_models_1_0-b0ccd.jar to /local_disk0/spark-1c8a2322-0b98-4da4-8056-97823a6fb0ca/userFiles-fc630912-4dbf-4b5d-ba91-5df8acdf08e6/addedFile2365597673424539553dbfs__FileStore_saak9001_pqm_core_models_1_0-b0ccd.jar
19/10/16 18:26:58 INFO SparkContext: Added JAR /local_disk0/tmp/addedFile2365597673424539553dbfs__FileStore_saak9001_pqm_core_models_1_0-b0ccd.jar at spark://10.139.64.182:33603/jars/addedFile2365597673424539553dbfs__FileStore_saak9001_pqm_core_models_1_0-b0ccd.jar with timestamp 1571250418581
19/10/16 18:26:58 INFO LibraryDownloadManager: Downloading a library that was not in the cache: UploadedLibrary(io/spray/spray-json_2.11-1.3.5.jar,dbfs:/FileStore/jars/maven/io/spray/spray-json_2.11-1.3.5.jar,scala)
19/10/16 18:26:58 INFO LibraryDownloadManager: Downloaded library io/spray/spray-json_2.11-1.3.5.jar as local file /local_disk0/tmp/addedFile5541486089544592835io_spray_spray_json_2_11_1_3_5-d5f78.jar
19/10/16 18:26:58 INFO SharedDriverContext: Adding libraries that haven't been added before: Set(/local_disk0/tmp/addedFile5541486089544592835io_spray_spray_json_2_11_1_3_5-d5f78.jar)
19/10/16 18:26:58 INFO SparkContext: Added file /local_disk0/tmp/addedFile5541486089544592835io_spray_spray_json_2_11_1_3_5-d5f78.jar at spark://10.139.64.182:33603/files/addedFile5541486089544592835io_spray_spray_json_2_11_1_3_5-d5f78.jar with timestamp 1571250418676
19/10/16 18:26:58 INFO Utils: Copying /local_disk0/tmp/addedFile5541486089544592835io_spray_spray_json_2_11_1_3_5-d5f78.jar to /local_disk0/spark-1c8a2322-0b98-4da4-8056-97823a6fb0ca/userFiles-fc630912-4dbf-4b5d-ba91-5df8acdf08e6/addedFile5541486089544592835io_spray_spray_json_2_11_1_3_5-d5f78.jar
19/10/16 18:26:58 INFO SparkContext: Added JAR /local_disk0/tmp/addedFile5541486089544592835io_spray_spray_json_2_11_1_3_5-d5f78.jar at spark://10.139.64.182:33603/jars/addedFile5541486089544592835io_spray_spray_json_2_11_1_3_5-d5f78.jar with timestamp 1571250418683
19/10/16 18:26:58 INFO LibraryDownloadManager: Downloading a library that was not in the cache: UploadedLibrary(dbfs:/FileStore/saak9001/transengine-1.0-SNAPSHOT.jar,dbfs:/FileStore/saak9001/transengine-1.0-SNAPSHOT.jar,scala)
19/10/16 18:26:58 INFO LibraryDownloadManager: Downloaded library dbfs:/FileStore/saak9001/transengine-1.0-SNAPSHOT.jar as local file /local_disk0/tmp/addedFile5681071070702781128dbfs__FileStore_saak9001_transengine_1_0_SNAPSHOT-ab9b9.jar
19/10/16 18:26:58 INFO SharedDriverContext: Adding libraries that haven't been added before: Set(/local_disk0/tmp/addedFile5681071070702781128dbfs__FileStore_saak9001_transengine_1_0_SNAPSHOT-ab9b9.jar)
19/10/16 18:26:58 INFO SparkContext: Added file /local_disk0/tmp/addedFile5681071070702781128dbfs__FileStore_saak9001_transengine_1_0_SNAPSHOT-ab9b9.jar at spark://10.139.64.182:33603/files/addedFile5681071070702781128dbfs__FileStore_saak9001_transengine_1_0_SNAPSHOT-ab9b9.jar with timestamp 1571250418753
19/10/16 18:26:58 INFO Utils: Copying /local_disk0/tmp/addedFile5681071070702781128dbfs__FileStore_saak9001_transengine_1_0_SNAPSHOT-ab9b9.jar to /local_disk0/spark-1c8a2322-0b98-4da4-8056-97823a6fb0ca/userFiles-fc630912-4dbf-4b5d-ba91-5df8acdf08e6/addedFile5681071070702781128dbfs__FileStore_saak9001_transengine_1_0_SNAPSHOT-ab9b9.jar
19/10/16 18:26:58 INFO SparkContext: Added JAR /local_disk0/tmp/addedFile5681071070702781128dbfs__FileStore_saak9001_transengine_1_0_SNAPSHOT-ab9b9.jar at spark://10.139.64.182:33603/jars/addedFile5681071070702781128dbfs__FileStore_saak9001_transengine_1_0_SNAPSHOT-ab9b9.jar with timestamp 1571250418761
19/10/16 18:26:58 INFO LibraryDownloadManager: Downloading a library that was not in the cache: UploadedLibrary(org/scalaj/scalaj-http_2.11-2.3.0.jar,dbfs:/FileStore/jars/maven/org/scalaj/scalaj-http_2.11-2.3.0.jar,scala)
19/10/16 18:26:58 INFO LibraryDownloadManager: Downloaded library org/scalaj/scalaj-http_2.11-2.3.0.jar as local file /local_disk0/tmp/addedFile6674954176252367935org_scalaj_scalaj_http_2_11_2_3_0-d69e4.jar
19/10/16 18:26:58 INFO SharedDriverContext: Adding libraries that haven't been added before: Set(/local_disk0/tmp/addedFile6674954176252367935org_scalaj_scalaj_http_2_11_2_3_0-d69e4.jar)
19/10/16 18:26:58 INFO SparkContext: Added file /local_disk0/tmp/addedFile6674954176252367935org_scalaj_scalaj_http_2_11_2_3_0-d69e4.jar at spark://10.139.64.182:33603/files/addedFile6674954176252367935org_scalaj_scalaj_http_2_11_2_3_0-d69e4.jar with timestamp 1571250418848
19/10/16 18:26:58 INFO Utils: Copying /local_disk0/tmp/addedFile6674954176252367935org_scalaj_scalaj_http_2_11_2_3_0-d69e4.jar to /local_disk0/spark-1c8a2322-0b98-4da4-8056-97823a6fb0ca/userFiles-fc630912-4dbf-4b5d-ba91-5df8acdf08e6/addedFile6674954176252367935org_scalaj_scalaj_http_2_11_2_3_0-d69e4.jar
19/10/16 18:26:58 INFO SparkContext: Added JAR /local_disk0/tmp/addedFile6674954176252367935org_scalaj_scalaj_http_2_11_2_3_0-d69e4.jar at spark://10.139.64.182:33603/jars/addedFile6674954176252367935org_scalaj_scalaj_http_2_11_2_3_0-d69e4.jar with timestamp 1571250418854
19/10/16 18:29:37 INFO DriverCorral: Starting repl ReplId-34660-d3c19-e6202-c
19/10/16 18:29:37 INFO LogicalPlanStats: Setting LogicalPlanStats visitor to com.databricks.sql.optimizer.statsEstimation.DatabricksLogicalPlanStatsVisitor$
19/10/16 18:29:38 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.0 using maven.
19/10/16 18:29:38 INFO IsolatedClientLoader: Initiating download of metastore jars from maven. This may take a while and is not recommended for production use. Please follow the instructions here: https://docs.databricks.com/user-guide/advanced/external-hive-metastore.html#spark-options on how to download the jars just once and use them in your cluster configuration. A log message beginning with 'Downloaded metastore jars' will print once the download is complete.
19/10/16 18:30:32 INFO IsolatedClientLoader: Downloaded metastore jars to /local_disk0/tmp/hive-v2_3-d104df53-6740-475e-b7d7-f5814b249fe0
19/10/16 18:30:32 INFO HiveConf: Found configuration file null
19/10/16 18:30:32 INFO SessionState: Created HDFS directory: /tmp/hive/root/74f4940e-a6bf-4ecf-b225-d4debe0f74b4
19/10/16 18:30:32 INFO SessionState: Created local directory: /local_disk0/tmp/root/74f4940e-a6bf-4ecf-b225-d4debe0f74b4
19/10/16 18:30:32 INFO SessionState: Created HDFS directory: /tmp/hive/root/74f4940e-a6bf-4ecf-b225-d4debe0f74b4/_tmp_space.db
19/10/16 18:30:32 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.3) is adl://connectedsystemsuatdl.azuredatalakestore.net/user/hive/warehouse
19/10/16 18:30:33 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
19/10/16 18:30:33 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
19/10/16 18:30:33 INFO HiveMetaStore: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
19/10/16 18:30:33 INFO ObjectStore: ObjectStore, initialize called
19/10/16 18:30:33 INFO Persistence: Property spark.hadoop.datanucleus.connectionPool.maxIdle unknown - will be ignored
19/10/16 18:30:33 INFO Persistence: Property spark.hadoop.datanucleus.connectionPool.maxPoolSize unknown - will be ignored
19/10/16 18:30:33 INFO Persistence: Property datanucleus.fixedDatastore unknown - will be ignored
19/10/16 18:30:33 INFO Persistence: Property spark.hadoop.datanucleus.connectionPool.minPoolSize unknown - will be ignored
19/10/16 18:30:33 INFO Persistence: Property spark.hadoop.javax.jdo.option.ConnectionDriverName unknown - will be ignored
19/10/16 18:30:33 INFO Persistence: Property spark.hadoop.javax.jdo.option.ConnectionURL unknown - will be ignored
19/10/16 18:30:33 INFO Persistence: Property spark.hadoop.javax.jdo.option.ConnectionUserName unknown - will be ignored
19/10/16 18:30:33 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
19/10/16 18:30:33 INFO Persistence: Property spark.hadoop.javax.jdo.option.ConnectionPassword unknown - will be ignored
19/10/16 18:30:33 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
19/10/16 18:30:34 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
19/10/16 18:30:35 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is POSTGRES
19/10/16 18:30:35 INFO ObjectStore: Initialized ObjectStore
19/10/16 18:30:35 INFO HiveMetaStore: Added admin role in metastore
19/10/16 18:30:35 INFO HiveMetaStore: Added public role in metastore
19/10/16 18:30:35 INFO HiveMetaStore: No user is added in admin role, since config is empty
19/10/16 18:30:35 INFO HiveMetaStore: 0: get_all_functions
19/10/16 18:30:35 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_all_functions	
19/10/16 18:30:35 INFO Hive: Registering function collect brickhouse.udf.collect.CollectUDAF
19/10/16 18:30:35 INFO Hive: Registering function collect brickhouse.udf.collect.CollectUDAF
19/10/16 18:30:35 INFO HiveMetaStore: 0: get_database: default
19/10/16 18:30:35 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
19/10/16 18:30:35 INFO SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=74f4940e-a6bf-4ecf-b225-d4debe0f74b4, clientType=HIVECLI]
19/10/16 18:30:35 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.
19/10/16 18:30:35 INFO metastore: Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
19/10/16 18:30:35 INFO HiveMetaStore: 0: Cleaning up thread local RawStore...
19/10/16 18:30:35 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
19/10/16 18:30:35 INFO HiveMetaStore: 0: Done cleaning up thread local RawStore
19/10/16 18:30:35 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
19/10/16 18:30:35 INFO SessionState: Added [/local_disk0/tmp/addedFile8624879695200027925xml_apis_xml_apis_1_0_b2-167e6.jar] to class path
19/10/16 18:30:35 INFO SessionState: Added resources: [/local_disk0/tmp/addedFile8624879695200027925xml_apis_xml_apis_1_0_b2-167e6.jar]
19/10/16 18:30:35 WARN SparkContext: The jar /local_disk0/tmp/addedFile8624879695200027925xml_apis_xml_apis_1_0_b2-167e6.jar has been added already. Overwriting of added jars is not supported in the current version.
19/10/16 18:30:35 INFO SessionState: Added [/local_disk0/tmp/addedFile6674954176252367935org_scalaj_scalaj_http_2_11_2_3_0-d69e4.jar] to class path
19/10/16 18:30:35 INFO SessionState: Added resources: [/local_disk0/tmp/addedFile6674954176252367935org_scalaj_scalaj_http_2_11_2_3_0-d69e4.jar]
19/10/16 18:30:35 WARN SparkContext: The jar /local_disk0/tmp/addedFile6674954176252367935org_scalaj_scalaj_http_2_11_2_3_0-d69e4.jar has been added already. Overwriting of added jars is not supported in the current version.
19/10/16 18:30:35 INFO SessionState: Added [/local_disk0/tmp/addedFile5681071070702781128dbfs__FileStore_saak9001_transengine_1_0_SNAPSHOT-ab9b9.jar] to class path
19/10/16 18:30:35 INFO SessionState: Added resources: [/local_disk0/tmp/addedFile5681071070702781128dbfs__FileStore_saak9001_transengine_1_0_SNAPSHOT-ab9b9.jar]
19/10/16 18:30:35 WARN SparkContext: The jar /local_disk0/tmp/addedFile5681071070702781128dbfs__FileStore_saak9001_transengine_1_0_SNAPSHOT-ab9b9.jar has been added already. Overwriting of added jars is not supported in the current version.
19/10/16 18:30:35 INFO SessionState: Added [/local_disk0/tmp/addedFile6883373321297042917org_rogach_scallop_2_11_3_3_1-ecc25.jar] to class path
19/10/16 18:30:35 INFO SessionState: Added resources: [/local_disk0/tmp/addedFile6883373321297042917org_rogach_scallop_2_11_3_3_1-ecc25.jar]
19/10/16 18:30:35 WARN SparkContext: The jar /local_disk0/tmp/addedFile6883373321297042917org_rogach_scallop_2_11_3_3_1-ecc25.jar has been added already. Overwriting of added jars is not supported in the current version.
19/10/16 18:30:35 INFO SessionState: Added [/local_disk0/tmp/addedFile2365597673424539553dbfs__FileStore_saak9001_pqm_core_models_1_0-b0ccd.jar] to class path
19/10/16 18:30:35 INFO SessionState: Added resources: [/local_disk0/tmp/addedFile2365597673424539553dbfs__FileStore_saak9001_pqm_core_models_1_0-b0ccd.jar]
19/10/16 18:30:35 WARN SparkContext: The jar /local_disk0/tmp/addedFile2365597673424539553dbfs__FileStore_saak9001_pqm_core_models_1_0-b0ccd.jar has been added already. Overwriting of added jars is not supported in the current version.
19/10/16 18:30:35 INFO SessionState: Added [/local_disk0/tmp/addedFile5162712074542881962com_typesafe_config_1_3_1-e3e42.jar] to class path
19/10/16 18:30:35 INFO SessionState: Added resources: [/local_disk0/tmp/addedFile5162712074542881962com_typesafe_config_1_3_1-e3e42.jar]
19/10/16 18:30:35 WARN SparkContext: The jar /local_disk0/tmp/addedFile5162712074542881962com_typesafe_config_1_3_1-e3e42.jar has been added already. Overwriting of added jars is not supported in the current version.
19/10/16 18:30:35 INFO SessionState: Added [/local_disk0/tmp/addedFile5541486089544592835io_spray_spray_json_2_11_1_3_5-d5f78.jar] to class path
19/10/16 18:30:35 INFO SessionState: Added resources: [/local_disk0/tmp/addedFile5541486089544592835io_spray_spray_json_2_11_1_3_5-d5f78.jar]
19/10/16 18:30:35 WARN SparkContext: The jar /local_disk0/tmp/addedFile5541486089544592835io_spray_spray_json_2_11_1_3_5-d5f78.jar has been added already. Overwriting of added jars is not supported in the current version.
19/10/16 18:30:35 INFO SessionState: Added [/local_disk0/tmp/addedFile236936972251169783dom4j_dom4j_1_6_1-3b35e.jar] to class path
19/10/16 18:30:35 INFO SessionState: Added resources: [/local_disk0/tmp/addedFile236936972251169783dom4j_dom4j_1_6_1-3b35e.jar]
19/10/16 18:30:35 WARN SparkContext: The jar /local_disk0/tmp/addedFile236936972251169783dom4j_dom4j_1_6_1-3b35e.jar has been added already. Overwriting of added jars is not supported in the current version.
19/10/16 18:30:35 INFO ScalaDriverWrapper: setupRepl:ReplId-34660-d3c19-e6202-c: finished to load
19/10/16 18:30:35 WARN ScalaDriverLocal: User entity information is incomplete, missing userId,orgId,user.
19/10/16 18:30:35 INFO ProgressReporter$: Added result fetcher for 3775719889380450348_7575602824216384756_d4819c00-cf2d-4bb4-bd4e-c30137c092c3
19/10/16 18:30:36 INFO SignalUtils: Registered signal handler for INT
19/10/16 18:30:36 INFO SQLAppStatusListener: Execution ID: 0 Total Executor Run Time: 0
19/10/16 18:30:36 INFO SQLAppStatusListener: Execution ID: 1 Total Executor Run Time: 0
19/10/16 18:30:36 INFO SQLAppStatusListener: Execution ID: 2 Total Executor Run Time: 0
19/10/16 18:30:36 INFO SQLAppStatusListener: Execution ID: 3 Total Executor Run Time: 0
19/10/16 18:30:36 INFO SQLAppStatusListener: Execution ID: 4 Total Executor Run Time: 0
19/10/16 18:30:36 INFO SQLAppStatusListener: Execution ID: 5 Total Executor Run Time: 0
19/10/16 18:30:36 INFO SQLAppStatusListener: Execution ID: 6 Total Executor Run Time: 0
19/10/16 18:30:36 INFO SQLAppStatusListener: Execution ID: 7 Total Executor Run Time: 0
19/10/16 18:30:37 INFO DriverILoop: Set class prefix to: line6505de3626214813a08a9d42e5e5e322
19/10/16 18:30:37 INFO DriverILoop: set ContextClassLoader
19/10/16 18:30:37 INFO DriverILoop: initialized intp
19/10/16 18:30:40 INFO ProgressReporter$: Removed result fetcher for 3775719889380450348_7575602824216384756_d4819c00-cf2d-4bb4-bd4e-c30137c092c3
19/10/16 18:30:44 INFO DriverCorral: Starting repl ReplId-24102-44cb9-7d5d4-a
19/10/16 18:30:44 INFO LogicalPlanStats: Setting LogicalPlanStats visitor to com.databricks.sql.optimizer.statsEstimation.DatabricksLogicalPlanStatsVisitor$
19/10/16 18:30:44 INFO SessionState: Added [/local_disk0/tmp/addedFile8624879695200027925xml_apis_xml_apis_1_0_b2-167e6.jar] to class path
19/10/16 18:30:44 INFO SessionState: Added resources: [/local_disk0/tmp/addedFile8624879695200027925xml_apis_xml_apis_1_0_b2-167e6.jar]
19/10/16 18:30:44 WARN SparkContext: The jar /local_disk0/tmp/addedFile8624879695200027925xml_apis_xml_apis_1_0_b2-167e6.jar has been added already. Overwriting of added jars is not supported in the current version.
19/10/16 18:30:44 INFO SQLAppStatusListener: Execution ID: 8 Total Executor Run Time: 0
19/10/16 18:30:44 INFO SessionState: Added [/local_disk0/tmp/addedFile6674954176252367935org_scalaj_scalaj_http_2_11_2_3_0-d69e4.jar] to class path
19/10/16 18:30:44 INFO SessionState: Added resources: [/local_disk0/tmp/addedFile6674954176252367935org_scalaj_scalaj_http_2_11_2_3_0-d69e4.jar]
19/10/16 18:30:44 WARN SparkContext: The jar /local_disk0/tmp/addedFile6674954176252367935org_scalaj_scalaj_http_2_11_2_3_0-d69e4.jar has been added already. Overwriting of added jars is not supported in the current version.
19/10/16 18:30:44 INFO SQLAppStatusListener: Execution ID: 9 Total Executor Run Time: 0
19/10/16 18:30:44 INFO SessionState: Added [/local_disk0/tmp/addedFile5681071070702781128dbfs__FileStore_saak9001_transengine_1_0_SNAPSHOT-ab9b9.jar] to class path
19/10/16 18:30:44 INFO SessionState: Added resources: [/local_disk0/tmp/addedFile5681071070702781128dbfs__FileStore_saak9001_transengine_1_0_SNAPSHOT-ab9b9.jar]
19/10/16 18:30:44 WARN SparkContext: The jar /local_disk0/tmp/addedFile5681071070702781128dbfs__FileStore_saak9001_transengine_1_0_SNAPSHOT-ab9b9.jar has been added already. Overwriting of added jars is not supported in the current version.
19/10/16 18:30:44 INFO SQLAppStatusListener: Execution ID: 10 Total Executor Run Time: 0
19/10/16 18:30:44 INFO SessionState: Added [/local_disk0/tmp/addedFile6883373321297042917org_rogach_scallop_2_11_3_3_1-ecc25.jar] to class path
19/10/16 18:30:44 INFO SessionState: Added resources: [/local_disk0/tmp/addedFile6883373321297042917org_rogach_scallop_2_11_3_3_1-ecc25.jar]
19/10/16 18:30:44 WARN SparkContext: The jar /local_disk0/tmp/addedFile6883373321297042917org_rogach_scallop_2_11_3_3_1-ecc25.jar has been added already. Overwriting of added jars is not supported in the current version.
19/10/16 18:30:44 INFO SQLAppStatusListener: Execution ID: 11 Total Executor Run Time: 0
19/10/16 18:30:44 INFO SessionState: Added [/local_disk0/tmp/addedFile2365597673424539553dbfs__FileStore_saak9001_pqm_core_models_1_0-b0ccd.jar] to class path
19/10/16 18:30:44 INFO SessionState: Added resources: [/local_disk0/tmp/addedFile2365597673424539553dbfs__FileStore_saak9001_pqm_core_models_1_0-b0ccd.jar]
19/10/16 18:30:44 WARN SparkContext: The jar /local_disk0/tmp/addedFile2365597673424539553dbfs__FileStore_saak9001_pqm_core_models_1_0-b0ccd.jar has been added already. Overwriting of added jars is not supported in the current version.
19/10/16 18:30:44 INFO SQLAppStatusListener: Execution ID: 12 Total Executor Run Time: 0
19/10/16 18:30:44 INFO SessionState: Added [/local_disk0/tmp/addedFile5162712074542881962com_typesafe_config_1_3_1-e3e42.jar] to class path
19/10/16 18:30:44 INFO SessionState: Added resources: [/local_disk0/tmp/addedFile5162712074542881962com_typesafe_config_1_3_1-e3e42.jar]
19/10/16 18:30:44 WARN SparkContext: The jar /local_disk0/tmp/addedFile5162712074542881962com_typesafe_config_1_3_1-e3e42.jar has been added already. Overwriting of added jars is not supported in the current version.
19/10/16 18:30:44 INFO SQLAppStatusListener: Execution ID: 13 Total Executor Run Time: 0
19/10/16 18:30:44 INFO SessionState: Added [/local_disk0/tmp/addedFile5541486089544592835io_spray_spray_json_2_11_1_3_5-d5f78.jar] to class path
19/10/16 18:30:44 INFO SessionState: Added resources: [/local_disk0/tmp/addedFile5541486089544592835io_spray_spray_json_2_11_1_3_5-d5f78.jar]
19/10/16 18:30:44 WARN SparkContext: The jar /local_disk0/tmp/addedFile5541486089544592835io_spray_spray_json_2_11_1_3_5-d5f78.jar has been added already. Overwriting of added jars is not supported in the current version.
19/10/16 18:30:44 INFO SQLAppStatusListener: Execution ID: 14 Total Executor Run Time: 0
19/10/16 18:30:44 INFO SessionState: Added [/local_disk0/tmp/addedFile236936972251169783dom4j_dom4j_1_6_1-3b35e.jar] to class path
19/10/16 18:30:44 INFO SessionState: Added resources: [/local_disk0/tmp/addedFile236936972251169783dom4j_dom4j_1_6_1-3b35e.jar]
19/10/16 18:30:44 WARN SparkContext: The jar /local_disk0/tmp/addedFile236936972251169783dom4j_dom4j_1_6_1-3b35e.jar has been added already. Overwriting of added jars is not supported in the current version.
19/10/16 18:30:44 INFO ScalaDriverWrapper: setupRepl:ReplId-24102-44cb9-7d5d4-a: finished to load
19/10/16 18:30:44 INFO SQLAppStatusListener: Execution ID: 15 Total Executor Run Time: 0
19/10/16 18:30:44 WARN ScalaDriverLocal: User entity information is incomplete, missing userId,user.
19/10/16 18:30:44 INFO ProgressReporter$: Added result fetcher for 2598616896940891466_6784036610922160267_job-67852-run-1-action-72028
19/10/16 18:30:45 INFO DriverILoop: Set class prefix to: lined5ae43a8bacb48d1b9d367f68ba79872
19/10/16 18:30:45 INFO DriverILoop: set ContextClassLoader
19/10/16 18:30:45 INFO DriverILoop: initialized intp
19/10/16 18:30:48 WARN SparkContext: Using an existing SparkContext; some configuration may not take effect.
19/10/16 18:30:48 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
19/10/16 18:30:48 INFO Instructions: Instruction File has been Read Successfully.>>>>>>>>>>>
19/10/16 18:30:48 INFO HiveMetaStore: 1: get_database: rms_1000255_pqm
19/10/16 18:30:48 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: rms_1000255_pqm	
19/10/16 18:30:48 WARN HiveConf: HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist
19/10/16 18:30:48 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
19/10/16 18:30:48 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
19/10/16 18:30:48 INFO HiveMetaStore: 1: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
19/10/16 18:30:48 INFO ObjectStore: ObjectStore, initialize called
19/10/16 18:30:48 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is POSTGRES
19/10/16 18:30:48 INFO ObjectStore: Initialized ObjectStore
19/10/16 18:30:48 INFO HiveMetaStore: 1: get_database: global_temp
19/10/16 18:30:48 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: global_temp	
19/10/16 18:30:48 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException
19/10/16 18:30:48 INFO HiveMetaStore: 1: get_database: rms_1000255_pqm
19/10/16 18:30:48 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: rms_1000255_pqm	
19/10/16 18:30:48 INFO HiveMetaStore: 1: get_database: rms_1000255_pqm
19/10/16 18:30:48 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: rms_1000255_pqm	
19/10/16 18:30:48 INFO HiveMetaStore: 1: get_tables: db=rms_1000255_pqm pat=*
19/10/16 18:30:48 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_tables: db=rms_1000255_pqm pat=*	
19/10/16 18:30:48 INFO HiveMetaStore: 1: get_database: rms_1000255_pqm
19/10/16 18:30:48 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: rms_1000255_pqm	
19/10/16 18:30:48 INFO HiveMetaStore: 1: get_all_tables: db=rms_1000255_pqm
19/10/16 18:30:48 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_all_tables: db=rms_1000255_pqm	
19/10/16 18:30:48 INFO HiveMetaStore: 1: get_table : db=rms_1000255_pqm tbl=mrkt_dim
19/10/16 18:30:48 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=rms_1000255_pqm tbl=mrkt_dim	
19/10/16 18:30:49 INFO HiveMetaStore: 1: drop_table : db=rms_1000255_pqm tbl=mrkt_dim
19/10/16 18:30:49 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=drop_table : db=rms_1000255_pqm tbl=mrkt_dim	
19/10/16 18:30:49 DEBUG AccessTokenProvider: AADToken: no token. Returning expiring=true
19/10/16 18:30:49 DEBUG AccessTokenProvider: AAD Token is missing or expired: Calling refresh-token from abstract base class
19/10/16 18:30:49 DEBUG ClientCredsTokenProvider: AADToken: refreshing client-credential based token
19/10/16 18:30:49 DEBUG AzureADAuthenticator: AADToken: starting to fetch token using client creds for client ID 003106c3-ff8c-4c62-a060-24b8b32ea512
19/10/16 18:30:49 DEBUG AzureADAuthenticator: AADToken: fetched token with expiry Thu Oct 17 02:30:49 UTC 2019
19/10/16 18:30:49 INFO deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum
19/10/16 18:30:49 INFO TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
19/10/16 18:30:49 DEBUG HttpTransport: HTTPRequest,Failed,cReqId:8e23d22e-0122-49a4-98bb-29ec791cc1da.0,lat:6,err:HTTP404(FileNotFoundException),Reqlen:0,Resplen:268,token_ns:5700,sReqId:51783b30-88e7-4ff1-9810-7d4cdf442f5d,path:/user/hive/warehouse/RMS_1000255_PQM.db/mrkt_dim,qp:op=GETFILESTATUS&tooid=true&api-version=2017-08-01
19/10/16 18:30:49 INFO HiveMetaStore: 1: get_table : db=rms_1000255_pqm tbl=prd_dim
19/10/16 18:30:49 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=rms_1000255_pqm tbl=prd_dim	
19/10/16 18:30:49 INFO HiveMetaStore: 1: drop_table : db=rms_1000255_pqm tbl=prd_dim
19/10/16 18:30:49 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=drop_table : db=rms_1000255_pqm tbl=prd_dim	
19/10/16 18:30:50 INFO TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
19/10/16 18:30:50 DEBUG HttpTransport: HTTPRequest,Failed,cReqId:9a88593d-d238-42e3-87fc-aa59151b5a2f.0,lat:6,err:HTTP404(FileNotFoundException),Reqlen:0,Resplen:267,token_ns:3500,sReqId:c7e777b7-c363-45bc-8640-201a68985cc3,path:/user/hive/warehouse/RMS_1000255_PQM.db/prd_dim,qp:op=GETFILESTATUS&tooid=true&api-version=2017-08-01
19/10/16 18:30:50 INFO HiveMetaStore: 1: drop_database: rms_1000255_pqm
19/10/16 18:30:50 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=drop_database: rms_1000255_pqm	
19/10/16 18:30:50 INFO HiveMetaStore: 1: get_all_tables: db=rms_1000255_pqm
19/10/16 18:30:50 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_all_tables: db=rms_1000255_pqm	
19/10/16 18:30:50 INFO HiveMetaStore: 1: get_functions: db=rms_1000255_pqm pat=*
19/10/16 18:30:50 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=rms_1000255_pqm pat=*	
19/10/16 18:30:50 INFO ObjectStore: Dropping database rms_1000255_pqm along with all tables
19/10/16 18:30:50 INFO TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
19/10/16 18:30:50 DEBUG HttpTransport: HTTPRequest,Failed,cReqId:f0e4fa22-609b-4d71-835b-b09cb5864f7d.0,lat:6,err:HTTP404(FileNotFoundException),Reqlen:0,Resplen:259,token_ns:3500,sReqId:b0d8fad3-1e31-4787-bbaf-f7c391bf6f9d,path:/user/hive/warehouse/RMS_1000255_PQM.db,qp:op=GETFILESTATUS&tooid=true&api-version=2017-08-01
19/10/16 18:30:50 INFO TxnHandler: START========"HiveConf()"========
hiveDefaultUrl=null
hiveSiteURL=null
hiveServer2SiteUrl=null
hivemetastoreSiteUrl=null
Values omitted for security reason if present: [fs.s3n.awsAccessKeyId, fs.s3a.access.key, fs.s3.awsAccessKeyId, hive.server2.keystore.password, fs.s3a.proxy.password, javax.jdo.option.ConnectionPassword, fs.s3.awsSecretAccessKey, fs.s3n.awsSecretAccessKey, fs.s3a.secret.key]
_hive.hdfs.session.path=
    /tmp/hive/root/74f4940e-a6bf-4ecf-b225-d4debe0f74b4:
_hive.local.session.path=
    /local_disk0/tmp/root/74f4940e-a6bf-4ecf-b225-d4debe0f74b4:
_hive.tmp_table_space=/tmp/hive/root/74f4940e-a6bf-4ecf-b225-d4debe0f74b4/_tmp_space.db
databricks.dbfs.client.version=v2
databricks.s3commit.client.sslTrustAll=false
datanucleus.autoCreateSchema=true
datanucleus.autoStartMechanismMode=ignored
datanucleus.cache.level2=false
datanucleus.cache.level2.type=none
datanucleus.connectionPool.initialPoolSize=0
datanucleus.connectionPool.maxIdle=1
datanucleus.connectionPool.maxPoolSize=30
datanucleus.connectionPool.minPoolSize=15
datanucleus.connectionPoolingType=BONECP
datanucleus.fixedDatastore=false
datanucleus.identifierFactory=datanucleus1
datanucleus.plugin.pluginRegistryBundleCheck=LOG
datanucleus.rdbms.initializeColumnInfo=NONE
datanucleus.rdbms.useLegacyNativeValueStrategy=true
datanucleus.schema.autoCreateAll=false
datanucleus.schema.validateColumns=false
datanucleus.schema.validateConstraints=false
datanucleus.schema.validateTables=false
datanucleus.storeManagerType=rdbms
datanucleus.transactionIsolation=read-committed
dfs.adls.oauth2.access.token.provider.type=ClientCredential
dfs.adls.oauth2.client.id=003106c3-ff8c-4c62-a060-24b8b32ea512
dfs.adls.oauth2.credential=viUHaEfNg8cHQRJSRNMzpIv1T6goiUg+c9R/XSbYBFA=
dfs.adls.oauth2.refresh.url=https://login.microsoftonline.com/6ac7a1f4-5fb1-4153-bb4f-12d2020a1f7d/oauth2/token
dfs.block.access.key.update.interval=600
dfs.block.access.token.enable=false
dfs.block.access.token.lifetime=600
dfs.block.scanner.volume.bytes.per.second=1048576
dfs.blockreport.initialDelay=0
dfs.blockreport.intervalMsec=21600000
dfs.blockreport.split.threshold=1000000
dfs.blocksize=134217728
dfs.bytes-per-checksum=512
dfs.cachereport.intervalMsec=10000
dfs.client-write-packet-size=65536
dfs.client.block.write.replace-datanode-on-failure.best-effort=false
dfs.client.block.write.replace-datanode-on-failure.enable=true
dfs.client.block.write.replace-datanode-on-failure.policy=DEFAULT
dfs.client.block.write.retries=3
dfs.client.cached.conn.retry=3
dfs.client.context=default
dfs.client.datanode-restart.timeout=30
dfs.client.domain.socket.data.traffic=false
dfs.client.failover.connection.retries=0
dfs.client.failover.connection.retries.on.timeouts=0
dfs.client.failover.max.attempts=15
dfs.client.failover.sleep.base.millis=500
dfs.client.failover.sleep.max.millis=15000
dfs.client.file-block-storage-locations.num-threads=10
dfs.client.file-block-storage-locations.timeout.millis=1000
dfs.client.https.keystore.resource=ssl-client.xml
dfs.client.https.need-auth=false
dfs.client.mmap.cache.size=256
dfs.client.mmap.cache.timeout.ms=3600000
dfs.client.mmap.enabled=true
dfs.client.mmap.retry.timeout.ms=300000
dfs.client.read.shortcircuit=false
dfs.client.read.shortcircuit.skip.checksum=false
dfs.client.read.shortcircuit.streams.cache.expiry.ms=300000
dfs.client.read.shortcircuit.streams.cache.size=256
dfs.client.short.circuit.replica.stale.threshold.ms=1800000
dfs.client.slow.io.warning.threshold.ms=30000
dfs.client.use.datanode.hostname=false
dfs.client.use.legacy.blockreader.local=false
dfs.client.write.exclude.nodes.cache.expiry.interval.millis=600000
dfs.datanode.address=0.0.0.0:50010
dfs.datanode.available-space-volume-choosing-policy.balanced-space-preference-fraction=0.75f
dfs.datanode.available-space-volume-choosing-policy.balanced-space-threshold=10737418240
dfs.datanode.balance.bandwidthPerSec=1048576
dfs.datanode.block-pinning.enabled=false
dfs.datanode.block.id.layout.upgrade.threads=12
dfs.datanode.bp-ready.timeout=20
dfs.datanode.cache.revocation.polling.ms=500
dfs.datanode.cache.revocation.timeout.ms=900000
dfs.datanode.data.dir=file:///local_disk0/tmp/dfs/data
dfs.datanode.data.dir.perm=700
dfs.datanode.directoryscan.interval=21600
dfs.datanode.directoryscan.threads=1
dfs.datanode.dns.interface=default
dfs.datanode.dns.nameserver=default
dfs.datanode.drop.cache.behind.reads=false
dfs.datanode.drop.cache.behind.writes=false
dfs.datanode.du.reserved=0
dfs.datanode.failed.volumes.tolerated=0
dfs.datanode.fsdatasetcache.max.threads.per.volume=4
dfs.datanode.handler.count=10
dfs.datanode.hdfs-blocks-metadata.enabled=false
dfs.datanode.http.address=0.0.0.0:50075
dfs.datanode.https.address=0.0.0.0:50475
dfs.datanode.ipc.address=0.0.0.0:50020
dfs.datanode.max.locked.memory=0
dfs.datanode.max.transfer.threads=4096
dfs.datanode.readahead.bytes=4194304
dfs.datanode.scan.period.hours=504
dfs.datanode.shared.file.descriptor.paths=
    /dev/shm,/tmp:
dfs.datanode.slow.io.warning.threshold.ms=300
dfs.datanode.sync.behind.writes=false
dfs.datanode.use.datanode.hostname=false
dfs.default.chunk.view.size=32768
dfs.encrypt.data.transfer=false
dfs.encrypt.data.transfer.cipher.key.bitlength=128
dfs.ha.automatic-failover.enabled=false
dfs.ha.fencing.ssh.connect-timeout=30000
dfs.ha.log-roll.period=120
dfs.ha.tail-edits.period=60
dfs.heartbeat.interval=3
dfs.http.policy=HTTP_ONLY
dfs.https.server.keystore.resource=ssl-server.xml
dfs.image.compress=false
dfs.image.compression.codec=org.apache.hadoop.io.compress.DefaultCodec
dfs.image.transfer.bandwidthPerSec=0
dfs.image.transfer.chunksize=65536
dfs.image.transfer.timeout=60000
dfs.journalnode.http-address=0.0.0.0:8480
dfs.journalnode.https-address=0.0.0.0:8481
dfs.journalnode.rpc-address=0.0.0.0:8485
dfs.namenode.accesstime.precision=3600000
dfs.namenode.acls.enabled=false
dfs.namenode.audit.loggers=default
dfs.namenode.avoid.read.stale.datanode=false
dfs.namenode.avoid.write.stale.datanode=false
dfs.namenode.backup.address=0.0.0.0:50100
dfs.namenode.backup.http-address=0.0.0.0:50105
dfs.namenode.block-placement-policy.default.prefer-local-node=true
dfs.namenode.blocks.per.postponedblocks.rescan=10000
dfs.namenode.checkpoint.check.period=60
dfs.namenode.checkpoint.dir=file:///local_disk0/tmp/dfs/namesecondary
dfs.namenode.checkpoint.edits.dir=file:///local_disk0/tmp/dfs/namesecondary
dfs.namenode.checkpoint.max-retries=3
dfs.namenode.checkpoint.period=3600
dfs.namenode.checkpoint.txns=1000000
dfs.namenode.datanode.registration.ip-hostname-check=true
dfs.namenode.decommission.blocks.per.interval=500000
dfs.namenode.decommission.interval=30
dfs.namenode.decommission.max.concurrent.tracked.nodes=100
dfs.namenode.delegation.key.update-interval=86400000
dfs.namenode.delegation.token.max-lifetime=604800000
dfs.namenode.delegation.token.renew-interval=86400000
dfs.namenode.edit.log.autoroll.check.interval.ms=300000
dfs.namenode.edit.log.autoroll.multiplier.threshold=2.0
dfs.namenode.edits.dir=file:///local_disk0/tmp/dfs/name
dfs.namenode.edits.journal-plugin.qjournal=org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager
dfs.namenode.edits.noeditlogchannelflush=false
dfs.namenode.enable.retrycache=true
dfs.namenode.fs-limits.max-blocks-per-file=1048576
dfs.namenode.fs-limits.max-component-length=255
dfs.namenode.fs-limits.max-directory-items=1048576
dfs.namenode.fs-limits.max-xattr-size=16384
dfs.namenode.fs-limits.max-xattrs-per-inode=32
dfs.namenode.fs-limits.min-block-size=1048576
dfs.namenode.handler.count=10
dfs.namenode.heartbeat.recheck-interval=300000
dfs.namenode.http-address=0.0.0.0:50070
dfs.namenode.https-address=0.0.0.0:50470
dfs.namenode.inotify.max.events.per.rpc=1000
dfs.namenode.invalidate.work.pct.per.iteration=0.32f
dfs.namenode.kerberos.internal.spnego.principal=${dfs.web.authentication.kerberos.principal}
dfs.namenode.kerberos.principal.pattern=*
dfs.namenode.lazypersist.file.scrub.interval.sec=300
dfs.namenode.list.cache.directives.num.responses=100
dfs.namenode.list.cache.pools.num.responses=100
dfs.namenode.list.encryption.zones.num.responses=100
dfs.namenode.max.extra.edits.segments.retained=10000
dfs.namenode.max.objects=0
dfs.namenode.name.dir=file:///local_disk0/tmp/dfs/name
dfs.namenode.name.dir.restore=false
dfs.namenode.num.checkpoints.retained=2
dfs.namenode.num.extra.edits.retained=1000000
dfs.namenode.path.based.cache.block.map.allocation.percent=
    0.25:
dfs.namenode.path.based.cache.refresh.interval.ms=
    30000:
dfs.namenode.path.based.cache.retry.interval.ms=
    30000:
dfs.namenode.reject-unresolved-dn-topology-mapping=false
dfs.namenode.replication.considerLoad=true
dfs.namenode.replication.interval=3
dfs.namenode.replication.min=1
dfs.namenode.replication.work.multiplier.per.iteration=2
dfs.namenode.resource.check.interval=5000
dfs.namenode.resource.checked.volumes.minimum=1
dfs.namenode.resource.du.reserved=104857600
dfs.namenode.retrycache.expirytime.millis=600000
dfs.namenode.retrycache.heap.percent=0.03f
dfs.namenode.safemode.extension=30000
dfs.namenode.safemode.min.datanodes=0
dfs.namenode.safemode.threshold-pct=0.999f
dfs.namenode.secondary.http-address=0.0.0.0:50090
dfs.namenode.secondary.https-address=0.0.0.0:50091
dfs.namenode.stale.datanode.interval=30000
dfs.namenode.startup.delay.block.deletion.sec=0
dfs.namenode.support.allow.format=true
dfs.namenode.top.enabled=true
dfs.namenode.top.num.users=10
dfs.namenode.top.window.num.buckets=10
dfs.namenode.top.windows.minutes=1,5,25
dfs.namenode.write.stale.datanode.ratio=0.5f
dfs.namenode.xattrs.enabled=true
dfs.permissions.enabled=true
dfs.permissions.superusergroup=supergroup
dfs.replication=3
dfs.replication.max=512
dfs.secondary.namenode.kerberos.internal.spnego.principal=${dfs.web.authentication.kerberos.principal}
dfs.short.circuit.shared.memory.watcher.interrupt.check.ms=60000
dfs.storage.policy.enabled=true
dfs.stream-buffer-size=4096
dfs.user.home.dir.prefix=/user
dfs.webhdfs.enabled=true
dfs.webhdfs.user.provider.user.pattern=^[A-Za-z_][A-Za-z0-9._-]*[$]?$
eventLog.rolloverIntervalSeconds=3600
file.blocksize=67108864
file.bytes-per-checksum=512
file.client-write-packet-size=65536
file.replication=1
file.stream-buffer-size=4096
fs.AbstractFileSystem.file.impl=org.apache.hadoop.fs.local.LocalFs
fs.AbstractFileSystem.ftp.impl=org.apache.hadoop.fs.ftp.FtpFs
fs.AbstractFileSystem.har.impl=org.apache.hadoop.fs.HarFs
fs.AbstractFileSystem.hdfs.impl=org.apache.hadoop.fs.Hdfs
fs.AbstractFileSystem.viewfs.impl=org.apache.hadoop.fs.viewfs.ViewFs
fs.abfs.impl=shaded.databricks.v20180920_b33d810.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem
fs.abfs.impl.disable.cache=true
fs.abfss.impl=shaded.databricks.v20180920_b33d810.org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem
fs.abfss.impl.disable.cache=true
fs.adl.impl=com.databricks.adl.AdlFileSystem
fs.adl.impl.disable.cache=false
fs.automatic.close=true
fs.azure.account.key.teuatsa.blob.core.windows.net=IsrxMnnzLeO9N44beU4eMw0LaiKJdi1hHmKZFUiPa8nFNKZAFyhH0Hv/UCoC4qhgcdq63zwTRGFhUKV9d4ZsBw==
fs.azure.skip.metrics=true
fs.client.resolve.remote.symlinks=true
fs.dbfs.impl=com.databricks.backend.daemon.data.client.DBFS
fs.dbfs2.impl=com.databricks.backend.daemon.data.client.DBFSV2
fs.dbfsartifacts.impl=com.databricks.backend.daemon.data.client.DBFSV1
fs.dbs3n.impl=com.databricks.backend.daemon.data.client.DBS3N
fs.dbtachyon.impl=com.databricks.backend.daemon.data.client.DBTACHYON
fs.defaultFS=dbfs:///
fs.df.interval=60000
fs.du.interval=600000
fs.ftp.host=0.0.0.0
fs.ftp.host.port=21
fs.har.impl=org.apache.hadoop.hive.shims.HiveHarFileSystem
fs.har.impl.disable.cache=true
fs.permissions.umask-mode=022
fs.s3.block.size=67108864
fs.s3.buffer.dir=/local_disk0/tmp/s3
fs.s3.impl=com.databricks.s3a.S3AFileSystem
fs.s3.maxRetries=4
fs.s3.sleepTimeSeconds=10
fs.s3a.attempts.maximum=10
fs.s3a.buffer.dir=/local_disk0/tmp/s3a
fs.s3a.connection.establish.timeout=5000
fs.s3a.connection.maximum=200
fs.s3a.connection.ssl.enabled=true
fs.s3a.connection.timeout=50000
fs.s3a.fast.buffer.size=1048576
fs.s3a.fast.upload=true
fs.s3a.fast.upload.default=true
fs.s3a.impl=com.databricks.s3a.S3AFileSystem
fs.s3a.max.total.tasks=1000
fs.s3a.multipart.purge=false
fs.s3a.multipart.purge.age=86400
fs.s3a.multipart.size=10485760
fs.s3a.multipart.threshold=104857600
fs.s3a.paging.maximum=5000
fs.s3a.threads.core=15
fs.s3a.threads.keepalivetime=60
fs.s3a.threads.max=136
fs.s3araw.impl=com.databricks.s3a.S3AFileSystem
fs.s3n.block.size=67108864
fs.s3n.impl=com.databricks.s3a.S3AFileSystem
fs.s3n.multipart.copy.block.size=5368709120
fs.s3n.multipart.uploads.block.size=67108864
fs.s3n.multipart.uploads.enabled=false
fs.s3raw.impl=com.databricks.s3.StableNativeS3FileSystem
fs.scheme.class=dfs
fs.swift.impl=org.apache.hadoop.fs.swift.snative.SwiftNativeFileSystem
fs.tachyon.impl=tachyon.hadoop.TFS
fs.trash.checkpoint.interval=0
fs.trash.interval=0
fs.wasb.impl=shaded.databricks.org.apache.hadoop.fs.azure.NativeAzureFileSystem
fs.wasb.impl.disable.cache=true
fs.wasbs.impl=shaded.databricks.org.apache.hadoop.fs.azure.NativeAzureFileSystem
fs.wasbs.impl.disable.cache=true
ftp.blocksize=67108864
ftp.bytes-per-checksum=512
ftp.client-write-packet-size=65536
ftp.replication=3
ftp.stream-buffer-size=4096
ha.failover-controller.cli-check.rpc-timeout.ms=20000
ha.failover-controller.graceful-fence.connection.retries=1
ha.failover-controller.graceful-fence.rpc-timeout.ms=5000
ha.failover-controller.new-active.rpc-timeout.ms=60000
ha.health-monitor.check-interval.ms=1000
ha.health-monitor.connect-retry-interval.ms=1000
ha.health-monitor.rpc-timeout.ms=45000
ha.health-monitor.sleep-after-disconnect.ms=1000
ha.zookeeper.acl=world:anyone:rwcda
ha.zookeeper.parent-znode=/hadoop-ha
ha.zookeeper.session-timeout.ms=5000
hadoop.bin.path=
    /usr/bin/hadoop:
hadoop.common.configuration.version=0.23.0
hadoop.fuse.connection.timeout=300
hadoop.fuse.timer.period=5
hadoop.hdfs.configuration.version=1
hadoop.http.authentication.kerberos.keytab=/root/hadoop.keytab
hadoop.http.authentication.kerberos.principal=HTTP/_HOST@LOCALHOST
hadoop.http.authentication.signature.secret.file=/root/hadoop-http-auth-signature-secret
hadoop.http.authentication.simple.anonymous.allowed=true
hadoop.http.authentication.token.validity=36000
hadoop.http.authentication.type=simple
hadoop.http.cross-origin.allowed-headers=X-Requested-With,Content-Type,Accept,Origin
hadoop.http.cross-origin.allowed-methods=GET,POST,HEAD
hadoop.http.cross-origin.allowed-origins=*
hadoop.http.cross-origin.enabled=false
hadoop.http.cross-origin.max-age=1800
hadoop.http.filter.initializers=org.apache.hadoop.http.lib.StaticUserWebFilter
hadoop.http.staticuser.user=dr.who
hadoop.jetty.logs.serve.aliases=true
hadoop.kerberos.kinit.command=kinit
hadoop.registry.jaas.context=Client
hadoop.registry.rm.enabled=false
hadoop.registry.secure=false
hadoop.registry.system.acls=sasl:yarn@, sasl:mapred@, sasl:hdfs@
hadoop.registry.zk.connection.timeout.ms=15000
hadoop.registry.zk.quorum=localhost:2181
hadoop.registry.zk.retry.ceiling.ms=60000
hadoop.registry.zk.retry.interval.ms=1000
hadoop.registry.zk.retry.times=5
hadoop.registry.zk.root=/registry
hadoop.registry.zk.session.timeout.ms=60000
hadoop.rpc.protection=authentication
hadoop.rpc.socket.factory.class.default=org.apache.hadoop.net.StandardSocketFactory
hadoop.security.authentication=simple
hadoop.security.authorization=false
hadoop.security.crypto.buffer.size=8192
hadoop.security.crypto.cipher.suite=AES/CTR/NoPadding
hadoop.security.crypto.codec.classes.aes.ctr.nopadding=org.apache.hadoop.crypto.OpensslAesCtrCryptoCodec,org.apache.hadoop.crypto.JceAesCtrCryptoCodec
hadoop.security.group.mapping=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback
hadoop.security.group.mapping.ldap.directory.search.timeout=10000
hadoop.security.group.mapping.ldap.search.attr.group.name=cn
hadoop.security.group.mapping.ldap.search.attr.member=member
hadoop.security.group.mapping.ldap.search.filter.group=(objectClass=group)
hadoop.security.group.mapping.ldap.search.filter.user=(&(objectClass=user)(sAMAccountName={0}))
hadoop.security.group.mapping.ldap.ssl=false
hadoop.security.groups.cache.secs=300
hadoop.security.groups.cache.warn.after.ms=5000
hadoop.security.groups.negative-cache.secs=30
hadoop.security.instrumentation.requires.admin=false
hadoop.security.java.secure.random.algorithm=SHA1PRNG
hadoop.security.kms.client.authentication.retry-count=1
hadoop.security.kms.client.encrypted.key.cache.expiry=43200000
hadoop.security.kms.client.encrypted.key.cache.low-watermark=0.3f
hadoop.security.kms.client.encrypted.key.cache.num.refill.threads=2
hadoop.security.kms.client.encrypted.key.cache.size=500
hadoop.security.random.device.file.path=
    /dev/urandom:
hadoop.security.uid.cache.secs=14400
hadoop.ssl.client.conf=ssl-client.xml
hadoop.ssl.enabled=false
hadoop.ssl.enabled.protocols=TLSv1
hadoop.ssl.hostname.verifier=DEFAULT
hadoop.ssl.keystores.factory.class=org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory
hadoop.ssl.require.client.cert=false
hadoop.ssl.server.conf=ssl-server.xml
hadoop.tmp.dir=/local_disk0/tmp
hadoop.user.group.static.mapping.overrides=dr.who=;
hadoop.util.hash.type=murmur
hadoop.work.around.non.threadsafe.getpwuid=false
hive.allow.udf.load.on.demand=false
hive.analyze.stmt.collect.partlevel.stats=true
hive.archive.enabled=false
hive.async.log.enabled=true
hive.ats.hook.queue.capacity=64
hive.auto.convert.join=true
hive.auto.convert.join.hashtable.max.entries=40000000
hive.auto.convert.join.noconditionaltask=true
hive.auto.convert.join.noconditionaltask.size=10000000
hive.auto.convert.join.use.nonstaged=false
hive.auto.convert.sortmerge.join=false
hive.auto.convert.sortmerge.join.bigtable.selection.policy=org.apache.hadoop.hive.ql.optimizer.AvgPartitionSizeBasedBigTableSelectorForAutoSMJ
hive.auto.convert.sortmerge.join.reduce.side=true
hive.auto.convert.sortmerge.join.to.mapjoin=false
hive.auto.progress.timeout=0
hive.autogen.columnalias.prefix.includefuncname=false
hive.autogen.columnalias.prefix.label=_c
hive.binary.record.max.length=1000
hive.blobstore.optimizations.enabled=true
hive.blobstore.supported.schemes=s3,s3a,s3n
hive.blobstore.use.blobstore.as.scratchdir=false
hive.cache.expr.evaluation=true
hive.cbo.cnf.maxnodes=-1
hive.cbo.costmodel.cpu=0.000001
hive.cbo.costmodel.extended=false
hive.cbo.costmodel.hdfs.read=1.5
hive.cbo.costmodel.hdfs.write=10.0
hive.cbo.costmodel.local.fs.read=4.0
hive.cbo.costmodel.local.fs.write=4.0
hive.cbo.costmodel.network=150.0
hive.cbo.enable=true
hive.cbo.returnpath.hiveop=
    false:
hive.cbo.show.warnings=true
hive.cli.errors.ignore=false
hive.cli.pretty.output.num.cols=-1
hive.cli.print.current.db=false
hive.cli.print.header=false
hive.cli.prompt=hive
hive.cli.tez.session.async=true
hive.cluster.delegation.token.store.class=org.apache.hadoop.hive.thrift.MemoryTokenStore
hive.cluster.delegation.token.store.zookeeper.znode=/hivedelegation
hive.compactor.abortedtxn.threshold=1000
hive.compactor.check.interval=300
hive.compactor.cleaner.run.interval=5000
hive.compactor.delta.num.threshold=10
hive.compactor.delta.pct.threshold=0.1
hive.compactor.history.reaper.interval=2m
hive.compactor.history.retention.attempted=2
hive.compactor.history.retention.failed=3
hive.compactor.history.retention.succeeded=3
hive.compactor.initiator.failed.compacts.threshold=2
hive.compactor.initiator.on=false
hive.compactor.max.num.delta=500
hive.compactor.worker.threads=0
hive.compactor.worker.timeout=86400
hive.compat=0.12
hive.compute.query.using.stats=true
hive.compute.splits.in.am=true
hive.conf.hidden.list=javax.jdo.option.ConnectionPassword,hive.server2.keystore.password,fs.s3.awsAccessKeyId,fs.s3.awsSecretAccessKey,fs.s3n.awsAccessKeyId,fs.s3n.awsSecretAccessKey,fs.s3a.access.key,fs.s3a.secret.key,fs.s3a.proxy.password
hive.conf.internal.variable.list=hive.added.files.path,hive.added.jars.path,hive.added.archives.path
hive.conf.restricted.list=hive.security.authenticator.manager,hive.security.authorization.manager,hive.security.metastore.authorization.manager,hive.security.metastore.authenticator.manager,hive.users.in.admin.role,hive.server2.xsrf.filter.enabled,hive.security.authorization.enabled,hive.server2.authentication.ldap.baseDN,hive.server2.authentication.ldap.url,hive.server2.authentication.ldap.Domain,hive.server2.authentication.ldap.groupDNPattern,hive.server2.authentication.ldap.groupFilter,hive.server2.authentication.ldap.userDNPattern,hive.server2.authentication.ldap.userFilter,hive.server2.authentication.ldap.groupMembershipKey,hive.server2.authentication.ldap.userMembershipKey,hive.server2.authentication.ldap.groupClassKey,hive.server2.authentication.ldap.customLDAPQuery
hive.conf.validation=true
hive.convert.join.bucket.mapjoin.tez=false
hive.count.open.txns.interval=1s
hive.counters.group.name=HIVE
hive.debug.localtask=false
hive.decode.partition.name=false
hive.default.fileformat=TextFile
hive.default.fileformat.managed=none
hive.default.rcfile.serde=org.apache.hadoop.hive.serde2.columnar.LazyBinaryColumnarSerDe
hive.default.serde=org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
hive.direct.sql.max.elements.in.clause=1000
hive.direct.sql.max.elements.values.clause=1000
hive.direct.sql.max.query.length=100
hive.display.partition.cols.separately=true
hive.downloaded.resources.dir=/local_disk0/tmp/74f4940e-a6bf-4ecf-b225-d4debe0f74b4_resources
hive.driver.parallel.compilation=false
hive.druid.broker.address.default=localhost:8082
hive.druid.coordinator.address.default=localhost:8081
hive.druid.http.numConnection=20
hive.druid.http.read.timeout=PT1M
hive.druid.indexer.memory.rownum.max=75000
hive.druid.indexer.partition.size.max=5000000
hive.druid.indexer.segments.granularity=DAY
hive.druid.maxTries=5
hive.druid.metadata.base=druid
hive.druid.metadata.db.type=mysql
hive.druid.passiveWaitTimeMs=30000
hive.druid.select.distribute=true
hive.druid.select.threshold=10000
hive.druid.sleep.time=PT10S
hive.druid.storage.storageDirectory=/druid/segments
hive.druid.working.directory=/tmp/workingDirectory
hive.enforce.bucketmapjoin=false
hive.enforce.sortmergebucketmapjoin=false
hive.entity.capture.transform=false
hive.entity.separator=@
hive.error.on.empty.partition=false
hive.exec.check.crossproducts=true
hive.exec.compress.intermediate=false
hive.exec.compress.output=false
hive.exec.concatenate.check.index=true
hive.exec.copyfile.maxnumfiles=1
hive.exec.copyfile.maxsize=33554432
hive.exec.counters.pull.interval=1000
hive.exec.default.partition.name=__HIVE_DEFAULT_PARTITION__
hive.exec.drop.ignorenonexistent=true
hive.exec.dynamic.partition=true
hive.exec.dynamic.partition.mode=strict
hive.exec.infer.bucket.sort=false
hive.exec.infer.bucket.sort.num.buckets.power.two=false
hive.exec.input.listing.max.threads=0
hive.exec.job.debug.capture.stacktraces=true
hive.exec.job.debug.timeout=30000
hive.exec.local.scratchdir=/local_disk0/tmp/root
hive.exec.max.created.files=100000
hive.exec.max.dynamic.partitions=1000
hive.exec.max.dynamic.partitions.pernode=100
hive.exec.mode.local.auto=false
hive.exec.mode.local.auto.input.files.max=4
hive.exec.mode.local.auto.inputbytes.max=134217728
hive.exec.orc.base.delta.ratio=8
hive.exec.orc.split.strategy=HYBRID
hive.exec.parallel=false
hive.exec.parallel.thread.number=8
hive.exec.perf.logger=org.apache.hadoop.hive.ql.log.PerfLogger
hive.exec.rcfile.use.explicit.header=true
hive.exec.rcfile.use.sync.cache=true
hive.exec.reducers.bytes.per.reducer=256000000
hive.exec.reducers.max=1009
hive.exec.rowoffset=false
hive.exec.schema.evolution=true
hive.exec.scratchdir=/tmp/hive
hive.exec.script.allow.partial.consumption=false
hive.exec.script.maxerrsize=100000
hive.exec.script.trust=false
hive.exec.show.job.failure.debug.info=true
hive.exec.stagingdir=.hive-staging
hive.exec.submit.local.task.via.child=true
hive.exec.submitviachild=false
hive.exec.tasklog.debug.timeout=20000
hive.exec.temporary.table.storage=default
hive.execution.engine=mr
hive.execution.mode=container
hive.exim.strict.repl.tables=true
hive.exim.uri.scheme.whitelist=hdfs,pfile,file,s3,s3a
hive.explain.dependency.append.tasktype=false
hive.explain.user=true
hive.fetch.output.serde=org.apache.hadoop.hive.serde2.DelimitedJSONSerDe
hive.fetch.task.aggr=false
hive.fetch.task.conversion=more
hive.fetch.task.conversion.threshold=1073741824
hive.file.max.footer=100
hive.fileformat.check=true
hive.groupby.limit.extrastep=true
hive.groupby.mapaggr.checkinterval=100000
hive.groupby.orderby.position.alias=false
hive.groupby.position.alias=false
hive.groupby.skewindata=false
hive.hash.table.inflation.factor=2.0
hive.hashtable.initialCapacity=100000
hive.hashtable.key.count.adjustment=1.0
hive.hashtable.loadfactor=0.75
hive.hbase.generatehfiles=false
hive.hbase.snapshot.restoredir=/tmp
hive.hbase.wal.enabled=true
hive.heartbeat.interval=1000
hive.hmshandler.force.reload.conf=false
hive.hmshandler.retry.attempts=10
hive.hmshandler.retry.interval=2000
hive.ignore.mapjoin.hint=true
hive.in.test=false
hive.in.tez.test=false
hive.index.compact.binary.search=true
hive.index.compact.file.ignore.hdfs=false
hive.index.compact.query.max.entries=10000000
hive.index.compact.query.max.size=10737418240
hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat
hive.insert.into.external.tables=true
hive.insert.into.multilevel.dirs=false
hive.int.timestamp.conversion.in.seconds=false
hive.internal.ss.authz.settings.applied.marker=true
hive.io.rcfile.column.number.conf=0
hive.io.rcfile.record.buffer.size=4194304
hive.io.rcfile.record.interval=2147483647
hive.io.rcfile.tolerate.corruptions=false
hive.io.sarg.cache.max.weight.mb=10
hive.jobname.length=50
hive.join.cache.size=25000
hive.join.emit.interval=1000
hive.lazysimple.extended_boolean_literal=false
hive.limit.optimize.enable=false
hive.limit.optimize.fetch.max=50000
hive.limit.optimize.limit.file=10
hive.limit.pushdown.memory.usage=0.1
hive.limit.query.max.table.partition=-1
hive.limit.row.max.size=100000
hive.llap.allow.permanent.fns=true
hive.llap.am.liveness.connection.sleep.between.retries.ms=2000ms
hive.llap.am.liveness.connection.timeout.ms=10000ms
hive.llap.am.use.fqdn=false
hive.llap.auto.allow.uber=false
hive.llap.auto.auth=false
hive.llap.auto.enforce.stats=true
hive.llap.auto.enforce.tree=true
hive.llap.auto.enforce.vectorized=true
hive.llap.auto.max.input.size=10737418240
hive.llap.auto.max.output.size=1073741824
hive.llap.cache.allow.synthetic.fileid=false
hive.llap.client.consistent.splits=false
hive.llap.daemon.acl=*
hive.llap.daemon.am-reporter.max.threads=4
hive.llap.daemon.am.liveness.heartbeat.interval.ms=10000ms
hive.llap.daemon.communicator.num.threads=10
hive.llap.daemon.delegation.token.lifetime=14d
hive.llap.daemon.download.permanent.fns=false
hive.llap.daemon.logger=query-routing
hive.llap.daemon.memory.per.instance.mb=4096
hive.llap.daemon.num.executors=4
hive.llap.daemon.num.file.cleaner.threads=1
hive.llap.daemon.output.service.max.pending.writes=8
hive.llap.daemon.output.service.port=15003
hive.llap.daemon.output.service.send.buffer.size=131072
hive.llap.daemon.output.stream.timeout=120s
hive.llap.daemon.rpc.num.handlers=5
hive.llap.daemon.rpc.port=0
hive.llap.daemon.service.refresh.interval.sec=60s
hive.llap.daemon.shuffle.dir.watcher.enabled=false
hive.llap.daemon.task.preemption.metrics.intervals=30,60,300
hive.llap.daemon.task.scheduler.enable.preemption=true
hive.llap.daemon.task.scheduler.wait.queue.size=10
hive.llap.daemon.vcpus.per.instance=4
hive.llap.daemon.wait.queue.comparator.class.name=org.apache.hadoop.hive.llap.daemon.impl.comparator.ShortestJobFirstComparator
hive.llap.daemon.web.port=15002
hive.llap.daemon.web.ssl=false
hive.llap.daemon.xmx.headroom=5%
hive.llap.daemon.yarn.container.mb=-1
hive.llap.daemon.yarn.shuffle.port=15551
hive.llap.enable.grace.join.in.llap=false
hive.llap.execution.mode=none
hive.llap.file.cleanup.delay.seconds=300s
hive.llap.hs2.coordinator.enabled=true
hive.llap.io.allocator.alloc.max=16Mb
hive.llap.io.allocator.alloc.min=256Kb
hive.llap.io.allocator.arena.count=8
hive.llap.io.allocator.direct=true
hive.llap.io.allocator.mmap=false
hive.llap.io.allocator.mmap.path=
    /tmp:
hive.llap.io.decoding.metrics.percentiles.intervals=30
hive.llap.io.encode.alloc.size=256Kb
hive.llap.io.encode.enabled=true
hive.llap.io.encode.formats=org.apache.hadoop.mapred.TextInputFormat,
hive.llap.io.encode.slice.lrr=true
hive.llap.io.encode.slice.row.count=100000
hive.llap.io.encode.vector.serde.async.enabled=true
hive.llap.io.encode.vector.serde.enabled=true
hive.llap.io.lrfu.lambda=0.01
hive.llap.io.memory.mode=cache
hive.llap.io.memory.size=1Gb
hive.llap.io.metadata.fraction=0.1
hive.llap.io.nonvector.wrapper.enabled=true
hive.llap.io.orc.time.counters=true
hive.llap.io.threadpool.size=10
hive.llap.io.use.fileid.path=
    true:
hive.llap.io.use.lrfu=true
hive.llap.management.acl=*
hive.llap.management.rpc.port=15004
hive.llap.object.cache.enabled=true
hive.llap.orc.gap.cache=true
hive.llap.remote.token.requires.signing=true
hive.llap.skip.compile.udf.check=false
hive.llap.task.communicator.connection.sleep.between.retries.ms=2000ms
hive.llap.task.communicator.connection.timeout.ms=16000ms
hive.llap.task.communicator.listener.thread-count=30
hive.llap.task.scheduler.locality.delay=0ms
hive.llap.task.scheduler.node.disable.backoff.factor=1.5
hive.llap.task.scheduler.node.reenable.max.timeout.ms=10000ms
hive.llap.task.scheduler.node.reenable.min.timeout.ms=200ms
hive.llap.task.scheduler.num.schedulable.tasks.per.node=0
hive.llap.task.scheduler.timeout.seconds=60s
hive.llap.validate.acls=true
hive.load.dynamic.partitions.thread=15
hive.localize.resource.num.wait.attempts=5
hive.localize.resource.wait.interval=5000
hive.lock.manager=org.apache.hadoop.hive.ql.lockmgr.zookeeper.ZooKeeperHiveLockManager
hive.lock.mapred.only.operation=false
hive.lock.numretries=100
hive.lock.sleep.between.retries=60
hive.lockmgr.zookeeper.default.partition.name=__HIVE_DEFAULT_ZOOKEEPER_PARTITION__
hive.log.every.n.records=0
hive.log.explain.output=false
hive.map.aggr=true
hive.map.aggr.hash.force.flush.memory.threshold=0.9
hive.map.aggr.hash.min.reduction=0.5
hive.map.aggr.hash.percentmemory=0.5
hive.map.groupby.sorted=true
hive.mapjoin.bucket.cache.size=100
hive.mapjoin.check.memory.rows=100000
hive.mapjoin.followby.gby.localtask.max.memory.usage=0.55
hive.mapjoin.followby.map.aggr.hash.percentmemory=0.3
hive.mapjoin.hybridgrace.bloomfilter=true
hive.mapjoin.hybridgrace.hashtable=true
hive.mapjoin.hybridgrace.memcheckfrequency=1024
hive.mapjoin.hybridgrace.minnumpartitions=16
hive.mapjoin.hybridgrace.minwbsize=524288
hive.mapjoin.localtask.max.memory.usage=0.9
hive.mapjoin.optimized.hashtable=true
hive.mapjoin.optimized.hashtable.probe.percent=0.5
hive.mapjoin.optimized.hashtable.wbsize=8388608
hive.mapjoin.smalltable.filesize=25000000
hive.mapper.cannot.span.multiple.partitions=false
hive.mapred.local.mem=0
hive.mapred.partitioner=org.apache.hadoop.hive.ql.io.DefaultHivePartitioner
hive.mapred.reduce.tasks.speculative.execution=true
hive.materializedview.fileformat=ORC
hive.materializedview.rewriting=false
hive.materializedview.serde=org.apache.hadoop.hive.ql.io.orc.OrcSerde
hive.max.open.txns=100000
hive.merge.cardinality.check=true
hive.merge.mapfiles=true
hive.merge.mapredfiles=false
hive.merge.nway.joins=true
hive.merge.orcfile.stripe.level=true
hive.merge.rcfile.block.level=true
hive.merge.size.per.task=256000000
hive.merge.smallfiles.avgsize=16000000
hive.merge.sparkfiles=false
hive.merge.tezfiles=false
hive.metadata.move.exported.metadata.to.trash=true
hive.metastore.aggregate.stats.cache.clean.until=0.8
hive.metastore.aggregate.stats.cache.enabled=true
hive.metastore.aggregate.stats.cache.fpp=0.01
hive.metastore.aggregate.stats.cache.max.full=0.9
hive.metastore.aggregate.stats.cache.max.partitions=10000
hive.metastore.aggregate.stats.cache.max.reader.wait=1000
hive.metastore.aggregate.stats.cache.max.variance=0.01
hive.metastore.aggregate.stats.cache.max.writer.wait=5000
hive.metastore.aggregate.stats.cache.size=10000
hive.metastore.aggregate.stats.cache.ttl=600
hive.metastore.archive.intermediate.archived=_INTERMEDIATE_ARCHIVED
hive.metastore.archive.intermediate.extracted=_INTERMEDIATE_EXTRACTED
hive.metastore.archive.intermediate.original=_INTERMEDIATE_ORIGINAL
hive.metastore.authorization.storage.check.externaltable.drop=true
hive.metastore.authorization.storage.checks=false
hive.metastore.batch.retrieve.max=300
hive.metastore.batch.retrieve.table.partition.max=1000
hive.metastore.cache.pinobjtypes=Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order
hive.metastore.client.capability.check=true
hive.metastore.client.connect.retry.delay=10
hive.metastore.client.drop.partitions.using.expressions=true
hive.metastore.client.socket.lifetime=0
hive.metastore.client.socket.timeout=3600
hive.metastore.connect.retries=3
hive.metastore.direct.sql.batch.size=0
hive.metastore.disallow.incompatible.col.type.changes=true
hive.metastore.dml.events=false
hive.metastore.event.clean.freq=0
hive.metastore.event.db.listener.timetolive=86400
hive.metastore.event.expiry.duration=0
hive.metastore.event.message.factory=org.apache.hadoop.hive.metastore.messaging.json.JSONMessageFactory
hive.metastore.execute.setugi=true
hive.metastore.expression.proxy=org.apache.hadoop.hive.ql.optimizer.ppr.PartitionExpressionForMetastore
hive.metastore.failure.retries=30
hive.metastore.fastpath=
    false:
hive.metastore.filter.hook=org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
hive.metastore.fs.handler.class=org.apache.hadoop.hive.metastore.HiveMetaStoreFsImpl
hive.metastore.fshandler.threads=15
hive.metastore.hbase.aggr.stats.cache.entries=10000
hive.metastore.hbase.aggr.stats.hbase.ttl=604800s
hive.metastore.hbase.aggr.stats.invalidator.frequency=5s
hive.metastore.hbase.aggr.stats.memory.ttl=60s
hive.metastore.hbase.aggregate.stats.cache.size=10000
hive.metastore.hbase.aggregate.stats.false.positive.probability=0.01
hive.metastore.hbase.aggregate.stats.max.partitions=10000
hive.metastore.hbase.aggregate.stats.max.variance=0.1
hive.metastore.hbase.cache.clean.until=0.8
hive.metastore.hbase.cache.max.full=0.9
hive.metastore.hbase.cache.max.reader.wait=1000ms
hive.metastore.hbase.cache.max.writer.wait=5000ms
hive.metastore.hbase.cache.ttl=600s
hive.metastore.hbase.catalog.cache.size=50000
hive.metastore.hbase.connection.class=org.apache.hadoop.hive.metastore.hbase.VanillaHBaseConnection
hive.metastore.hbase.file.metadata.threads=1
hive.metastore.initial.metadata.count.enabled=true
hive.metastore.integral.jdo.pushdown=false
hive.metastore.kerberos.principal=hive-metastore/_HOST@EXAMPLE.COM
hive.metastore.limit.partition.request=-1
hive.metastore.metrics.enabled=false
hive.metastore.orm.retrieveMapNullsAsEmptyStrings=false
hive.metastore.port=9083
hive.metastore.rawstore.impl=org.apache.hadoop.hive.metastore.ObjectStore
hive.metastore.sasl.enabled=false
hive.metastore.schema.verification=true
hive.metastore.schema.verification.record.version=false
hive.metastore.server.max.message.size=104857600
hive.metastore.server.max.threads=1000
hive.metastore.server.min.threads=200
hive.metastore.server.tcp.keepalive=true
hive.metastore.stats.ndv.densityfunction=false
hive.metastore.stats.ndv.tuner=0.0
hive.metastore.thrift.compact.protocol.enabled=false
hive.metastore.thrift.framed.transport.enabled=false
hive.metastore.try.direct.sql=true
hive.metastore.try.direct.sql.ddl=true
hive.metastore.txn.store.impl=org.apache.hadoop.hive.metastore.txn.CompactionTxnHandler
hive.metastore.use.SSL=false
hive.metastore.warehouse.dir=adl://connectedsystemsuatdl.azuredatalakestore.net/user/hive/warehouse
hive.msck.path.validation=
    throw:
hive.msck.repair.batch.size=0
hive.multi.insert.move.tasks.share.dependencies=false
hive.multigroupby.singlereducer=true
hive.mv.files.thread=15
hive.new.job.grouping.set.cardinality=30
hive.optimize.bucketingsorting=true
hive.optimize.bucketmapjoin=false
hive.optimize.bucketmapjoin.sortedmerge=false
hive.optimize.constant.propagation=true
hive.optimize.correlation=false
hive.optimize.cte.materialize.threshold=-1
hive.optimize.distinct.rewrite=true
hive.optimize.dynamic.partition.hashjoin=false
hive.optimize.filter.stats.reduction=false
hive.optimize.groupby=true
hive.optimize.index.autoupdate=false
hive.optimize.index.filter=false
hive.optimize.index.filter.compact.maxsize=-1
hive.optimize.index.filter.compact.minsize=5368709120
hive.optimize.index.groupby=false
hive.optimize.limittranspose=false
hive.optimize.limittranspose.reductionpercentage=1.0
hive.optimize.limittranspose.reductiontuples=0
hive.optimize.listbucketing=false
hive.optimize.metadataonly=false
hive.optimize.null.scan=true
hive.optimize.partition.columns.separate=true
hive.optimize.point.lookup=true
hive.optimize.point.lookup.min=31
hive.optimize.ppd=true
hive.optimize.ppd.storage=true
hive.optimize.ppd.windowing=true
hive.optimize.reducededuplication=true
hive.optimize.reducededuplication.min.reducer=4
hive.optimize.remove.identity.project=true
hive.optimize.sampling.orderby=false
hive.optimize.sampling.orderby.number=1000
hive.optimize.sampling.orderby.percent=0.1
hive.optimize.semijoin.conversion=true
hive.optimize.skewjoin=false
hive.optimize.skewjoin.compiletime=false
hive.optimize.sort.dynamic.partition=false
hive.optimize.union.remove=false
hive.orc.cache.stripe.details.mem.size=256Mb
hive.orc.cache.use.soft.references=false
hive.orc.compute.splits.num.threads=10
hive.orc.splits.allow.synthetic.fileid=true
hive.orc.splits.directory.batch.ms=0
hive.orc.splits.include.file.footer=false
hive.orc.splits.include.fileid=true
hive.orc.splits.ms.footer.cache.enabled=false
hive.orc.splits.ms.footer.cache.ppd.enabled=true
hive.order.columnalignment=true
hive.orderby.position.alias=true
hive.parquet.timestamp.skip.conversion=true
hive.ppd.recognizetransivity=true
hive.ppd.remove.duplicatefilters=true
hive.prewarm.enabled=false
hive.prewarm.numcontainers=10
hive.query.result.fileformat=SequenceFile
hive.query.timeout.seconds=0s
hive.querylog.enable.plan.progress=true
hive.querylog.location=/local_disk0/tmp/root
hive.querylog.plan.progress.interval=60000
hive.reorder.nway.joins=true
hive.repl.cm.enabled=false
hive.repl.cm.interval=3600s
hive.repl.cm.retain=24h
hive.repl.cmrootdir=/user/hive/cmroot/
hive.repl.rootdir=/user/hive/repl/
hive.repl.task.factory=org.apache.hive.hcatalog.api.repl.exim.EximReplicationTaskFactory
hive.resultset.use.unique.column.names=true
hive.rework.mapredwork=false
hive.rpc.query.plan=false
hive.sample.seednumber=0
hive.scratch.dir.permission=700
hive.scratchdir.lock=false
hive.script.auto.progress=false
hive.script.operator.env.blacklist=hive.txn.valid.txns,hive.script.operator.env.blacklist
hive.script.operator.id.env.var=HIVE_SCRIPT_OPERATOR_ID
hive.script.operator.truncate.env=false
hive.script.recordreader=org.apache.hadoop.hive.ql.exec.TextRecordReader
hive.script.recordwriter=org.apache.hadoop.hive.ql.exec.TextRecordWriter
hive.script.serde=org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
hive.security.authenticator.manager=org.apache.hadoop.hive.ql.security.HadoopDefaultAuthenticator
hive.security.authorization.createtable.owner.grants=INSERT,SELECT,UPDATE,DELETE
hive.security.authorization.enabled=false
hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactory
hive.security.authorization.sqlstd.confwhitelist=hive\.auto\..*|hive\.cbo\..*|hive\.convert\..*|hive\.exec\.dynamic\.partition.*|hive\.exec\.max\.dynamic\.partitions.*|hive\.exec\.compress\..*|hive\.exec\.infer\..*|hive\.exec\.mode.local\..*|hive\.exec\.orc\..*|hive\.exec\.parallel.*|hive\.explain\..*|hive\.fetch.task\..*|hive\.groupby\..*|hive\.hbase\..*|hive\.index\..*|hive\.index\..*|hive\.intermediate\..*|hive\.join\..*|hive\.limit\..*|hive\.log\..*|hive\.mapjoin\..*|hive\.merge\..*|hive\.optimize\..*|hive\.orc\..*|hive\.outerjoin\..*|hive\.parquet\..*|hive\.ppd\..*|hive\.prewarm\..*|hive\.server2\.thrift\.resultset\.default\.fetch\.size|hive\.server2\.proxy\.user|hive\.skewjoin\..*|hive\.smbjoin\..*|hive\.stats\..*|hive\.strict\..*|hive\.tez\..*|hive\.vectorized\..*|mapred\.map\..*|mapred\.reduce\..*|mapred\.output\.compression\.codec|mapred\.job\.queuename|mapred\.output\.compression\.type|mapred\.min\.split\.size|mapreduce\.job\.reduce\.slowstart\.completedmaps|mapreduce\.job\.queuename|mapreduce\.job\.tags|mapreduce\.input\.fileinputformat\.split\.minsize|mapreduce\.map\..*|mapreduce\.reduce\..*|mapreduce\.output\.fileoutputformat\.compress\.codec|mapreduce\.output\.fileoutputformat\.compress\.type|oozie\..*|tez\.am\..*|tez\.task\..*|tez\.runtime\..*|tez\.queue\.name|hive\.transpose\.aggr\.join|hive\.exec\.reducers\.bytes\.per\.reducer|hive\.client\.stats\.counters|hive\.exec\.default\.partition\.name|hive\.exec\.drop\.ignorenonexistent|hive\.counters\.group\.name|hive\.default\.fileformat\.managed|hive\.enforce\.bucketmapjoin|hive\.enforce\.sortmergebucketmapjoin|hive\.cache\.expr\.evaluation|hive\.query\.result\.fileformat|hive\.hashtable\.loadfactor|hive\.hashtable\.initialCapacity|hive\.ignore\.mapjoin\.hint|hive\.limit\.row\.max\.size|hive\.mapred\.mode|hive\.map\.aggr|hive\.compute\.query\.using\.stats|hive\.exec\.rowoffset|hive\.variable\.substitute|hive\.variable\.substitute\.depth|hive\.autogen\.columnalias\.prefix\.includefuncname|hive\.autogen\.columnalias\.prefix\.label|hive\.exec\.check\.crossproducts|hive\.cli\.tez\.session\.async|hive\.compat|hive\.exec\.concatenate\.check\.index|hive\.display\.partition\.cols\.separately|hive\.error\.on\.empty\.partition|hive\.execution\.engine|hive\.exec\.copyfile\.maxsize|hive\.exim\.uri\.scheme\.whitelist|hive\.file\.max\.footer|hive\.insert\.into\.multilevel\.dirs|hive\.localize\.resource\.num\.wait\.attempts|hive\.multi\.insert\.move\.tasks\.share\.dependencies|hive\.support\.quoted\.identifiers|hive\.resultset\.use\.unique\.column\.names|hive\.analyze\.stmt\.collect\.partlevel\.stats|hive\.exec\.schema\.evolution|hive\.server2\.logging\.operation\.level|hive\.server2\.thrift\.resultset\.serialize\.in\.tasks|hive\.support\.special\.characters\.tablename|hive\.exec\.job\.debug\.capture\.stacktraces|hive\.exec\.job\.debug\.timeout|hive\.llap\.io\.enabled|hive\.llap\.io\.use\.fileid\.path|hive\.llap\.daemon\.service\.hosts|hive\.llap\.execution\.mode|hive\.llap\.auto\.allow\.uber|hive\.llap\.auto\.enforce\.tree|hive\.llap\.auto\.enforce\.vectorized|hive\.llap\.auto\.enforce\.stats|hive\.llap\.auto\.max\.input\.size|hive\.llap\.auto\.max\.output\.size|hive\.llap\.skip\.compile\.udf\.check|hive\.llap\.client\.consistent\.splits|hive\.llap\.enable\.grace\.join\.in\.llap|hive\.llap\.allow\.permanent\.fns|hive\.exec\.max\.created\.files|hive\.exec\.reducers\.max|hive\.reorder\.nway\.joins|hive\.output\.file\.extension|hive\.exec\.show\.job\.failure\.debug\.info|hive\.exec\.tasklog\.debug\.timeout|hive\.query\.id
hive.security.authorization.task.factory=org.apache.hadoop.hive.ql.parse.authorization.HiveAuthorizationTaskFactoryImpl
hive.security.command.whitelist=set,reset,dfs,add,list,delete,reload,compile
hive.security.metastore.authenticator.manager=org.apache.hadoop.hive.ql.security.HadoopDefaultMetastoreAuthenticator
hive.security.metastore.authorization.auth.reads=true
hive.security.metastore.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.DefaultHiveMetastoreAuthorizationProvider
hive.serdes.using.metastore.for.schema=org.apache.hadoop.hive.ql.io.orc.OrcSerde,org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe,org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe,org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDe,org.apache.hadoop.hive.serde2.MetadataTypedColumnsetSerDe,org.apache.hadoop.hive.serde2.columnar.LazyBinaryColumnarSerDe,org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe,org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
hive.server.read.socket.timeout=10
hive.server.tcp.keepalive=true
hive.server2.allow.user.substitution=true
hive.server2.async.exec.async.compile=false
hive.server2.async.exec.keepalive.time=10
hive.server2.async.exec.shutdown.timeout=10
hive.server2.async.exec.threads=100
hive.server2.async.exec.wait.queue.size=100
hive.server2.authentication=NONE
hive.server2.authentication.ldap.groupClassKey=groupOfNames
hive.server2.authentication.ldap.groupMembershipKey=member
hive.server2.authentication.ldap.guidKey=uid
hive.server2.clear.dangling.scratchdir=false
hive.server2.clear.dangling.scratchdir.interval=1800s
hive.server2.close.session.on.disconnect=true
hive.server2.compile.lock.timeout=0s
hive.server2.enable.doAs=true
hive.server2.global.init.file.location=${env:HIVE_CONF_DIR}
hive.server2.idle.operation.timeout=7200000
hive.server2.idle.session.check.operation=true
hive.server2.idle.session.timeout=900000
hive.server2.in.place.progress=true
hive.server2.keystore.password=
hive.server2.keystore.path=
    /databricks/keys/jetty-ssl-driver-keystore.jks:
hive.server2.llap.concurrent.queries=-1
hive.server2.logging.operation.enabled=true
hive.server2.logging.operation.level=EXECUTION
hive.server2.logging.operation.log.location=/local_disk0/tmp/root/operation_logs
hive.server2.long.polling.timeout=5000
hive.server2.map.fair.scheduler.queue=true
hive.server2.max.start.attempts=30
hive.server2.metrics.enabled=false
hive.server2.parallel.ops.in.session=true
hive.server2.session.check.interval=60000
hive.server2.sleep.interval.between.start.attempts=60s
hive.server2.support.dynamic.service.discovery=false
hive.server2.table.type.mapping=CLASSIC
hive.server2.tez.initialize.default.sessions=false
hive.server2.tez.session.lifetime=162h
hive.server2.tez.session.lifetime.jitter=3h
hive.server2.tez.sessions.custom.queue.allowed=true
hive.server2.tez.sessions.init.threads=16
hive.server2.tez.sessions.per.default.queue=1
hive.server2.thrift.client.connect.retry.limit=1
hive.server2.thrift.client.password=anonymous
hive.server2.thrift.client.retry.delay.seconds=1s
hive.server2.thrift.client.retry.limit=1
hive.server2.thrift.client.user=anonymous
hive.server2.thrift.exponential.backoff.slot.length=100
hive.server2.thrift.http.cookie.auth.enabled=true
hive.server2.thrift.http.cookie.is.httponly=true
hive.server2.thrift.http.cookie.is.secure=true
hive.server2.thrift.http.cookie.max.age=86400
hive.server2.thrift.http.max.idle.time=1800000
hive.server2.thrift.http.path=
    cliservice:
hive.server2.thrift.http.port=10000
hive.server2.thrift.http.request.header.size=6144
hive.server2.thrift.http.response.header.size=6144
hive.server2.thrift.http.worker.keepalive.time=60
hive.server2.thrift.login.timeout=20
hive.server2.thrift.max.message.size=104857600
hive.server2.thrift.max.worker.threads=500
hive.server2.thrift.min.worker.threads=5
hive.server2.thrift.port=10000
hive.server2.thrift.resultset.default.fetch.size=1000
hive.server2.thrift.resultset.max.fetch.size=10000
hive.server2.thrift.resultset.serialize.in.tasks=false
hive.server2.thrift.sasl.qop=auth
hive.server2.thrift.worker.keepalive.time=60
hive.server2.transport.mode=http
hive.server2.use.SSL=true
hive.server2.webui.host=0.0.0.0
hive.server2.webui.max.historic.queries=25
hive.server2.webui.max.threads=50
hive.server2.webui.port=10002
hive.server2.webui.spnego.principal=HTTP/_HOST@EXAMPLE.COM
hive.server2.webui.use.spnego=false
hive.server2.webui.use.ssl=false
hive.server2.xsrf.filter.enabled=false
hive.server2.zookeeper.namespace=hiveserver2
hive.server2.zookeeper.publish.configs=true
hive.service.metrics.class=org.apache.hadoop.hive.common.metrics.metrics2.CodahaleMetrics
hive.service.metrics.file.frequency=5s
hive.service.metrics.file.location=/tmp/report.json
hive.service.metrics.hadoop2.component=hive
hive.service.metrics.hadoop2.frequency=30s
hive.service.metrics.reporter=JSON_FILE, JMX
hive.session.history.enabled=false
hive.session.id=74f4940e-a6bf-4ecf-b225-d4debe0f74b4
hive.session.silent=false
hive.skewjoin.key=100000
hive.skewjoin.mapjoin.map.tasks=10000
hive.skewjoin.mapjoin.min.split=33554432
hive.smbjoin.cache.rows=10000
hive.spark.client.connect.timeout=1000
hive.spark.client.future.timeout=60
hive.spark.client.rpc.max.size=52428800
hive.spark.client.rpc.sasl.mechanisms=DIGEST-MD5
hive.spark.client.rpc.threads=8
hive.spark.client.secret.bits=256
hive.spark.client.server.connect.timeout=90000
hive.spark.dynamic.partition.pruning=false
hive.spark.dynamic.partition.pruning.max.data.size=104857600
hive.spark.exec.inplace.progress=true
hive.spark.job.monitor.timeout=60
hive.spark.use.file.size.for.mapjoin=false
hive.spark.use.groupby.shuffle=true
hive.spark.use.op.stats=true
hive.ssl.protocol.blacklist=SSLv2,SSLv3
hive.stageid.rearrange=none
hive.start.cleanup.scratchdir=false
hive.stats.atomic=false
hive.stats.autogather=false
hive.stats.collect.scancols=false
hive.stats.collect.tablekeys=false
hive.stats.column.autogather=false
hive.stats.dbclass=fs
hive.stats.deserialization.factor=1.0
hive.stats.fetch.column.stats=false
hive.stats.fetch.partition.stats=true
hive.stats.filter.in.factor=1.0
hive.stats.gather.num.threads=10
hive.stats.jdbc.timeout=30
hive.stats.join.factor=1.1
hive.stats.list.num.entries=10
hive.stats.map.num.entries=10
hive.stats.max.variable.length=100
hive.stats.ndv.error=20.0
hive.stats.reliable=false
hive.stats.retries.wait=3000
hive.strict.checks.bucketing=true
hive.strict.checks.cartesian.product=true
hive.strict.checks.large.query=false
hive.strict.checks.type.safety=true
hive.support.concurrency=false
hive.support.quoted.identifiers=column
hive.support.special.characters.tablename=true
hive.test.authz.sstd.hs2.mode=false
hive.test.fail.compaction=false
hive.test.fail.heartbeater=false
hive.test.mode=false
hive.test.mode.prefix=test_
hive.test.mode.samplefreq=32
hive.test.rollbacktxn=false
hive.tez.auto.reducer.parallelism=false
hive.tez.bigtable.minsize.semijoin.reduction=1000000
hive.tez.bloom.filter.factor=2.0
hive.tez.bucket.pruning=false
hive.tez.bucket.pruning.compat=true
hive.tez.container.max.java.heap.fraction=0.8
hive.tez.container.size=-1
hive.tez.cpu.vcores=-1
hive.tez.dynamic.partition.pruning=true
hive.tez.dynamic.partition.pruning.max.data.size=104857600
hive.tez.dynamic.partition.pruning.max.event.size=1048576
hive.tez.dynamic.semijoin.reduction=true
hive.tez.dynamic.semijoin.reduction.threshold=0.5
hive.tez.enable.memory.manager=true
hive.tez.exec.inplace.progress=true
hive.tez.exec.print.summary=false
hive.tez.hs2.user.access=true
hive.tez.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat
hive.tez.input.generate.consistent.splits=true
hive.tez.log.level=INFO
hive.tez.max.bloom.filter.entries=100000000
hive.tez.max.partition.factor=2.0
hive.tez.min.bloom.filter.entries=1000000
hive.tez.min.partition.factor=0.25
hive.tez.smb.number.waves=0.5
hive.tez.task.scale.memory.reserve-fraction.min=0.3
hive.tez.task.scale.memory.reserve.fraction=-1.0
hive.tez.task.scale.memory.reserve.fraction.max=0.5
hive.timedout.txn.reaper.interval=180s
hive.timedout.txn.reaper.start=100s
hive.transactional.events.mem=10000000
hive.transactional.table.scan=false
hive.transform.escape.input=false
hive.transpose.aggr.join=false
hive.txn.heartbeat.threadpool.size=5
hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DummyTxnManager
hive.txn.manager.dump.lock.state.on.acquire.timeout=false
hive.txn.max.open.batch=1000
hive.txn.operational.properties=0
hive.txn.strict.locking.mode=true
hive.txn.timeout=300
hive.typecheck.on.insert=true
hive.udtf.auto.progress=false
hive.unlock.numretries=10
hive.user.install.directory=/user/
hive.variable.substitute=true
hive.variable.substitute.depth=40
hive.vectorized.adaptor.usage.mode=all
hive.vectorized.execution.enabled=false
hive.vectorized.execution.mapjoin.minmax.enabled=false
hive.vectorized.execution.mapjoin.native.enabled=true
hive.vectorized.execution.mapjoin.native.fast.hashtable.enabled=false
hive.vectorized.execution.mapjoin.native.multikey.only.enabled=false
hive.vectorized.execution.mapjoin.overflow.repeated.threshold=-1
hive.vectorized.execution.reduce.enabled=true
hive.vectorized.execution.reduce.groupby.enabled=true
hive.vectorized.execution.reducesink.new.enabled=true
hive.vectorized.groupby.checkinterval=100000
hive.vectorized.groupby.flush.percent=0.1
hive.vectorized.groupby.maxentries=1000000
hive.vectorized.use.row.serde.deserialize=false
hive.vectorized.use.vector.serde.deserialize=true
hive.vectorized.use.vectorized.input.format=true
hive.warehouse.subdir.inherit.perms=false
hive.writeset.reaper.interval=60s
hive.zookeeper.clean.extra.nodes=false
hive.zookeeper.client.port=2181
hive.zookeeper.connection.basesleeptime=1000
hive.zookeeper.connection.max.retries=3
hive.zookeeper.namespace=hive_zookeeper_namespace
hive.zookeeper.session.timeout=1200000
io.compression.codec.bzip2.library=system-native
io.file.buffer.size=65536
io.map.index.interval=128
io.map.index.skip=0
io.mapfile.bloom.error.rate=0.005
io.mapfile.bloom.size=1048576
io.native.lib.available=true
io.seqfile.compress.blocksize=1000000
io.seqfile.lazydecompress=true
io.seqfile.local.dir=/local_disk0/tmp/io/local
io.seqfile.sorter.recordlimit=1000000
io.serializations=org.apache.hadoop.io.serializer.WritableSerialization,org.apache.hadoop.io.serializer.avro.AvroSpecificSerialization,org.apache.hadoop.io.serializer.avro.AvroReflectSerialization
io.skip.checksum.errors=false
ipc.client.connect.max.retries=10
ipc.client.connect.max.retries.on.timeouts=45
ipc.client.connect.retry.interval=1000
ipc.client.connect.timeout=20000
ipc.client.connection.maxidletime=10000
ipc.client.fallback-to-simple-auth-allowed=false
ipc.client.idlethreshold=4000
ipc.client.kill.max=10
ipc.client.ping=true
ipc.client.rpc-timeout.ms=0
ipc.maximum.data.length=67108864
ipc.ping.interval=60000
ipc.server.listen.queue.size=128
ipc.server.max.connections=0
javax.jdo.PersistenceManagerFactoryClass=org.datanucleus.api.jdo.JDOPersistenceManagerFactory
javax.jdo.option.ConnectionDriverName=org.postgresql.Driver
javax.jdo.option.ConnectionPassword=
javax.jdo.option.ConnectionURL=jdbc:postgresql://uathmsdb.trans.enterprisecsp.net:5432/hive_adls
javax.jdo.option.ConnectionUserName=tedbuat
javax.jdo.option.DetachAllOnCommit=true
javax.jdo.option.Multithreaded=true
javax.jdo.option.NonTransactionalRead=true
map.sort.class=org.apache.hadoop.util.QuickSort
mapred.child.java.opts=-Xmx200m
mapred.output.committer.class=com.databricks.backend.daemon.data.client.DirectOutputCommitter
mapreduce.am.max-attempts=2
mapreduce.app-submission.cross-platform=false
mapreduce.client.completion.pollinterval=5000
mapreduce.client.output.filter=FAILED
mapreduce.client.progressmonitor.pollinterval=1000
mapreduce.client.submit.file.replication=10
mapreduce.cluster.acls.enabled=false
mapreduce.cluster.local.dir=/local_disk0/tmp/mapred/local
mapreduce.cluster.temp.dir=/local_disk0/tmp/mapred/temp
mapreduce.fileoutputcommitter.algorithm.version=2
mapreduce.framework.name=local
mapreduce.ifile.readahead=true
mapreduce.ifile.readahead.bytes=4194304
mapreduce.input.fileinputformat.list-status.num-threads=1
mapreduce.input.fileinputformat.split.maxsize=256000000
mapreduce.input.fileinputformat.split.minsize=0
mapreduce.input.fileinputformat.split.minsize.per.node=1
mapreduce.input.fileinputformat.split.minsize.per.rack=1
mapreduce.input.lineinputformat.linespermap=1
mapreduce.job.acl-modify-job= 
mapreduce.job.acl-view-job= 
mapreduce.job.classloader=false
mapreduce.job.committer.setup.cleanup.needed=true
mapreduce.job.complete.cancel.delegation.tokens=true
mapreduce.job.counters.max=120
mapreduce.job.emit-timeline-data=false
mapreduce.job.end-notification.max.attempts=5
mapreduce.job.end-notification.max.retry.interval=5000
mapreduce.job.end-notification.retry.attempts=0
mapreduce.job.end-notification.retry.interval=1000
mapreduce.job.hdfs-servers=dbfs:///
mapreduce.job.jvm.numtasks=1
mapreduce.job.map.output.collector.class=org.apache.hadoop.mapred.MapTask$MapOutputBuffer
mapreduce.job.maps=2
mapreduce.job.max.split.locations=10
mapreduce.job.maxtaskfailures.per.tracker=3
mapreduce.job.queuename=default
mapreduce.job.reduce.shuffle.consumer.plugin.class=org.apache.hadoop.mapreduce.task.reduce.Shuffle
mapreduce.job.reduce.slowstart.completedmaps=0.05
mapreduce.job.reducer.preempt.delay.sec=0
mapreduce.job.reducer.unconditional-preempt.delay.sec=300
mapreduce.job.reduces=100
mapreduce.job.running.map.limit=0
mapreduce.job.running.reduce.limit=0
mapreduce.job.speculative.minimum-allowed-tasks=10
mapreduce.job.speculative.retry-after-no-speculate=1000
mapreduce.job.speculative.retry-after-speculate=15000
mapreduce.job.speculative.slowtaskthreshold=1.0
mapreduce.job.speculative.speculative-cap-running-tasks=0.1
mapreduce.job.speculative.speculative-cap-total-tasks=0.01
mapreduce.job.split.metainfo.maxsize=10000000
mapreduce.job.token.tracking.ids.enabled=false
mapreduce.job.ubertask.enable=false
mapreduce.job.ubertask.maxmaps=9
mapreduce.job.ubertask.maxreduces=1
mapreduce.job.userlog.retain.hours=24
mapreduce.jobhistory.address=0.0.0.0:10020
mapreduce.jobhistory.admin.acl=*
mapreduce.jobhistory.admin.address=0.0.0.0:10033
mapreduce.jobhistory.cleaner.enable=true
mapreduce.jobhistory.cleaner.interval-ms=86400000
mapreduce.jobhistory.client.thread-count=10
mapreduce.jobhistory.datestring.cache.size=200000
mapreduce.jobhistory.done-dir=/tmp/hadoop-yarn/staging/history/done
mapreduce.jobhistory.http.policy=HTTP_ONLY
mapreduce.jobhistory.intermediate-done-dir=/tmp/hadoop-yarn/staging/history/done_intermediate
mapreduce.jobhistory.joblist.cache.size=20000
mapreduce.jobhistory.keytab=/etc/security/keytab/jhs.service.keytab
mapreduce.jobhistory.loadedjobs.cache.size=5
mapreduce.jobhistory.max-age-ms=604800000
mapreduce.jobhistory.minicluster.fixed.ports=false
mapreduce.jobhistory.move.interval-ms=180000
mapreduce.jobhistory.move.thread-count=3
mapreduce.jobhistory.principal=jhs/_HOST@REALM.TLD
mapreduce.jobhistory.recovery.enable=false
mapreduce.jobhistory.recovery.store.class=org.apache.hadoop.mapreduce.v2.hs.HistoryServerFileSystemStateStoreService
mapreduce.jobhistory.recovery.store.fs.uri=/local_disk0/tmp/mapred/history/recoverystore
mapreduce.jobhistory.recovery.store.leveldb.path=
    /local_disk0/tmp/mapred/history/recoverystore:
mapreduce.jobhistory.webapp.address=0.0.0.0:19888
mapreduce.jobtracker.address=local
mapreduce.jobtracker.expire.trackers.interval=600000
mapreduce.jobtracker.handler.count=10
mapreduce.jobtracker.heartbeats.in.second=100
mapreduce.jobtracker.http.address=0.0.0.0:50030
mapreduce.jobtracker.instrumentation=org.apache.hadoop.mapred.JobTrackerMetricsInst
mapreduce.jobtracker.jobhistory.block.size=3145728
mapreduce.jobtracker.jobhistory.lru.cache.size=5
mapreduce.jobtracker.jobhistory.task.numberprogresssplits=12
mapreduce.jobtracker.maxtasks.perjob=-1
mapreduce.jobtracker.persist.jobstatus.active=true
mapreduce.jobtracker.persist.jobstatus.dir=/jobtracker/jobsInfo
mapreduce.jobtracker.persist.jobstatus.hours=1
mapreduce.jobtracker.restart.recover=false
mapreduce.jobtracker.retiredjobs.cache.size=1000
mapreduce.jobtracker.staging.root.dir=/local_disk0/tmp/mapred/staging
mapreduce.jobtracker.system.dir=/local_disk0/tmp/mapred/system
mapreduce.jobtracker.taskcache.levels=2
mapreduce.jobtracker.taskscheduler=org.apache.hadoop.mapred.JobQueueTaskScheduler
mapreduce.jobtracker.tasktracker.maxblacklists=4
mapreduce.local.clientfactory.class.name=org.apache.hadoop.mapred.LocalClientFactory
mapreduce.map.cpu.vcores=1
mapreduce.map.log.level=INFO
mapreduce.map.maxattempts=4
mapreduce.map.memory.mb=1024
mapreduce.map.output.compress=false
mapreduce.map.output.compress.codec=org.apache.hadoop.io.compress.DefaultCodec
mapreduce.map.skip.maxrecords=0
mapreduce.map.skip.proc.count.autoincr=true
mapreduce.map.sort.spill.percent=0.80
mapreduce.map.speculative=true
mapreduce.output.fileoutputformat.compress=false
mapreduce.output.fileoutputformat.compress.codec=org.apache.hadoop.io.compress.DefaultCodec
mapreduce.output.fileoutputformat.compress.type=RECORD
mapreduce.reduce.cpu.vcores=1
mapreduce.reduce.input.buffer.percent=0.0
mapreduce.reduce.log.level=INFO
mapreduce.reduce.markreset.buffer.percent=0.0
mapreduce.reduce.maxattempts=4
mapreduce.reduce.memory.mb=1024
mapreduce.reduce.merge.inmem.threshold=1000
mapreduce.reduce.shuffle.connect.timeout=180000
mapreduce.reduce.shuffle.fetch.retry.enabled=false
mapreduce.reduce.shuffle.fetch.retry.interval-ms=1000
mapreduce.reduce.shuffle.fetch.retry.timeout-ms=30000
mapreduce.reduce.shuffle.input.buffer.percent=0.70
mapreduce.reduce.shuffle.memory.limit.percent=0.25
mapreduce.reduce.shuffle.merge.percent=0.66
mapreduce.reduce.shuffle.parallelcopies=5
mapreduce.reduce.shuffle.read.timeout=180000
mapreduce.reduce.shuffle.retry-delay.max.ms=60000
mapreduce.reduce.skip.maxgroups=0
mapreduce.reduce.skip.proc.count.autoincr=true
mapreduce.reduce.speculative=true
mapreduce.shuffle.connection-keep-alive.enable=false
mapreduce.shuffle.connection-keep-alive.timeout=5
mapreduce.shuffle.max.connections=0
mapreduce.shuffle.max.threads=0
mapreduce.shuffle.port=13562
mapreduce.shuffle.ssl.enabled=false
mapreduce.shuffle.ssl.file.buffer.size=65536
mapreduce.shuffle.transfer.buffer.size=131072
mapreduce.task.combine.progress.records=10000
mapreduce.task.files.preserve.failedtasks=false
mapreduce.task.io.sort.factor=10
mapreduce.task.io.sort.mb=100
mapreduce.task.merge.progress.records=10000
mapreduce.task.profile=false
mapreduce.task.profile.map.params=-agentlib:hprof=cpu=samples,heap=sites,force=n,thread=y,verbose=n,file=%s
mapreduce.task.profile.maps=0-2
mapreduce.task.profile.params=-agentlib:hprof=cpu=samples,heap=sites,force=n,thread=y,verbose=n,file=%s
mapreduce.task.profile.reduce.params=-agentlib:hprof=cpu=samples,heap=sites,force=n,thread=y,verbose=n,file=%s
mapreduce.task.profile.reduces=0-2
mapreduce.task.skip.start.attempts=2
mapreduce.task.timeout=600000
mapreduce.task.userlog.limit.kb=0
mapreduce.tasktracker.dns.interface=default
mapreduce.tasktracker.dns.nameserver=default
mapreduce.tasktracker.healthchecker.interval=60000
mapreduce.tasktracker.healthchecker.script.timeout=600000
mapreduce.tasktracker.http.address=0.0.0.0:50060
mapreduce.tasktracker.http.threads=40
mapreduce.tasktracker.indexcache.mb=10
mapreduce.tasktracker.instrumentation=org.apache.hadoop.mapred.TaskTrackerMetricsInst
mapreduce.tasktracker.local.dir.minspacekill=0
mapreduce.tasktracker.local.dir.minspacestart=0
mapreduce.tasktracker.map.tasks.maximum=2
mapreduce.tasktracker.outofband.heartbeat=false
mapreduce.tasktracker.reduce.tasks.maximum=2
mapreduce.tasktracker.report.address=127.0.0.1:0
mapreduce.tasktracker.taskcontroller=org.apache.hadoop.mapred.DefaultTaskController
mapreduce.tasktracker.taskmemorymanager.monitoringinterval=5000
mapreduce.tasktracker.tasks.sleeptimebeforesigkill=5000
net.topology.impl=org.apache.hadoop.net.NetworkTopology
net.topology.node.switch.mapping.impl=org.apache.hadoop.net.ScriptBasedMapping
net.topology.script.number.args=100
nfs.allow.insecure.ports=true
nfs.dump.dir=/tmp/.hdfs-nfs
nfs.exports.allowed.hosts=* rw
nfs.mountd.port=4242
nfs.rtmax=1048576
nfs.server.port=2049
nfs.wtmax=1048576
parquet.memory.pool.ratio=0.5
rpc.metrics.quantile.enable=false
s3.blocksize=67108864
s3.bytes-per-checksum=512
s3.client-write-packet-size=65536
s3.replication=3
s3.stream-buffer-size=4096
s3native.blocksize=67108864
s3native.bytes-per-checksum=512
s3native.client-write-packet-size=65536
s3native.replication=3
s3native.stream-buffer-size=4096
spark.akka.frameSize=256
spark.app.id=app-20191016182633-0000
spark.app.name=Databricks Shell
spark.cleaner.referenceTracking.blocking=false
spark.databricks.acl.client=com.databricks.spark.sql.acl.client.SparkSqlAclClient
spark.databricks.acl.dfAclsEnabled=false
spark.databricks.acl.provider=com.databricks.sql.acl.ReflectionBackedAclProvider
spark.databricks.aggressiveWindowDownS=300
spark.databricks.cloudProvider=Azure
spark.databricks.clusterSource=JOB
spark.databricks.clusterUsageTags.autoTerminationMinutes=0
spark.databricks.clusterUsageTags.clusterAggressiveAutoscalingWindowDownSeconds=300
spark.databricks.clusterUsageTags.clusterAllTags=[{"key":"Vendor","value":"Databricks"},{"key":"Creator","value":"basheer.ahamed@nielsen.com"},{"key":"ClusterName","value":"job-67852-run-1"},{"key":"ClusterId","value":"1016-182237-curvy914"},{"key":"DatabricksEnvironment","value":"workerenv-5614106561397268"}]
spark.databricks.clusterUsageTags.clusterCreator=JobLauncher
spark.databricks.clusterUsageTags.clusterGeneration=0
spark.databricks.clusterUsageTags.clusterId=1016-182237-curvy914
spark.databricks.clusterUsageTags.clusterLogDeliveryEnabled=true
spark.databricks.clusterUsageTags.clusterLogDestination=dbfs:/mnt/te_mount/uat_logs/
spark.databricks.clusterUsageTags.clusterMaxWorkers=5
spark.databricks.clusterUsageTags.clusterMetastoreAccessType=RDS_DIRECT
spark.databricks.clusterUsageTags.clusterMinWorkers=1
spark.databricks.clusterUsageTags.clusterName=job-67852-run-1
spark.databricks.clusterUsageTags.clusterNoDriverDaemon=false
spark.databricks.clusterUsageTags.clusterNodeType=Standard_E16s_v3
spark.databricks.clusterUsageTags.clusterNumSshKeys=0
spark.databricks.clusterUsageTags.clusterOwnerOrgId=5614106561397268
spark.databricks.clusterUsageTags.clusterOwnerUserId=3464818780517260
spark.databricks.clusterUsageTags.clusterPinned=false
spark.databricks.clusterUsageTags.clusterPythonVersion=2
spark.databricks.clusterUsageTags.clusterResourceClass=default
spark.databricks.clusterUsageTags.clusterScalingType=autoscaling
spark.databricks.clusterUsageTags.clusterSku=STANDARD_SKU
spark.databricks.clusterUsageTags.clusterState=Pending
spark.databricks.clusterUsageTags.clusterStateMessage=Starting Spark
spark.databricks.clusterUsageTags.clusterTargetWorkers=1
spark.databricks.clusterUsageTags.clusterWorkers=1
spark.databricks.clusterUsageTags.containerType=LXC
spark.databricks.clusterUsageTags.driverContainerId=ccdfd0f301a84238ba2f33c2219a0f05
spark.databricks.clusterUsageTags.driverContainerPrivateIp=10.139.64.182
spark.databricks.clusterUsageTags.driverInstanceId=5424d279de114d129f1901ceb18591f4
spark.databricks.clusterUsageTags.driverInstancePrivateIp=10.139.0.182
spark.databricks.clusterUsageTags.driverNodeType=Standard_E16s_v3
spark.databricks.clusterUsageTags.driverPublicDns=52.232.160.41
spark.databricks.clusterUsageTags.enableCredentialPassthrough=false
spark.databricks.clusterUsageTags.enableDfAcls=false
spark.databricks.clusterUsageTags.enableElasticDisk=true
spark.databricks.clusterUsageTags.enableJdbcAutoStart=true
spark.databricks.clusterUsageTags.enableJobsAutostart=true
spark.databricks.clusterUsageTags.enableLocalDiskEncryption=false
spark.databricks.clusterUsageTags.enableSqlAclsOnly=false
spark.databricks.clusterUsageTags.hailEnabled=false
spark.databricks.clusterUsageTags.isSingleUserCluster=false
spark.databricks.clusterUsageTags.numPerClusterInitScriptsV2=0
spark.databricks.clusterUsageTags.sparkVersion=5.2.x-scala2.11
spark.databricks.clusterUsageTags.userProvidedRemoteVolumeCount=0
spark.databricks.clusterUsageTags.userProvidedRemoteVolumeSizeGb=0
spark.databricks.clusterUsageTags.userProvidedRemoteVolumeType=azure_disk_volume_type: PREMIUM_LRS

spark.databricks.clusterUsageTags.workerEnvironmentId=workerenv-5614106561397268
spark.databricks.credential.redactor=com.databricks.logging.secrets.CredentialRedactorProxyImpl
spark.databricks.delta.logStore.crossCloud.fatal=true
spark.databricks.delta.multiClusterWrites.enabled=true
spark.databricks.delta.preview.enabled=true
spark.databricks.driverNodeTypeId=Standard_E16s_v3
spark.databricks.eventLog.dir=eventlogs
spark.databricks.io.directoryCommit.enableLogicalDelete=false
spark.databricks.overrideDefaultCommitProtocol=org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol
spark.databricks.passthrough.adls.tokenProviderClassName=com.databricks.backend.daemon.data.client.adl.AdlCredentialContextTokenProvider
spark.databricks.passthrough.s3a.tokenProviderClassName=com.databricks.backend.daemon.driver.aws.AwsCredentialContextTokenProvider
spark.databricks.preemption.enabled=true
spark.databricks.r.cleanWorkspace=true
spark.databricks.redactor=com.databricks.spark.util.DatabricksSparkLogRedactorProxy
spark.databricks.session.share=false
spark.databricks.sparkContextId=1665401093835382800
spark.databricks.sql.hive.msck.batchSize=200
spark.databricks.sql.hive.msck.parallelism=20
spark.databricks.tahoe.logStore.aws.class=com.databricks.tahoe.store.MultiClusterLogStore
spark.databricks.tahoe.logStore.azure.class=com.databricks.tahoe.store.AzureLogStore
spark.databricks.tahoe.logStore.class=com.databricks.tahoe.store.DelegatingLogStore
spark.databricks.workerNodeTypeId=Standard_E16s_v3
spark.driver.allowMultipleContexts=false
spark.driver.host=10.139.64.182
spark.driver.maxResultSize=0
spark.driver.port=33603
spark.driver.tempDirectory=/local_disk0/tmp
spark.eventLog.enabled=false
spark.executor.extraClassPath=
    /databricks/spark/dbconf/log4j/executor:
    /databricks/spark/dbconf/jets3t/:
    /databricks/spark/dbconf/hadoop:
    /databricks/hive/conf:
    /databricks/jars/api-base--api-base_java-spark_2.4_2.11_deploy.jar:
    /databricks/jars/api-base--api-base-spark_2.4_2.11_deploy.jar:
    /databricks/jars/api--rpc--rpc_parser-spark_2.4_2.11_deploy.jar:
    /databricks/jars/chauffeur-api--api--endpoints--endpoints-spark_2.4_2.11_deploy.jar:
    /databricks/jars/chauffeur-api--chauffeur-api-spark_2.4_2.11_deploy.jar:
    /databricks/jars/common--client--client-spark_2.4_2.11_deploy.jar:
    /databricks/jars/common--common-spark_2.4_2.11_deploy.jar:
    /databricks/jars/common--credentials--credentials-spark_2.4_2.11_deploy.jar:
    /databricks/jars/common--hadoop--hadoop-spark_2.4_2.11_deploy.jar:
    /databricks/jars/common--jetty--client--client-spark_2.4_2.11_deploy.jar:
    /databricks/jars/common--lazy--lazy-spark_2.4_2.11_deploy.jar:
    /databricks/jars/common--libcommon_resources.jar:
    /databricks/jars/common--path--path-spark_2.4_2.11_deploy.jar:
    /databricks/jars/common--rate-limiter--rate-limiter-spark_2.4_2.11_deploy.jar:
    /databricks/jars/common--storage--storage-spark_2.4_2.11_deploy.jar:
    /databricks/jars/daemon--data--client--client-spark_2.4_2.11_deploy.jar:
    /databricks/jars/daemon--data--client--conf--conf-spark_2.4_2.11_deploy.jar:
    /databricks/jars/daemon--data--client--utils-spark_2.4_2.11_deploy.jar:
    /databricks/jars/daemon--data--data-common--data-common-spark_2.4_2.11_deploy.jar:
    /databricks/jars/dbfs--utils--dbfs-utils-spark_2.4_2.11_deploy.jar:
    /databricks/jars/extern--acl--auth--auth-spark_2.4_2.11_deploy.jar:
    /databricks/jars/extern--extern-spark_2.4_2.11_deploy.jar:
    /databricks/jars/extern--libaws-regions.jar:
    /databricks/jars/----jackson_annotations_shaded--libjackson-annotations.jar:
    /databricks/jars/----jackson_core_shaded--libjackson-core.jar:
    /databricks/jars/----jackson_databind_shaded--libjackson-databind.jar:
    /databricks/jars/----jackson_datatype_joda_shaded--libjackson-datatype-joda.jar:
    /databricks/jars/jsonutil--jsonutil-spark_2.4_2.11_deploy.jar:
    /databricks/jars/logging--log4j-mod--log4j-mod-spark_2.4_2.11_deploy.jar:
    /databricks/jars/logging--utils--logging-utils-spark_2.4_2.11_deploy.jar:
    /databricks/jars/s3commit--client--client-spark_2.4_2.11_deploy.jar:
    /databricks/jars/s3commit--common--common-spark_2.4_2.11_deploy.jar:
    /databricks/jars/s3--s3-spark_2.4_2.11_deploy.jar:
    /databricks/jars/secret-manager--api--api-spark_2.4_2.11_deploy.jar:
    /databricks/jars/spark--common--spark-common-spark_2.4_2.11_deploy.jar:
    /databricks/jars/spark--conf-reader--conf-reader_lib-spark_2.4_2.11_deploy.jar:
    /databricks/jars/spark--dbutils--dbutils-api-spark_2.4_2.11_deploy.jar:
    /databricks/jars/spark--driver--antlr--parser-spark_2.4_2.11_deploy.jar:
    /databricks/jars/spark--driver--common--driver-common-spark_2.4_2.11_deploy.jar:
    /databricks/jars/spark--driver--display--display-spark_2.4_2.11_deploy.jar:
    /databricks/jars/spark--driver--driver-spark_2.4_2.11_deploy.jar:
    /databricks/jars/spark--driver--events-spark_2.4_2.11_deploy.jar:
    /databricks/jars/spark--driver--ml--ml-spark_2.4_2.11_deploy.jar:
    /databricks/jars/spark--driver--secret-redaction-spark_2.4_2.11_deploy.jar:
    /databricks/jars/spark--driver--spark--resources-resources.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--antlr--antlr--antlr__antlr__2.7.7.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--amazon-kinesis-client--com.amazonaws__amazon-kinesis-client__1.8.10.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-autoscaling--com.amazonaws__aws-java-sdk-autoscaling__1.11.313.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-cloudformation--com.amazonaws__aws-java-sdk-cloudformation__1.11.313.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-cloudfront--com.amazonaws__aws-java-sdk-cloudfront__1.11.313.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-cloudhsm--com.amazonaws__aws-java-sdk-cloudhsm__1.11.313.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-cloudsearch--com.amazonaws__aws-java-sdk-cloudsearch__1.11.313.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-cloudtrail--com.amazonaws__aws-java-sdk-cloudtrail__1.11.313.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-cloudwatch--com.amazonaws__aws-java-sdk-cloudwatch__1.11.313.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-cloudwatchmetrics--com.amazonaws__aws-java-sdk-cloudwatchmetrics__1.11.313.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-codedeploy--com.amazonaws__aws-java-sdk-codedeploy__1.11.313.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-cognitoidentity--com.amazonaws__aws-java-sdk-cognitoidentity__1.11.313.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-cognitosync--com.amazonaws__aws-java-sdk-cognitosync__1.11.313.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-config--com.amazonaws__aws-java-sdk-config__1.11.313.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-core--com.amazonaws__aws-java-sdk-core__1.11.313.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-datapipeline--com.amazonaws__aws-java-sdk-datapipeline__1.11.313.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-directconnect--com.amazonaws__aws-java-sdk-directconnect__1.11.313.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-directory--com.amazonaws__aws-java-sdk-directory__1.11.313.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-dynamodb--com.amazonaws__aws-java-sdk-dynamodb__1.11.313.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-ec2--com.amazonaws__aws-java-sdk-ec2__1.11.313.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-ecs--com.amazonaws__aws-java-sdk-ecs__1.11.313.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-efs--com.amazonaws__aws-java-sdk-efs__1.11.313.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-elasticache--com.amazonaws__aws-java-sdk-elasticache__1.11.313.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-elasticbeanstalk--com.amazonaws__aws-java-sdk-elasticbeanstalk__1.11.313.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-elasticloadbalancing--com.amazonaws__aws-java-sdk-elasticloadbalancing__1.11.313.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-elastictranscoder--com.amazonaws__aws-java-sdk-elastictranscoder__1.11.313.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-emr--com.amazonaws__aws-java-sdk-emr__1.11.313.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-glacier--com.amazonaws__aws-java-sdk-glacier__1.11.313.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-iam--com.amazonaws__aws-java-sdk-iam__1.11.313.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-importexport--com.amazonaws__aws-java-sdk-importexport__1.11.313.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-kinesis--com.amazonaws__aws-java-sdk-kinesis__1.11.313.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-kms--com.amazonaws__aws-java-sdk-kms__1.11.313.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-lambda--com.amazonaws__aws-java-sdk-lambda__1.11.313.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-logs--com.amazonaws__aws-java-sdk-logs__1.11.313.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-machinelearning--com.amazonaws__aws-java-sdk-machinelearning__1.11.313.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-opsworks--com.amazonaws__aws-java-sdk-opsworks__1.11.313.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-rds--com.amazonaws__aws-java-sdk-rds__1.11.313.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-redshift--com.amazonaws__aws-java-sdk-redshift__1.11.313.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-route53--com.amazonaws__aws-java-sdk-route53__1.11.313.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-s3--com.amazonaws__aws-java-sdk-s3__1.11.313.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-ses--com.amazonaws__aws-java-sdk-ses__1.11.313.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-simpledb--com.amazonaws__aws-java-sdk-simpledb__1.11.313.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-simpleworkflow--com.amazonaws__aws-java-sdk-simpleworkflow__1.11.313.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-sns--com.amazonaws__aws-java-sdk-sns__1.11.313.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-sqs--com.amazonaws__aws-java-sdk-sqs__1.11.313.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-ssm--com.amazonaws__aws-java-sdk-ssm__1.11.313.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-storagegateway--com.amazonaws__aws-java-sdk-storagegateway__1.11.313.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-sts--com.amazonaws__aws-java-sdk-sts__1.11.313.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-support--com.amazonaws__aws-java-sdk-support__1.11.313.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-swf-libraries--com.amazonaws__aws-java-sdk-swf-libraries__1.11.22.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-workspaces--com.amazonaws__aws-java-sdk-workspaces__1.11.313.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--jmespath-java--com.amazonaws__jmespath-java__1.11.313.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.carrotsearch--hppc--com.carrotsearch__hppc__0.7.2.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.chuusai--shapeless_2.11--com.chuusai__shapeless_2.11__2.3.2.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.clearspring.analytics--stream--com.clearspring.analytics__stream__2.7.0.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.databricks--dbml-local_2.11--com.databricks__dbml-local_2.11__0.5.0-db8-spark2.4.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.databricks--dbml-local_2.11-tests--com.databricks__dbml-local_2.11-tests__0.5.0-db8-spark2.4.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.databricks--jets3t--com.databricks__jets3t__0.7.1-0.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.databricks--Rserve--com.databricks__Rserve__1.8-3.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.databricks.scalapb--compilerplugin_2.11--com.databricks.scalapb__compilerplugin_2.11__0.4.15-9.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.databricks.scalapb--scalapb-runtime_2.11--com.databricks.scalapb__scalapb-runtime_2.11__0.4.15-9.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.esotericsoftware--kryo-shaded--com.esotericsoftware__kryo-shaded__4.0.2.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.esotericsoftware--minlog--com.esotericsoftware__minlog__1.3.0.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.fasterxml--classmate--com.fasterxml__classmate__1.0.0.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.fasterxml.jackson.core--jackson-annotations--com.fasterxml.jackson.core__jackson-annotations__2.6.7.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.fasterxml.jackson.core--jackson-core--com.fasterxml.jackson.core__jackson-core__2.6.7.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.fasterxml.jackson.core--jackson-databind--com.fasterxml.jackson.core__jackson-databind__2.6.7.1.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.fasterxml.jackson.dataformat--jackson-dataformat-cbor--com.fasterxml.jackson.dataformat__jackson-dataformat-cbor__2.6.7.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.fasterxml.jackson.datatype--jackson-datatype-joda--com.fasterxml.jackson.datatype__jackson-datatype-joda__2.6.7.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.fasterxml.jackson.module--jackson-module-paranamer--com.fasterxml.jackson.module__jackson-module-paranamer__2.6.7.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.fasterxml.jackson.module--jackson-module-scala_2.11--com.fasterxml.jackson.module__jackson-module-scala_2.11__2.6.7.1.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.github.fommil--jniloader--com.github.fommil__jniloader__1.1.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.github.fommil.netlib--core--com.github.fommil.netlib__core__1.1.2.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.github.fommil.netlib--native_ref-java--com.github.fommil.netlib__native_ref-java__1.1.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.github.fommil.netlib--native_ref-java-natives--com.github.fommil.netlib__native_ref-java-natives__1.1.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.github.fommil.netlib--native_system-java--com.github.fommil.netlib__native_system-java__1.1.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.github.fommil.netlib--native_system-java-natives--com.github.fommil.netlib__native_system-java-natives__1.1.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.github.fommil.netlib--netlib-native_ref-linux-x86_64-natives--com.github.fommil.netlib__netlib-native_ref-linux-x86_64-natives__1.1.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.github.fommil.netlib--netlib-native_system-linux-x86_64-natives--com.github.fommil.netlib__netlib-native_system-linux-x86_64-natives__1.1.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.github.luben--zstd-jni--com.github.luben__zstd-jni__1.3.2-2.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.github.rwl--jtransforms--com.github.rwl__jtransforms__2.4.0.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.google.code.findbugs--jsr305--com.google.code.findbugs__jsr305__2.0.1.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.google.code.gson--gson--com.google.code.gson__gson__2.2.4.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.googlecode.javaewah--JavaEWAH--com.googlecode.javaewah__JavaEWAH__0.3.2.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.google.guava--guava--com.google.guava__guava__15.0.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.google.protobuf--protobuf-java--com.google.protobuf__protobuf-java__2.6.1.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.h2database--h2--com.h2database__h2__1.3.174.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.jcraft--jsch--com.jcraft__jsch__0.1.50.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.jolbox--bonecp--com.jolbox__bonecp__0.8.0.RELEASE.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.microsoft.azure--azure-data-lake-store-sdk--com.microsoft.azure__azure-data-lake-store-sdk__2.2.8.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.microsoft.sqlserver--mssql-jdbc--com.microsoft.sqlserver__mssql-jdbc__6.2.2.jre8.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--commons-beanutils--commons-beanutils--commons-beanutils__commons-beanutils__1.7.0.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--commons-beanutils--commons-beanutils-core--commons-beanutils__commons-beanutils-core__1.8.0.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--commons-cli--commons-cli--commons-cli__commons-cli__1.2.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--commons-codec--commons-codec--commons-codec__commons-codec__1.10.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--commons-collections--commons-collections--commons-collections__commons-collections__3.2.2.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--commons-configuration--commons-configuration--commons-configuration__commons-configuration__1.6.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--commons-dbcp--commons-dbcp--commons-dbcp__commons-dbcp__1.4.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--commons-digester--commons-digester--commons-digester__commons-digester__1.8.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--commons-httpclient--commons-httpclient--commons-httpclient__commons-httpclient__3.1.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--commons-io--commons-io--commons-io__commons-io__2.4.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--commons-lang--commons-lang--commons-lang__commons-lang__2.6.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--commons-logging--commons-logging--commons-logging__commons-logging__1.1.3.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--commons-net--commons-net--commons-net__commons-net__3.1.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--commons-pool--commons-pool--commons-pool__commons-pool__1.5.4.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.ning--compress-lzf--com.ning__compress-lzf__1.0.3.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.sun.mail--javax.mail--com.sun.mail__javax.mail__1.5.2.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.thoughtworks.paranamer--paranamer--com.thoughtworks.paranamer__paranamer__2.8.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.trueaccord.lenses--lenses_2.11--com.trueaccord.lenses__lenses_2.11__0.3.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.twitter--chill_2.11--com.twitter__chill_2.11__0.9.3.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.twitter--chill-java--com.twitter__chill-java__0.9.3.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.twitter--parquet-hadoop-bundle--com.twitter__parquet-hadoop-bundle__1.6.0.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.twitter--util-app_2.11--com.twitter__util-app_2.11__6.23.0.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.twitter--util-core_2.11--com.twitter__util-core_2.11__6.23.0.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.twitter--util-jvm_2.11--com.twitter__util-jvm_2.11__6.23.0.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.typesafe--config--com.typesafe__config__1.2.1.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.typesafe.scala-logging--scala-logging-api_2.11--com.typesafe.scala-logging__scala-logging-api_2.11__2.1.2.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.typesafe.scala-logging--scala-logging-slf4j_2.11--com.typesafe.scala-logging__scala-logging-slf4j_2.11__2.1.2.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.univocity--univocity-parsers--com.univocity__univocity-parsers__2.7.3.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.vlkan--flatbuffers--com.vlkan__flatbuffers__1.2.0-3f79e055.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--com.zaxxer--HikariCP--com.zaxxer__HikariCP__3.1.0.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--info.ganglia.gmetric4j--gmetric4j--info.ganglia.gmetric4j__gmetric4j__1.0.7.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--io.airlift--aircompressor--io.airlift__aircompressor__0.10.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--io.dropwizard.metrics--metrics-core--io.dropwizard.metrics__metrics-core__3.1.5.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--io.dropwizard.metrics--metrics-ganglia--io.dropwizard.metrics__metrics-ganglia__3.1.5.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--io.dropwizard.metrics--metrics-graphite--io.dropwizard.metrics__metrics-graphite__3.1.5.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--io.dropwizard.metrics--metrics-healthchecks--io.dropwizard.metrics__metrics-healthchecks__3.1.5.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--io.dropwizard.metrics--metrics-jetty9--io.dropwizard.metrics__metrics-jetty9__3.1.5.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--io.dropwizard.metrics--metrics-json--io.dropwizard.metrics__metrics-json__3.1.5.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--io.dropwizard.metrics--metrics-jvm--io.dropwizard.metrics__metrics-jvm__3.1.5.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--io.dropwizard.metrics--metrics-log4j--io.dropwizard.metrics__metrics-log4j__3.1.5.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--io.dropwizard.metrics--metrics-servlets--io.dropwizard.metrics__metrics-servlets__3.1.5.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--io.netty--netty-all--io.netty__netty-all__4.1.17.Final.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--io.netty--netty--io.netty__netty__3.9.9.Final.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--io.prometheus.jmx--collector--io.prometheus.jmx__collector__0.7.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--io.prometheus--simpleclient_common--io.prometheus__simpleclient_common__0.0.16.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--io.prometheus--simpleclient_dropwizard--io.prometheus__simpleclient_dropwizard__0.0.16.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--io.prometheus--simpleclient--io.prometheus__simpleclient__0.0.16.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--io.prometheus--simpleclient_servlet--io.prometheus__simpleclient_servlet__0.0.16.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--javax.activation--activation--javax.activation__activation__1.1.1.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--javax.annotation--javax.annotation-api--javax.annotation__javax.annotation-api__1.2.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--javax.el--javax.el-api--javax.el__javax.el-api__2.2.4.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--javax.jdo--jdo-api--javax.jdo__jdo-api__3.0.1.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--javax.servlet--javax.servlet-api--javax.servlet__javax.servlet-api__3.1.0.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--javax.servlet.jsp--jsp-api--javax.servlet.jsp__jsp-api__2.1.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--javax.transaction--jta--javax.transaction__jta__1.1.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--javax.validation--validation-api--javax.validation__validation-api__1.1.0.Final.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--javax.ws.rs--javax.ws.rs-api--javax.ws.rs__javax.ws.rs-api__2.0.1.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--javax.xml.bind--jaxb-api--javax.xml.bind__jaxb-api__2.2.2.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--javax.xml.stream--stax-api--javax.xml.stream__stax-api__1.0-2.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--javolution--javolution--javolution__javolution__5.5.1.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--jline--jline--jline__jline__2.11.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--joda-time--joda-time--joda-time__joda-time__2.9.3.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--log4j--apache-log4j-extras--log4j__apache-log4j-extras__1.2.17.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--log4j--log4j--log4j__log4j__1.2.17.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--net.hydromatic--eigenbase-properties--net.hydromatic__eigenbase-properties__1.1.5.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--net.razorvine--pyrolite--net.razorvine__pyrolite__4.13.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--net.sf.jpam--jpam--net.sf.jpam__jpam__1.1.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--net.sf.opencsv--opencsv--net.sf.opencsv__opencsv__2.3.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--net.sf.supercsv--super-csv--net.sf.supercsv__super-csv__2.2.0.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--net.snowflake--snowflake-ingest-sdk--net.snowflake__snowflake-ingest-sdk__0.9.5.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--net.snowflake--snowflake-jdbc--net.snowflake__snowflake-jdbc__3.6.15.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--net.snowflake--spark-snowflake_2.11--net.snowflake__spark-snowflake_2.11__2.4.10-spark_2.4.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--net.sourceforge.f2j--arpack_combined_all--net.sourceforge.f2j__arpack_combined_all__0.1.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.acplt--oncrpc--org.acplt__oncrpc__1.0.7.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.antlr--antlr4-runtime--org.antlr__antlr4-runtime__4.7.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.antlr--antlr-runtime--org.antlr__antlr-runtime__3.4.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.antlr--ST4--org.antlr__ST4__4.0.4.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.antlr--stringtemplate--org.antlr__stringtemplate__3.2.1.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.apache.ant--ant-jsch--org.apache.ant__ant-jsch__1.9.2.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.apache.ant--ant-launcher--org.apache.ant__ant-launcher__1.9.2.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.apache.ant--ant--org.apache.ant__ant__1.9.2.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.apache.arrow--arrow-format--org.apache.arrow__arrow-format__0.10.0.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.apache.arrow--arrow-memory--org.apache.arrow__arrow-memory__0.10.0.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.apache.arrow--arrow-vector--org.apache.arrow__arrow-vector__0.10.0.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.apache.avro--avro-ipc--org.apache.avro__avro-ipc__1.8.2.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.apache.avro--avro-mapred-hadoop2--org.apache.avro__avro-mapred-hadoop2__1.8.2.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.apache.avro--avro--org.apache.avro__avro__1.8.2.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.apache.calcite--calcite-avatica--org.apache.calcite__calcite-avatica__1.2.0-incubating.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.apache.calcite--calcite-core--org.apache.calcite__calcite-core__1.2.0-incubating.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.apache.calcite--calcite-linq4j--org.apache.calcite__calcite-linq4j__1.2.0-incubating.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.apache.commons--commons-compress--org.apache.commons__commons-compress__1.8.1.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.apache.commons--commons-crypto--org.apache.commons__commons-crypto__1.0.0.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.apache.commons--commons-lang3--org.apache.commons__commons-lang3__3.5.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.apache.commons--commons-math3--org.apache.commons__commons-math3__3.4.1.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.apache.curator--curator-client--org.apache.curator__curator-client__2.7.1.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.apache.curator--curator-framework--org.apache.curator__curator-framework__2.7.1.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.apache.curator--curator-recipes--org.apache.curator__curator-recipes__2.7.1.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.apache.derby--derby--org.apache.derby__derby__10.12.1.1.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.apache.directory.api--api-asn1-api--org.apache.directory.api__api-asn1-api__1.0.0-M20.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.apache.directory.api--api-util--org.apache.directory.api__api-util__1.0.0-M20.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.apache.directory.server--apacheds-i18n--org.apache.directory.server__apacheds-i18n__2.0.0-M15.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.apache.directory.server--apacheds-kerberos-codec--org.apache.directory.server__apacheds-kerberos-codec__2.0.0-M15.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.apache.hadoop--hadoop-annotations--org.apache.hadoop__hadoop-annotations__2.7.3.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.apache.hadoop--hadoop-auth--org.apache.hadoop__hadoop-auth__2.7.3.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.apache.hadoop--hadoop-client--org.apache.hadoop__hadoop-client__2.7.3.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.apache.hadoop--hadoop-common--org.apache.hadoop__hadoop-common__2.7.3.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.apache.hadoop--hadoop-hdfs--org.apache.hadoop__hadoop-hdfs__2.7.3.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.apache.hadoop--hadoop-mapreduce-client-app--org.apache.hadoop__hadoop-mapreduce-client-app__2.7.3.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.apache.hadoop--hadoop-mapreduce-client-common--org.apache.hadoop__hadoop-mapreduce-client-common__2.7.3.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.apache.hadoop--hadoop-mapreduce-client-core--org.apache.hadoop__hadoop-mapreduce-client-core__2.7.3.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.apache.hadoop--hadoop-mapreduce-client-jobclient--org.apache.hadoop__hadoop-mapreduce-client-jobclient__2.7.3.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.apache.hadoop--hadoop-mapreduce-client-shuffle--org.apache.hadoop__hadoop-mapreduce-client-shuffle__2.7.3.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.apache.hadoop--hadoop-yarn-api--org.apache.hadoop__hadoop-yarn-api__2.7.3.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.apache.hadoop--hadoop-yarn-client--org.apache.hadoop__hadoop-yarn-client__2.7.3.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.apache.hadoop--hadoop-yarn-common--org.apache.hadoop__hadoop-yarn-common__2.7.3.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.apache.hadoop--hadoop-yarn-server-common--org.apache.hadoop__hadoop-yarn-server-common__2.7.3.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.apache.htrace--htrace-core--org.apache.htrace__htrace-core__3.1.0-incubating.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.apache.httpcomponents--httpclient--org.apache.httpcomponents__httpclient__4.5.4.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.apache.httpcomponents--httpcore--org.apache.httpcomponents__httpcore__4.4.8.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.apache.ivy--ivy--org.apache.ivy__ivy__2.4.0.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.apache.orc--orc-core-nohive--org.apache.orc__orc-core-nohive__1.5.2.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.apache.orc--orc-mapreduce-nohive--org.apache.orc__orc-mapreduce-nohive__1.5.2.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.apache.orc--orc-shims--org.apache.orc__orc-shims__1.5.2.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.apache.parquet--parquet-column--org.apache.parquet__parquet-column__1.10.1-databricks3.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.apache.parquet--parquet-common--org.apache.parquet__parquet-common__1.10.1-databricks3.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.apache.parquet--parquet-encoding--org.apache.parquet__parquet-encoding__1.10.1-databricks3.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.apache.parquet--parquet-format--org.apache.parquet__parquet-format__2.4.0.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.apache.parquet--parquet-hadoop--org.apache.parquet__parquet-hadoop__1.10.1-databricks3.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.apache.parquet--parquet-jackson--org.apache.parquet__parquet-jackson__1.10.1-databricks3.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.apache.thrift--libfb303--org.apache.thrift__libfb303__0.9.3.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.apache.thrift--libthrift--org.apache.thrift__libthrift__0.9.3.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.apache.xbean--xbean-asm6-shaded--org.apache.xbean__xbean-asm6-shaded__4.8.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.apache.zookeeper--zookeeper--org.apache.zookeeper__zookeeper__3.4.6.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.codehaus.jackson--jackson-core-asl--org.codehaus.jackson__jackson-core-asl__1.9.13.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.codehaus.jackson--jackson-jaxrs--org.codehaus.jackson__jackson-jaxrs__1.9.13.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.codehaus.jackson--jackson-mapper-asl--org.codehaus.jackson__jackson-mapper-asl__1.9.13.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.codehaus.jackson--jackson-xc--org.codehaus.jackson__jackson-xc__1.9.13.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.codehaus.janino--commons-compiler--org.codehaus.janino__commons-compiler__3.0.10.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.codehaus.janino--janino--org.codehaus.janino__janino__3.0.10.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.datanucleus--datanucleus-api-jdo--org.datanucleus__datanucleus-api-jdo__3.2.6.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.datanucleus--datanucleus-core--org.datanucleus__datanucleus-core__3.2.10.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.datanucleus--datanucleus-rdbms--org.datanucleus__datanucleus-rdbms__3.2.9.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-client--org.eclipse.jetty__jetty-client__9.3.20.v20170531.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-continuation--org.eclipse.jetty__jetty-continuation__9.3.20.v20170531.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-http--org.eclipse.jetty__jetty-http__9.3.20.v20170531.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-io--org.eclipse.jetty__jetty-io__9.3.20.v20170531.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-jndi--org.eclipse.jetty__jetty-jndi__9.3.20.v20170531.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-plus--org.eclipse.jetty__jetty-plus__9.3.20.v20170531.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-proxy--org.eclipse.jetty__jetty-proxy__9.3.20.v20170531.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-security--org.eclipse.jetty__jetty-security__9.3.20.v20170531.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-server--org.eclipse.jetty__jetty-server__9.3.20.v20170531.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-servlet--org.eclipse.jetty__jetty-servlet__9.3.20.v20170531.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-servlets--org.eclipse.jetty__jetty-servlets__9.3.20.v20170531.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-util--org.eclipse.jetty__jetty-util__9.3.20.v20170531.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-webapp--org.eclipse.jetty__jetty-webapp__9.3.20.v20170531.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-xml--org.eclipse.jetty__jetty-xml__9.3.20.v20170531.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.fusesource.leveldbjni--leveldbjni-all--org.fusesource.leveldbjni__leveldbjni-all__1.8.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.glassfish.hk2.external--aopalliance-repackaged--org.glassfish.hk2.external__aopalliance-repackaged__2.4.0-b34.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.glassfish.hk2.external--javax.inject--org.glassfish.hk2.external__javax.inject__2.4.0-b34.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.glassfish.hk2--hk2-api--org.glassfish.hk2__hk2-api__2.4.0-b34.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.glassfish.hk2--hk2-locator--org.glassfish.hk2__hk2-locator__2.4.0-b34.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.glassfish.hk2--hk2-utils--org.glassfish.hk2__hk2-utils__2.4.0-b34.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.glassfish.hk2--osgi-resource-locator--org.glassfish.hk2__osgi-resource-locator__1.0.1.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.glassfish.jersey.bundles.repackaged--jersey-guava--org.glassfish.jersey.bundles.repackaged__jersey-guava__2.22.2.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.glassfish.jersey.containers--jersey-container-servlet-core--org.glassfish.jersey.containers__jersey-container-servlet-core__2.22.2.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.glassfish.jersey.containers--jersey-container-servlet--org.glassfish.jersey.containers__jersey-container-servlet__2.22.2.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.glassfish.jersey.core--jersey-client--org.glassfish.jersey.core__jersey-client__2.22.2.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.glassfish.jersey.core--jersey-common--org.glassfish.jersey.core__jersey-common__2.22.2.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.glassfish.jersey.core--jersey-server--org.glassfish.jersey.core__jersey-server__2.22.2.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.glassfish.jersey.media--jersey-media-jaxb--org.glassfish.jersey.media__jersey-media-jaxb__2.22.2.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.hibernate--hibernate-validator--org.hibernate__hibernate-validator__5.1.1.Final.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.iq80.snappy--snappy--org.iq80.snappy__snappy__0.2.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.javassist--javassist--org.javassist__javassist__3.18.1-GA.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.jboss.logging--jboss-logging--org.jboss.logging__jboss-logging__3.1.3.GA.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.jdbi--jdbi--org.jdbi__jdbi__2.63.1.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.joda--joda-convert--org.joda__joda-convert__1.7.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.jodd--jodd-core--org.jodd__jodd-core__3.5.2.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.json4s--json4s-ast_2.11--org.json4s__json4s-ast_2.11__3.5.3.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.json4s--json4s-core_2.11--org.json4s__json4s-core_2.11__3.5.3.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.json4s--json4s-jackson_2.11--org.json4s__json4s-jackson_2.11__3.5.3.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.json4s--json4s-scalap_2.11--org.json4s__json4s-scalap_2.11__3.5.3.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.lz4--lz4-java--org.lz4__lz4-java__1.4.0.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.mariadb.jdbc--mariadb-java-client--org.mariadb.jdbc__mariadb-java-client__2.1.2.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.mockito--mockito-all--org.mockito__mockito-all__1.9.5.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.objenesis--objenesis--org.objenesis__objenesis__2.5.1.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.postgresql--postgresql--org.postgresql__postgresql__42.1.4.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.roaringbitmap--RoaringBitmap--org.roaringbitmap__RoaringBitmap__0.5.11.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.rocksdb--rocksdbjni--org.rocksdb__rocksdbjni__5.2.1.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.rosuda.REngine--REngine--org.rosuda.REngine__REngine__2.1.0.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.scalacheck--scalacheck_2.11--org.scalacheck__scalacheck_2.11__1.12.5.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.scalactic--scalactic_2.11--org.scalactic__scalactic_2.11__3.0.3.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.scala-lang.modules--scala-parser-combinators_2.11--org.scala-lang.modules__scala-parser-combinators_2.11__1.0.2.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.scala-lang.modules--scala-xml_2.11--org.scala-lang.modules__scala-xml_2.11__1.0.5.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.scala-lang--scala-compiler_2.11--org.scala-lang__scala-compiler__2.11.8.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.scala-lang--scala-library_2.11--org.scala-lang__scala-library__2.11.8.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.scala-lang--scala-reflect_2.11--org.scala-lang__scala-reflect__2.11.8.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.scalanlp--breeze_2.11--org.scalanlp__breeze_2.11__0.13.2.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.scalanlp--breeze-macros_2.11--org.scalanlp__breeze-macros_2.11__0.13.2.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.scala-sbt--test-interface--org.scala-sbt__test-interface__1.0.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.scalatest--scalatest_2.11--org.scalatest__scalatest_2.11__3.0.3.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.slf4j--jcl-over-slf4j--org.slf4j__jcl-over-slf4j__1.7.16.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.slf4j--jul-to-slf4j--org.slf4j__jul-to-slf4j__1.7.16.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.slf4j--slf4j-api--org.slf4j__slf4j-api__1.7.16.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.slf4j--slf4j-log4j12--org.slf4j__slf4j-log4j12__1.7.16.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.spark-project.hive--hive-beeline--org.spark-project.hive__hive-beeline__1.2.1.spark2.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.spark-project.hive--hive-cli--org.spark-project.hive__hive-cli__1.2.1.spark2.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.spark-project.hive--hive-exec--org.spark-project.hive__hive-exec__1.2.1.spark2.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.spark-project.hive--hive-jdbc--org.spark-project.hive__hive-jdbc__1.2.1.spark2.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.spark-project.hive--hive-metastore--org.spark-project.hive__hive-metastore__1.2.1.spark2.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.spark-project.spark--unused--org.spark-project.spark__unused__1.0.0.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.spire-math--spire_2.11--org.spire-math__spire_2.11__0.13.0.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.spire-math--spire-macros_2.11--org.spire-math__spire-macros_2.11__0.13.0.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.springframework--spring-core--org.springframework__spring-core__4.1.4.RELEASE.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.springframework--spring-test--org.springframework__spring-test__4.1.4.RELEASE.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.tukaani--xz--org.tukaani__xz__1.5.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.typelevel--machinist_2.11--org.typelevel__machinist_2.11__0.6.1.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.typelevel--macro-compat_2.11--org.typelevel__macro-compat_2.11__1.1.1.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.xerial.snappy--snappy-java--org.xerial.snappy__snappy-java__1.1.7.1.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.xerial--sqlite-jdbc--org.xerial__sqlite-jdbc__3.8.11.2.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--org.yaml--snakeyaml--org.yaml__snakeyaml__1.16.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--oro--oro--oro__oro__2.0.8.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--software.amazon.ion--ion-java--software.amazon.ion__ion-java__1.0.2.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--stax--stax-api--stax__stax-api__1.0.1.jar:
    /databricks/jars/spark--maven-trees--spark_2.4--xmlenc--xmlenc--xmlenc__xmlenc__0.52.jar:
    /databricks/jars/spark--sql-extension--sql-extension-spark_2.4_2.11_deploy.jar:
    /databricks/jars/spark--versions--2.4--avro_2.11_deploy_shaded.jar:
    /databricks/jars/spark--versions--2.4--catalyst_2.11_deploy.jar:
    /databricks/jars/spark--versions--2.4--core_2.11_deploy.jar:
    /databricks/jars/spark--versions--2.4--ganglia-lgpl_2.11_deploy.jar:
    /databricks/jars/spark--versions--2.4--graphx_2.11_deploy.jar:
    /databricks/jars/spark--versions--2.4--hive_2.11_deploy_shaded.jar:
    /databricks/jars/spark--versions--2.4--hive-thriftserver_2.11_deploy.jar:
    /databricks/jars/spark--versions--2.4--io.confluent__common-utils__4.0.0_shaded-for-avro.jar:
    /databricks/jars/spark--versions--2.4--io.confluent__kafka-schema-registry-client__4.0.0_shaded-for-avro.jar:
    /databricks/jars/spark--versions--2.4--kafka_2.11_only_kafka08_shaded.jar:
    /databricks/jars/spark--versions--2.4--kafka-clients_only_kafka08_shaded.jar:
    /databricks/jars/spark--versions--2.4--kafka-clients_only_shaded.jar:
    /databricks/jars/spark--versions--2.4--libspark-sql-parser-compiled.jar:
    /databricks/jars/spark--versions--2.4--metrics-core_only_kafka08_shaded.jar:
    /databricks/jars/spark--versions--2.4--mllib_2.11_deploy_shaded.jar:
    /databricks/jars/spark--versions--2.4--mllib-local_2.11_deploy.jar:
    /databricks/jars/spark--versions--2.4--org.apache.commons__commons-pool2__2.5.0_shaded-for-hive.jar:
    /databricks/jars/spark--versions--2.4--org.jpmml__pmml-model__1.2.15_shaded-for-mllib.jar:
    /databricks/jars/spark--versions--2.4--org.jpmml__pmml-schema__1.2.15_shaded-for-mllib.jar:
    /databricks/jars/spark--versions--2.4--py4j_2.11_deploy.jar:
    /databricks/jars/spark--versions--2.4--redshift_2.11_deploy.jar:
    /databricks/jars/spark--versions--2.4--repl_2.11_deploy.jar:
    /databricks/jars/spark--versions--2.4--shim_2.11_deploy.jar:
    /databricks/jars/spark--versions--2.4--spark_2.11_deploy.jar:
    /databricks/jars/spark--versions--2.4--spark-2.4-avro-resources-jar-resources.jar:
    /databricks/jars/spark--versions--2.4--spark-2.4-core-resources-jar-resources.jar:
    /databricks/jars/spark--versions--2.4--spark-2.4-hive-resources-jar-resources.jar:
    /databricks/jars/spark--versions--2.4--spark-2.4-mllib-resources-jar-resources.jar:
    /databricks/jars/spark--versions--2.4--spark-2.4-spark-sql-aws-resources-jar-resources.jar:
    /databricks/jars/spark--versions--2.4--spark-2.4-spark-sql-azure-resources-jar-resources.jar:
    /databricks/jars/spark--versions--2.4--spark-2.4-spark-sql-kafka-0-10-resources-jar-resources.jar:
    /databricks/jars/spark--versions--2.4--spark-2.4-spark-sql-kafka-0-8-resources-jar-resources.jar:
    /databricks/jars/spark--versions--2.4--spark-2.4-sql-resources-jar-resources.jar:
    /databricks/jars/spark--versions--2.4--spark-2.4-streaming-resources-jar-resources.jar:
    /databricks/jars/spark--versions--2.4--spark-sql-aws_2.11_deploy.jar:
    /databricks/jars/spark--versions--2.4--spark-sql-azure_2.11_deploy.jar:
    /databricks/jars/spark--versions--2.4--spark-sql-kafka-0-10_2.11_deploy_shaded.jar:
    /databricks/jars/spark--versions--2.4--spark-sql-kafka-0-8_2.11_deploy_shaded.jar:
    /databricks/jars/spark--versions--2.4--spark-sql-notification_2.11_deploy.jar:
    /databricks/jars/spark--versions--2.4--sql_2.11_deploy.jar:
    /databricks/jars/spark--versions--2.4--sqldw_2.11_deploy.jar:
    /databricks/jars/spark--versions--2.4--streaming_2.11_deploy.jar:
    /databricks/jars/spark--versions--2.4--tags_2.11_deploy.jar:
    /databricks/jars/third_party--azure--com.fasterxml.jackson.core__jackson-core__2.7.2_shaded.jar:
    /databricks/jars/third_party--azure--com.microsoft.azure__azure-keyvault-core__1.0.0_shaded.jar:
    /databricks/jars/third_party--azure--com.microsoft.azure__azure-storage__5.2.0_shaded.jar:
    /databricks/jars/third_party--azure--org.apache.commons__commons-lang3__3.4_shaded.jar:
    /databricks/jars/third_party--datalake--datalake-spark_2.4_2.11_deploy.jar:
    /databricks/jars/third_party--hadoop-azure-2.7.3-abfs--aopalliance__aopalliance__1.0_2.11_shaded_20180625_3682417.jar:
    /databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.fasterxml.jackson.core__jackson-annotations__2.7.0_2.11_shaded_20180625_3682417.jar:
    /databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.fasterxml.jackson.core__jackson-core__2.7.2_2.11_shaded_20180625_3682417.jar:
    /databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.fasterxml.jackson.core__jackson-core__2.7.2_2.11_shaded_20180920_b33d810.jar:
    /databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.fasterxml.jackson.core__jackson-databind__2.7.2_2.11_shaded_20180625_3682417.jar:
    /databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.fasterxml.jackson.datatype__jackson-datatype-joda__2.7.2_2.11_shaded_20180625_3682417.jar:
    /databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.google.code.findbugs__jsr305__1.3.9_2.11_shaded_20180920_b33d810.jar:
    /databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.google.guava__guava__11.0.2_2.11_shaded_20180920_b33d810.jar:
    /databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.google.guava__guava__16.0_2.11_shaded_20180625_3682417.jar:
    /databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.google.inject__guice__3.0_2.11_shaded_20180625_3682417.jar:
    /databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.microsoft.azure__azure-annotations__1.2.0_2.11_shaded_20180625_3682417.jar:
    /databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.microsoft.azure__azure-keyvault-core__1.0.0_2.11_shaded_20180920_b33d810.jar:
    /databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.microsoft.azure__azure-storage__7.0.0_2.11_shaded_20180920_b33d810.jar:
    /databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.microsoft.rest__client-runtime__1.1.0_2.11_shaded_20180625_3682417.jar:
    /databricks/jars/third_party--hadoop-azure-2.7.3-abfs--commons-codec__commons-codec__1.9_2.11_shaded_20180625_3682417.jar:
    /databricks/jars/third_party--hadoop-azure-2.7.3-abfs--commons-codec__commons-codec__1.9_2.11_shaded_20180920_b33d810.jar:
    /databricks/jars/third_party--hadoop-azure-2.7.3-abfs--commons-logging__commons-logging__1.2_2.11_shaded_20180625_3682417.jar:
    /databricks/jars/third_party--hadoop-azure-2.7.3-abfs--commons-logging__commons-logging__1.2_2.11_shaded_20180920_b33d810.jar:
    /databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.squareup.okhttp3__logging-interceptor__3.3.1_2.11_shaded_20180625_3682417.jar:
    /databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.squareup.okhttp3__okhttp__3.3.1_2.11_shaded_20180625_3682417.jar:
    /databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.squareup.okhttp3__okhttp-urlconnection__3.3.1_2.11_shaded_20180625_3682417.jar:
    /databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.squareup.okio__okio__1.8.0_2.11_shaded_20180625_3682417.jar:
    /databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.squareup.retrofit2__adapter-rxjava__2.1.0_2.11_shaded_20180625_3682417.jar:
    /databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.squareup.retrofit2__converter-jackson__2.1.0_2.11_shaded_20180625_3682417.jar:
    /databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.squareup.retrofit2__retrofit__2.1.0_2.11_shaded_20180625_3682417.jar:
    /databricks/jars/third_party--hadoop-azure-2.7.3-abfs--com.sun.xml.bind__jaxb-impl__2.2.3-1_2.11_shaded_20180625_3682417.jar:
    /databricks/jars/third_party--hadoop-azure-2.7.3-abfs--hadoop-azure-2.7.3-abfs-external-20180625_3682417-spark_2.4_2.11_deploy_shaded.jar:
    /databricks/jars/third_party--hadoop-azure-2.7.3-abfs--hadoop-azure-2.7.3-abfs-external-20180920_b33d810-spark_2.4_2.11_deploy_shaded.jar:
    /databricks/jars/third_party--hadoop-azure-2.7.3-abfs--io.netty__netty-all__4.0.52.Final_2.11_shaded_20180625_3682417.jar:
    /databricks/jars/third_party--hadoop-azure-2.7.3-abfs--io.reactivex__rxjava__1.2.4_2.11_shaded_20180625_3682417.jar:
    /databricks/jars/third_party--hadoop-azure-2.7.3-abfs--javax.activation__activation__1.1_2.11_shaded_20180625_3682417.jar:
    /databricks/jars/third_party--hadoop-azure-2.7.3-abfs--javax.inject__javax.inject__1_2.11_shaded_20180625_3682417.jar:
    /databricks/jars/third_party--hadoop-azure-2.7.3-abfs--javax.xml.bind__jaxb-api__2.2.2_2.11_shaded_20180625_3682417.jar:
    /databricks/jars/third_party--hadoop-azure-2.7.3-abfs--javax.xml.stream__stax-api__1.0-2_2.11_shaded_20180625_3682417.jar:
    /databricks/jars/third_party--hadoop-azure-2.7.3-abfs--joda-time__joda-time__2.4_2.11_shaded_20180625_3682417.jar:
    /databricks/jars/third_party--hadoop-azure-2.7.3-abfs--org.apache.htrace__htrace-core__3.1.0-incubating_2.11_shaded_20180625_3682417.jar:
    /databricks/jars/third_party--hadoop-azure-2.7.3-abfs--org.apache.httpcomponents__httpclient__4.5.2_2.11_shaded_20180625_3682417.jar:
    /databricks/jars/third_party--hadoop-azure-2.7.3-abfs--org.apache.httpcomponents__httpclient__4.5.2_2.11_shaded_20180920_b33d810.jar:
    /databricks/jars/third_party--hadoop-azure-2.7.3-abfs--org.apache.httpcomponents__httpcore__4.4.4_2.11_shaded_20180625_3682417.jar:
    /databricks/jars/third_party--hadoop-azure-2.7.3-abfs--org.apache.httpcomponents__httpcore__4.4.4_2.11_shaded_20180920_b33d810.jar:
    /databricks/jars/third_party--jackson--guava_only_shaded.jar:
    /databricks/jars/third_party--jackson--jackson-module-scala-shaded_2.11_deploy.jar:
    /databricks/jars/third_party--jackson--jsr305_only_shaded.jar:
    /databricks/jars/third_party--jackson--paranamer_only_shaded.jar:
    /databricks/jars/third_party--jetty8-shaded-client--databricks-patched-jetty-client-jar_shaded.jar:
    /databricks/jars/third_party--jetty8-shaded-client--databricks-patched-jetty-http-jar_shaded.jar:
    /databricks/jars/third_party--jetty8-shaded-client--jetty-io_shaded.jar:
    /databricks/jars/third_party--jetty8-shaded-client--jetty-jmx_shaded.jar:
    /databricks/jars/third_party--jetty8-shaded-client--jetty-util_shaded.jar:
    /databricks/jars/utils--process_utils-spark_2.4_2.11_deploy.jar:
    /databricks/jars/workflow--workflow-spark_2.4_2.11_deploy.jar:
spark.executor.extraJavaOptions=-Djava.io.tmpdir=/local_disk0/tmp -XX:ReservedCodeCacheSize=256m -XX:+UseCodeCacheFlushing -Djava.security.properties=/databricks/spark/dbconf/java/extra.security -XX:+PrintFlagsFinal -XX:+PrintGCDateStamps -verbose:gc -XX:+PrintGCDetails -Xss4m -Djavax.xml.datatype.DatatypeFactory=com.sun.org.apache.xerces.internal.jaxp.datatype.DatatypeFactoryImpl -Djavax.xml.parsers.DocumentBuilderFactory=com.sun.org.apache.xerces.internal.jaxp.DocumentBuilderFactoryImpl -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -Djavax.xml.validation.SchemaFactory:http://www.w3.org/2001/XMLSchema=com.sun.org.apache.xerces.internal.jaxp.validation.XMLSchemaFactory -Dorg.xml.sax.driver=com.sun.org.apache.xerces.internal.parsers.SAXParser -Dorg.w3c.dom.DOMImplementationSourceList=com.sun.org.apache.xerces.internal.dom.DOMXSImplementationSourceImpl -Ddatabricks.serviceName=spark-executor-1
spark.executor.id=driver
spark.executor.memory=97871m
spark.executor.tempDirectory=/local_disk0/tmp
spark.extraListeners=com.databricks.backend.daemon.driver.DBCEventLoggingListener
spark.files.fetchFailure.unRegisterOutputOnHost=true
spark.files.overwrite=true
spark.files.useFetchCache=false
spark.hadoop.databricks.dbfs.client.version=v2
spark.hadoop.databricks.s3commit.client.sslTrustAll=false
spark.hadoop.datanucleus.connectionPool.maxIdle=1
spark.hadoop.datanucleus.connectionPool.maxPoolSize=30
spark.hadoop.datanucleus.connectionPool.minPoolSize=15
spark.hadoop.dfs.adls.oauth2.access.token.provider.type=ClientCredential
spark.hadoop.dfs.adls.oauth2.client.id=003106c3-ff8c-4c62-a060-24b8b32ea512
spark.hadoop.dfs.adls.oauth2.credential=viUHaEfNg8cHQRJSRNMzpIv1T6goiUg+c9R/XSbYBFA=
spark.hadoop.dfs.adls.oauth2.refresh.url=https://login.microsoftonline.com/6ac7a1f4-5fb1-4153-bb4f-12d2020a1f7d/oauth2/token
spark.hadoop.fs.abfs.impl=shaded.databricks.v20180920_b33d810.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem
spark.hadoop.fs.abfs.impl.disable.cache=true
spark.hadoop.fs.abfss.impl=shaded.databricks.v20180920_b33d810.org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem
spark.hadoop.fs.abfss.impl.disable.cache=true
spark.hadoop.fs.adl.impl=com.databricks.adl.AdlFileSystem
spark.hadoop.fs.adl.impl.disable.cache=false
spark.hadoop.fs.azure.skip.metrics=true
spark.hadoop.fs.s3.impl=com.databricks.s3a.S3AFileSystem
spark.hadoop.fs.s3a.connection.maximum=200
spark.hadoop.fs.s3a.fast.upload=true
spark.hadoop.fs.s3a.fast.upload.default=true
spark.hadoop.fs.s3a.impl=com.databricks.s3a.S3AFileSystem
spark.hadoop.fs.s3a.multipart.size=10485760
spark.hadoop.fs.s3a.multipart.threshold=104857600
spark.hadoop.fs.s3a.threads.max=136
spark.hadoop.fs.s3n.impl=com.databricks.s3a.S3AFileSystem
spark.hadoop.fs.wasb.impl=shaded.databricks.org.apache.hadoop.fs.azure.NativeAzureFileSystem
spark.hadoop.fs.wasb.impl.disable.cache=true
spark.hadoop.fs.wasbs.impl=shaded.databricks.org.apache.hadoop.fs.azure.NativeAzureFileSystem
spark.hadoop.fs.wasbs.impl.disable.cache=true
spark.hadoop.hive.server2.idle.operation.timeout=7200000
spark.hadoop.hive.server2.idle.session.timeout=900000
spark.hadoop.hive.server2.keystore.password=gb1gQqZ9ZIHS
spark.hadoop.hive.server2.keystore.path=
    /databricks/keys/jetty-ssl-driver-keystore.jks:
spark.hadoop.hive.server2.session.check.interval=60000
spark.hadoop.hive.server2.thrift.http.port=10000
spark.hadoop.hive.server2.transport.mode=http
spark.hadoop.hive.server2.use.SSL=true
spark.hadoop.hive.warehouse.subdir.inherit.perms=false
spark.hadoop.javax.jdo.option.ConnectionDriverName=org.postgresql.Driver
spark.hadoop.javax.jdo.option.ConnectionPassword=560I890@fd80709
spark.hadoop.javax.jdo.option.ConnectionURL=jdbc:postgresql://uathmsdb.trans.enterprisecsp.net:5432/hive_adls
spark.hadoop.javax.jdo.option.ConnectionUserName=tedbuat
spark.hadoop.mapred.output.committer.class=com.databricks.backend.daemon.data.client.DirectOutputCommitter
spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version=2
spark.hadoop.parquet.memory.pool.ratio=0.5
spark.hadoop.spark.sql.parquet.output.committer.class=org.apache.spark.sql.parquet.DirectParquetOutputCommitter
spark.hadoop.spark.sql.sources.outputCommitterClass=com.databricks.backend.daemon.data.client.MapReduceDirectOutputCommitter
spark.hadoop.spark.thriftserver.customHeadersToProperties=X-Databricks-User-Token:spark.databricks.token,X-Databricks-Api-Url:spark.databricks.api.url
spark.home=/databricks/spark
spark.logConf=true
spark.master=spark://10.139.64.182:7077
spark.metrics.conf=/databricks/spark/conf/metrics.properties
spark.r.backendConnectionTimeout=604800
spark.r.numRBackendThreads=1
spark.r.sql.derby.temp.dir=/tmp/RtmpMuGVi7
spark.rdd.compress=true
spark.repl.class.outputDir=/local_disk0/tmp/repl/spark-1665401093835382800-d383f379-cd90-4f0a-b537-047f02636013
spark.repl.class.uri=spark://10.139.64.182:33603/classes
spark.scheduler.listenerbus.eventqueue.size=20000
spark.scheduler.mode=FAIR
spark.serializer.objectStreamReset=100
spark.shuffle.manager=SORT
spark.shuffle.memoryFraction=0.2
spark.shuffle.reduceLocality.enabled=false
spark.shuffle.service.enabled=true
spark.shuffle.service.port=4048
spark.sparkr.use.daemon=false
spark.speculation=false
spark.speculation.multiplier=3
spark.speculation.quantile=0.9
spark.sql.allowMultipleContexts=false
spark.sql.catalogImplementation=hive
spark.sql.hive.convertCTAS=true
spark.sql.hive.convertMetastoreParquet=true
spark.sql.hive.metastore.jars=maven
spark.sql.hive.metastore.sharedPrefixes=org.mariadb.jdbc,com.mysql.jdbc,org.postgresql,com.microsoft.sqlserver,microsoft.sql.DateTimeOffset,microsoft.sql.Types,com.databricks,com.codahale,com.fasterxml.jackson,shaded.databricks
spark.sql.hive.metastore.version=2.3.0
spark.sql.parquet.cacheMetadata=true
spark.sql.parquet.compression.codec=snappy
spark.sql.parquet.filterPushdown=false
spark.sql.parquet.output.committer.class=org.apache.spark.sql.parquet.DirectParquetOutputCommitter
spark.sql.shuffle.partitions=2000
spark.sql.sources.commitProtocolClass=com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol
spark.sql.sources.outputCommitterClass=com.databricks.backend.daemon.data.client.MapReduceDirectOutputCommitter
spark.sql.streaming.checkpointFileManagerClass=com.databricks.spark.sql.streaming.DatabricksCheckpointFileManager
spark.sql.ui.retainedExecutions=100
spark.sql.warehouse.dir=adl://connectedsystemsuatdl.azuredatalakestore.net/user/hive/warehouse
spark.storage.blockManagerTimeoutIntervalMs=300000
spark.storage.memoryFraction=0.5
spark.streaming.driver.writeAheadLog.allowBatching=true
spark.streaming.driver.writeAheadLog.closeFileAfterWrite=true
spark.task.maxFailures=10
spark.task.reaper.enabled=true
spark.task.reaper.killTimeout=60s
spark.thriftserver.customHeadersToProperties=X-Databricks-User-Token:spark.databricks.token,X-Databricks-Api-Url:spark.databricks.api.url
spark.ui.port=44095
spark.worker.cleanup.enabled=false
stream.stderr.reporter.enabled=true
stream.stderr.reporter.prefix=reporter:
tfile.fs.input.buffer.size=262144
tfile.fs.output.buffer.size=262144
tfile.io.chunk.size=1048576
yarn.acl.enable=false
yarn.admin.acl=*
yarn.am.liveness-monitor.expiry-interval-ms=600000
yarn.app.mapreduce.am.command-opts=-Xmx1024m
yarn.app.mapreduce.am.container.log.backups=0
yarn.app.mapreduce.am.container.log.limit.kb=0
yarn.app.mapreduce.am.containerlauncher.threadpool-initial-size=10
yarn.app.mapreduce.am.hard-kill-timeout-ms=10000
yarn.app.mapreduce.am.job.committer.cancel-timeout=60000
yarn.app.mapreduce.am.job.committer.commit-window=10000
yarn.app.mapreduce.am.job.task.listener.thread-count=30
yarn.app.mapreduce.am.resource.cpu-vcores=1
yarn.app.mapreduce.am.resource.mb=1536
yarn.app.mapreduce.am.scheduler.heartbeat.interval-ms=1000
yarn.app.mapreduce.am.staging-dir=/tmp/hadoop-yarn/staging
yarn.app.mapreduce.client-am.ipc.max-retries=3
yarn.app.mapreduce.client-am.ipc.max-retries-on-timeouts=3
yarn.app.mapreduce.client.job.max-retries=0
yarn.app.mapreduce.client.job.retry-interval=2000
yarn.app.mapreduce.client.max-retries=3
yarn.app.mapreduce.shuffle.log.backups=0
yarn.app.mapreduce.shuffle.log.limit.kb=0
yarn.app.mapreduce.shuffle.log.separate=true
yarn.app.mapreduce.task.container.log.backups=0
yarn.bin.path=
    yarn:
yarn.client.application-client-protocol.poll-interval-ms=200
yarn.client.failover-proxy-provider=org.apache.hadoop.yarn.client.ConfiguredRMFailoverProxyProvider
yarn.client.failover-retries=0
yarn.client.failover-retries-on-socket-timeouts=0
yarn.client.max-cached-nodemanagers-proxies=0
yarn.client.nodemanager-client-async.thread-pool-max-size=500
yarn.client.nodemanager-connect.max-wait-ms=180000
yarn.client.nodemanager-connect.retry-interval-ms=10000
yarn.dispatcher.drain-events.timeout=300000
yarn.fail-fast=false
yarn.http.policy=HTTP_ONLY
yarn.ipc.rpc.class=org.apache.hadoop.yarn.ipc.HadoopYarnProtoRPC
yarn.log-aggregation-enable=false
yarn.log-aggregation.retain-check-interval-seconds=-1
yarn.log-aggregation.retain-seconds=-1
yarn.nm.liveness-monitor.expiry-interval-ms=600000
yarn.nodemanager.address=0.0.0.0:0
yarn.nodemanager.admin-env=MALLOC_ARENA_MAX=$MALLOC_ARENA_MAX
yarn.nodemanager.aux-services.mapreduce_shuffle.class=org.apache.hadoop.mapred.ShuffleHandler
yarn.nodemanager.container-executor.class=org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor
yarn.nodemanager.container-manager.thread-count=20
yarn.nodemanager.container-metrics.unregister-delay-ms=10000
yarn.nodemanager.container-monitor.interval-ms=3000
yarn.nodemanager.container-monitor.procfs-tree.smaps-based-rss.enabled=false
yarn.nodemanager.delete.debug-delay-sec=0
yarn.nodemanager.delete.thread-count=4
yarn.nodemanager.disk-health-checker.interval-ms=120000
yarn.nodemanager.disk-health-checker.max-disk-utilization-per-disk-percentage=90.0
yarn.nodemanager.disk-health-checker.min-free-space-per-disk-mb=0
yarn.nodemanager.disk-health-checker.min-healthy-disks=0.25
yarn.nodemanager.docker-container-executor.exec-name=/usr/bin/docker
yarn.nodemanager.env-whitelist=JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,HADOOP_YARN_HOME
yarn.nodemanager.health-checker.interval-ms=600000
yarn.nodemanager.health-checker.script.timeout-ms=1200000
yarn.nodemanager.hostname=0.0.0.0
yarn.nodemanager.keytab=/etc/krb5.keytab
yarn.nodemanager.linux-container-executor.cgroups.hierarchy=/hadoop-yarn
yarn.nodemanager.linux-container-executor.cgroups.mount=false
yarn.nodemanager.linux-container-executor.cgroups.strict-resource-usage=false
yarn.nodemanager.linux-container-executor.nonsecure-mode.limit-users=true
yarn.nodemanager.linux-container-executor.nonsecure-mode.local-user=nobody
yarn.nodemanager.linux-container-executor.nonsecure-mode.user-pattern=^[_.A-Za-z0-9][-@_.A-Za-z0-9]{0,255}?[$]?$
yarn.nodemanager.linux-container-executor.resources-handler.class=org.apache.hadoop.yarn.server.nodemanager.util.DefaultLCEResourcesHandler
yarn.nodemanager.local-cache.max-files-per-directory=8192
yarn.nodemanager.local-dirs=/local_disk0/tmp/nm-local-dir
yarn.nodemanager.localizer.address=0.0.0.0:8040
yarn.nodemanager.localizer.cache.cleanup.interval-ms=600000
yarn.nodemanager.localizer.cache.target-size-mb=10240
yarn.nodemanager.localizer.client.thread-count=5
yarn.nodemanager.localizer.fetch.thread-count=4
yarn.nodemanager.log-aggregation.compression-type=none
yarn.nodemanager.log-aggregation.roll-monitoring-interval-seconds=-1
yarn.nodemanager.log-dirs=${yarn.log.dir}/userlogs
yarn.nodemanager.log.retain-seconds=10800
yarn.nodemanager.pmem-check-enabled=true
yarn.nodemanager.process-kill-wait.ms=2000
yarn.nodemanager.recovery.compaction-interval-secs=3600
yarn.nodemanager.recovery.dir=/local_disk0/tmp/yarn-nm-recovery
yarn.nodemanager.recovery.enabled=false
yarn.nodemanager.remote-app-log-dir=/tmp/logs
yarn.nodemanager.remote-app-log-dir-suffix=logs
yarn.nodemanager.resource.cpu-vcores=8
yarn.nodemanager.resource.memory-mb=8192
yarn.nodemanager.resource.percentage-physical-cpu-limit=100
yarn.nodemanager.resourcemanager.minimum.version=NONE
yarn.nodemanager.sleep-delay-before-sigkill.ms=250
yarn.nodemanager.vmem-check-enabled=true
yarn.nodemanager.vmem-pmem-ratio=2.1
yarn.nodemanager.webapp.address=0.0.0.0:8042
yarn.nodemanager.webapp.cross-origin.enabled=false
yarn.nodemanager.windows-container.cpu-limit.enabled=false
yarn.nodemanager.windows-container.memory-limit.enabled=false
yarn.resourcemanager.address=0.0.0.0:8032
yarn.resourcemanager.admin.address=0.0.0.0:8033
yarn.resourcemanager.admin.client.thread-count=1
yarn.resourcemanager.am-rm-tokens.master-key-rolling-interval-secs=86400
yarn.resourcemanager.am.max-attempts=2
yarn.resourcemanager.amlauncher.thread-count=50
yarn.resourcemanager.client.thread-count=50
yarn.resourcemanager.configuration.provider-class=org.apache.hadoop.yarn.LocalConfigurationProvider
yarn.resourcemanager.connect.max-wait.ms=900000
yarn.resourcemanager.connect.retry-interval.ms=30000
yarn.resourcemanager.container-tokens.master-key-rolling-interval-secs=86400
yarn.resourcemanager.container.liveness-monitor.interval-ms=600000
yarn.resourcemanager.delayed.delegation-token.removal-interval-ms=30000
yarn.resourcemanager.fail-fast=false
yarn.resourcemanager.fs.state-store.num-retries=0
yarn.resourcemanager.fs.state-store.retry-interval-ms=1000
yarn.resourcemanager.fs.state-store.retry-policy-spec=2000, 500
yarn.resourcemanager.fs.state-store.uri=/local_disk0/tmp/yarn/system/rmstore
yarn.resourcemanager.ha.automatic-failover.embedded=true
yarn.resourcemanager.ha.automatic-failover.enabled=true
yarn.resourcemanager.ha.automatic-failover.zk-base-path=
    /yarn-leader-election:
yarn.resourcemanager.ha.enabled=false
yarn.resourcemanager.hostname=0.0.0.0
yarn.resourcemanager.keytab=/etc/krb5.keytab
yarn.resourcemanager.leveldb-state-store.compaction-interval-secs=3600
yarn.resourcemanager.leveldb-state-store.path=
    /local_disk0/tmp/yarn/system/rmstore:
yarn.resourcemanager.max-completed-applications=10000
yarn.resourcemanager.nodemanager-connect-retries=10
yarn.resourcemanager.nodemanager.minimum.version=NONE
yarn.resourcemanager.nodemanagers.heartbeat-interval-ms=1000
yarn.resourcemanager.proxy-user-privileges.enabled=false
yarn.resourcemanager.recovery.enabled=false
yarn.resourcemanager.resource-tracker.address=0.0.0.0:8031
yarn.resourcemanager.resource-tracker.client.thread-count=50
yarn.resourcemanager.scheduler.address=0.0.0.0:8030
yarn.resourcemanager.scheduler.class=org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler
yarn.resourcemanager.scheduler.client.thread-count=50
yarn.resourcemanager.scheduler.monitor.enable=false
yarn.resourcemanager.scheduler.monitor.policies=org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy
yarn.resourcemanager.state-store.max-completed-applications=10000
yarn.resourcemanager.store.class=org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore
yarn.resourcemanager.system-metrics-publisher.dispatcher.pool-size=10
yarn.resourcemanager.system-metrics-publisher.enabled=false
yarn.resourcemanager.webapp.address=0.0.0.0:8088
yarn.resourcemanager.webapp.cross-origin.enabled=false
yarn.resourcemanager.webapp.delegation-token-auth-filter.enabled=true
yarn.resourcemanager.webapp.https.address=0.0.0.0:8090
yarn.resourcemanager.work-preserving-recovery.enabled=true
yarn.resourcemanager.work-preserving-recovery.scheduling-wait-ms=10000
yarn.resourcemanager.zk-acl=world:anyone:rwcda
yarn.resourcemanager.zk-num-retries=1000
yarn.resourcemanager.zk-retry-interval-ms=1000
yarn.resourcemanager.zk-state-store.parent-path=
    /rmstore:
yarn.resourcemanager.zk-timeout-ms=10000
yarn.scheduler.maximum-allocation-mb=8192
yarn.scheduler.maximum-allocation-vcores=32
yarn.scheduler.minimum-allocation-mb=1024
yarn.scheduler.minimum-allocation-vcores=1
yarn.sharedcache.admin.address=0.0.0.0:8047
yarn.sharedcache.admin.thread-count=1
yarn.sharedcache.app-checker.class=org.apache.hadoop.yarn.server.sharedcachemanager.RemoteAppChecker
yarn.sharedcache.checksum.algo.impl=org.apache.hadoop.yarn.sharedcache.ChecksumSHA256Impl
yarn.sharedcache.cleaner.initial-delay-mins=10
yarn.sharedcache.cleaner.period-mins=1440
yarn.sharedcache.cleaner.resource-sleep-ms=0
yarn.sharedcache.client-server.address=0.0.0.0:8045
yarn.sharedcache.client-server.thread-count=50
yarn.sharedcache.enabled=false
yarn.sharedcache.nested-level=3
yarn.sharedcache.nm.uploader.replication.factor=10
yarn.sharedcache.nm.uploader.thread-count=20
yarn.sharedcache.root-dir=/sharedcache
yarn.sharedcache.store.class=org.apache.hadoop.yarn.server.sharedcachemanager.store.InMemorySCMStore
yarn.sharedcache.store.in-memory.check-period-mins=720
yarn.sharedcache.store.in-memory.initial-delay-mins=10
yarn.sharedcache.store.in-memory.staleness-period-mins=10080
yarn.sharedcache.uploader.server.address=0.0.0.0:8046
yarn.sharedcache.uploader.server.thread-count=50
yarn.sharedcache.webapp.address=0.0.0.0:8788
yarn.timeline-service.address=0.0.0.0:10200
yarn.timeline-service.client.best-effort=false
yarn.timeline-service.client.max-retries=30
yarn.timeline-service.client.retry-interval-ms=1000
yarn.timeline-service.enabled=false
yarn.timeline-service.generic-application-history.max-applications=10000
yarn.timeline-service.handler-thread-count=10
yarn.timeline-service.hostname=0.0.0.0
yarn.timeline-service.http-authentication.simple.anonymous.allowed=true
yarn.timeline-service.http-authentication.type=simple
yarn.timeline-service.keytab=/etc/krb5.keytab
yarn.timeline-service.leveldb-state-store.path=
    /local_disk0/tmp/yarn/timeline:
yarn.timeline-service.leveldb-timeline-store.path=
    /local_disk0/tmp/yarn/timeline:
yarn.timeline-service.leveldb-timeline-store.read-cache-size=104857600
yarn.timeline-service.leveldb-timeline-store.start-time-read-cache-size=10000
yarn.timeline-service.leveldb-timeline-store.start-time-write-cache-size=10000
yarn.timeline-service.leveldb-timeline-store.ttl-interval-ms=300000
yarn.timeline-service.recovery.enabled=false
yarn.timeline-service.state-store-class=org.apache.hadoop.yarn.server.timeline.recovery.LeveldbTimelineStateStore
yarn.timeline-service.store-class=org.apache.hadoop.yarn.server.timeline.LeveldbTimelineStore
yarn.timeline-service.ttl-enable=true
yarn.timeline-service.ttl-ms=604800000
yarn.timeline-service.webapp.address=0.0.0.0:8188
yarn.timeline-service.webapp.https.address=0.0.0.0:8190
END========"new HiveConf()"========

19/10/16 18:30:50 INFO SQLAppStatusListener: Execution ID: 16 Total Executor Run Time: 0
19/10/16 18:30:50 INFO HiveMetaStore: 1: create_database: Database(name:rms_1000255_pqm, description:, locationUri:adl://connectedsystemsuatdl.azuredatalakestore.net/user/hive/warehouse/RMS_1000255_PQM.db, parameters:{})
19/10/16 18:30:50 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=create_database: Database(name:rms_1000255_pqm, description:, locationUri:adl://connectedsystemsuatdl.azuredatalakestore.net/user/hive/warehouse/RMS_1000255_PQM.db, parameters:{})	
19/10/16 18:30:50 WARN ObjectStore: Failed to get database rms_1000255_pqm, returning NoSuchObjectException
19/10/16 18:30:50 DEBUG HttpTransport: HTTPRequest,Failed,cReqId:679f1ad4-1866-4186-a75f-845148b39590.0,lat:7,err:HTTP404(FileNotFoundException),Reqlen:0,Resplen:259,token_ns:6800,sReqId:3978faae-ef8b-4d5a-9d45-91e89c033a75,path:/user/hive/warehouse/RMS_1000255_PQM.db,qp:op=GETFILESTATUS&tooid=true&api-version=2017-08-01
19/10/16 18:30:50 INFO FileUtils: Creating directory if it doesn't exist: adl://connectedsystemsuatdl.azuredatalakestore.net/user/hive/warehouse/RMS_1000255_PQM.db
19/10/16 18:30:50 INFO SQLAppStatusListener: Execution ID: 17 Total Executor Run Time: 0
19/10/16 18:30:50 INFO FileToDimension: FileToDimension Some(MarketCSV) for MRKT_DIM input file wasbs://teuatcontainer01@teuatstorage.blob.core.windows.net//RMS_1000255_PQM/MRKT_DIM.csv>>>>> Started
19/10/16 18:30:50 INFO InMemoryFileIndex: Start listing leaf files and directories. Size of Paths: 1; threshold: 32
19/10/16 18:30:50 INFO InMemoryFileIndex: Start listing leaf files and directories. Size of Paths: 0; threshold: 32
19/10/16 18:30:50 INFO InMemoryFileIndex: Start listing leaf files and directories. Size of Paths: 1; threshold: 32
19/10/16 18:30:50 INFO InMemoryFileIndex: Start listing leaf files and directories. Size of Paths: 0; threshold: 32
19/10/16 18:30:51 INFO FileSourceStrategy: Pruning directories with: 
19/10/16 18:30:51 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#34, None)) > 0)
19/10/16 18:30:51 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
19/10/16 18:30:51 INFO FileSourceScanExec: Pushed Filters: 
19/10/16 18:30:52 INFO CodeGenerator: Code generated in 327.070211 ms
19/10/16 18:30:52 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 649.6 KB, free 54.2 GB)
19/10/16 18:30:52 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 46.9 KB, free 54.2 GB)
19/10/16 18:30:52 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.139.64.182:46123 (size: 46.9 KB, free: 54.2 GB)
19/10/16 18:30:52 INFO SparkContext: Created broadcast 0 from csv at FileToDimension.scala:30
19/10/16 18:30:52 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
19/10/16 18:30:52 INFO SparkContext: Starting job: csv at FileToDimension.scala:30
19/10/16 18:30:52 INFO DAGScheduler: Got job 0 (csv at FileToDimension.scala:30) with 1 output partitions
19/10/16 18:30:52 INFO DAGScheduler: Final stage: ResultStage 0 (csv at FileToDimension.scala:30)
19/10/16 18:30:52 INFO DAGScheduler: Parents of final stage: List()
19/10/16 18:30:52 INFO DAGScheduler: Missing parents: List()
19/10/16 18:30:52 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at csv at FileToDimension.scala:30), which has no missing parents
19/10/16 18:30:52 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 14.1 KB, free 54.2 GB)
19/10/16 18:30:52 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.0 KB, free 54.2 GB)
19/10/16 18:30:52 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.139.64.182:46123 (size: 7.0 KB, free: 54.2 GB)
19/10/16 18:30:52 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1316
19/10/16 18:30:52 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at csv at FileToDimension.scala:30) (first 15 tasks are for partitions Vector(0))
19/10/16 18:30:52 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
19/10/16 18:30:52 INFO TaskSetManager: Jars for session None: Map(spark://10.139.64.182:33603/jars/addedFile5681071070702781128dbfs__FileStore_saak9001_transengine_1_0_SNAPSHOT-ab9b9.jar -> 1571250418761, spark://10.139.64.182:33603/jars/addedFile2365597673424539553dbfs__FileStore_saak9001_pqm_core_models_1_0-b0ccd.jar -> 1571250418581, spark://10.139.64.182:33603/jars/addedFile6883373321297042917org_rogach_scallop_2_11_3_3_1-ecc25.jar -> 1571250417966, spark://10.139.64.182:33603/jars/addedFile8624879695200027925xml_apis_xml_apis_1_0_b2-167e6.jar -> 1571250418245, spark://10.139.64.182:33603/jars/addedFile5162712074542881962com_typesafe_config_1_3_1-e3e42.jar -> 1571250418145, spark://10.139.64.182:33603/jars/addedFile6674954176252367935org_scalaj_scalaj_http_2_11_2_3_0-d69e4.jar -> 1571250418854, spark://10.139.64.182:33603/jars/addedFile236936972251169783dom4j_dom4j_1_6_1-3b35e.jar -> 1571250418356, spark://10.139.64.182:33603/jars/addedFile5541486089544592835io_spray_spray_json_2_11_1_3_5-d5f78.jar -> 1571250418683)
19/10/16 18:30:52 INFO TaskSetManager: Files for session None: Map(spark://10.139.64.182:33603/files/addedFile8624879695200027925xml_apis_xml_apis_1_0_b2-167e6.jar -> 1571250418235, spark://10.139.64.182:33603/files/addedFile5162712074542881962com_typesafe_config_1_3_1-e3e42.jar -> 1571250418138, spark://10.139.64.182:33603/files/addedFile2365597673424539553dbfs__FileStore_saak9001_pqm_core_models_1_0-b0ccd.jar -> 1571250418573, spark://10.139.64.182:33603/files/addedFile5681071070702781128dbfs__FileStore_saak9001_transengine_1_0_SNAPSHOT-ab9b9.jar -> 1571250418753, spark://10.139.64.182:33603/files/addedFile236936972251169783dom4j_dom4j_1_6_1-3b35e.jar -> 1571250418346, spark://10.139.64.182:33603/files/addedFile5541486089544592835io_spray_spray_json_2_11_1_3_5-d5f78.jar -> 1571250418676, spark://10.139.64.182:33603/files/addedFile6883373321297042917org_rogach_scallop_2_11_3_3_1-ecc25.jar -> 1571250417952, spark://10.139.64.182:33603/files/addedFile6674954176252367935org_scalaj_scalaj_http_2_11_2_3_0-d69e4.jar -> 1571250418848)
19/10/16 18:30:52 WARN FairSchedulableBuilder: A job was submitted with scheduler pool 2598616896940891466, which has not been configured. This can happen when the file that pools are read from isn't set, or when that file doesn't contain 2598616896940891466. Created 2598616896940891466 with default configuration (schedulingMode: FIFO, minShare: 0, weight: 1)
19/10/16 18:30:52 INFO FairSchedulableBuilder: Added task set TaskSet_0.0 tasks to pool 2598616896940891466
19/10/16 18:30:52 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, 10.139.64.183, executor 0, partition 0, PROCESS_LOCAL, 6056 bytes)
19/10/16 18:30:53 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.139.64.183:34813 (size: 7.0 KB, free: 50.8 GB)
19/10/16 18:30:54 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.139.64.183:34813 (size: 46.9 KB, free: 50.8 GB)
19/10/16 18:30:55 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2654 ms on 10.139.64.183 (executor 0) (1/1)
19/10/16 18:30:55 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 2598616896940891466
19/10/16 18:30:55 INFO DAGScheduler: ResultStage 0 (csv at FileToDimension.scala:30) finished in 2.791 s
19/10/16 18:30:55 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
19/10/16 18:30:55 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
19/10/16 18:30:55 INFO DAGScheduler: Job 0 finished: csv at FileToDimension.scala:30, took 2.877808 s
19/10/16 18:30:55 INFO CodeGenerator: Code generated in 25.800979 ms
19/10/16 18:30:55 INFO FileSourceStrategy: Pruning directories with: 
19/10/16 18:30:55 INFO FileSourceStrategy: Post-Scan Filters: 
19/10/16 18:30:55 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
19/10/16 18:30:55 INFO FileSourceScanExec: Pushed Filters: 
19/10/16 18:30:55 INFO SQLAppStatusListener: Execution ID: 18 Total Executor Run Time: 1866
19/10/16 18:30:55 INFO CodeGenerator: Code generated in 50.407247 ms
19/10/16 18:30:55 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 649.6 KB, free 54.2 GB)
19/10/16 18:30:55 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 46.9 KB, free 54.2 GB)
19/10/16 18:30:55 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.139.64.182:46123 (size: 46.9 KB, free: 54.2 GB)
19/10/16 18:30:55 INFO SparkContext: Created broadcast 2 from csv at FileToDimension.scala:30
19/10/16 18:30:55 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
19/10/16 18:30:55 INFO SparkContext: Starting job: csv at FileToDimension.scala:30
19/10/16 18:30:55 INFO DAGScheduler: Got job 1 (csv at FileToDimension.scala:30) with 1 output partitions
19/10/16 18:30:55 INFO DAGScheduler: Final stage: ResultStage 1 (csv at FileToDimension.scala:30)
19/10/16 18:30:55 INFO DAGScheduler: Parents of final stage: List()
19/10/16 18:30:55 INFO DAGScheduler: Missing parents: List()
19/10/16 18:30:55 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at csv at FileToDimension.scala:30), which has no missing parents
19/10/16 18:30:55 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 18.0 KB, free 54.2 GB)
19/10/16 18:30:55 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 9.0 KB, free 54.2 GB)
19/10/16 18:30:55 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.139.64.182:46123 (size: 9.0 KB, free: 54.2 GB)
19/10/16 18:30:55 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1316
19/10/16 18:30:55 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at csv at FileToDimension.scala:30) (first 15 tasks are for partitions Vector(0))
19/10/16 18:30:55 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks
19/10/16 18:30:55 INFO TaskSetManager: Jars for session None: Map(spark://10.139.64.182:33603/jars/addedFile5681071070702781128dbfs__FileStore_saak9001_transengine_1_0_SNAPSHOT-ab9b9.jar -> 1571250418761, spark://10.139.64.182:33603/jars/addedFile2365597673424539553dbfs__FileStore_saak9001_pqm_core_models_1_0-b0ccd.jar -> 1571250418581, spark://10.139.64.182:33603/jars/addedFile6883373321297042917org_rogach_scallop_2_11_3_3_1-ecc25.jar -> 1571250417966, spark://10.139.64.182:33603/jars/addedFile8624879695200027925xml_apis_xml_apis_1_0_b2-167e6.jar -> 1571250418245, spark://10.139.64.182:33603/jars/addedFile5162712074542881962com_typesafe_config_1_3_1-e3e42.jar -> 1571250418145, spark://10.139.64.182:33603/jars/addedFile6674954176252367935org_scalaj_scalaj_http_2_11_2_3_0-d69e4.jar -> 1571250418854, spark://10.139.64.182:33603/jars/addedFile236936972251169783dom4j_dom4j_1_6_1-3b35e.jar -> 1571250418356, spark://10.139.64.182:33603/jars/addedFile5541486089544592835io_spray_spray_json_2_11_1_3_5-d5f78.jar -> 1571250418683)
19/10/16 18:30:55 INFO TaskSetManager: Files for session None: Map(spark://10.139.64.182:33603/files/addedFile8624879695200027925xml_apis_xml_apis_1_0_b2-167e6.jar -> 1571250418235, spark://10.139.64.182:33603/files/addedFile5162712074542881962com_typesafe_config_1_3_1-e3e42.jar -> 1571250418138, spark://10.139.64.182:33603/files/addedFile2365597673424539553dbfs__FileStore_saak9001_pqm_core_models_1_0-b0ccd.jar -> 1571250418573, spark://10.139.64.182:33603/files/addedFile5681071070702781128dbfs__FileStore_saak9001_transengine_1_0_SNAPSHOT-ab9b9.jar -> 1571250418753, spark://10.139.64.182:33603/files/addedFile236936972251169783dom4j_dom4j_1_6_1-3b35e.jar -> 1571250418346, spark://10.139.64.182:33603/files/addedFile5541486089544592835io_spray_spray_json_2_11_1_3_5-d5f78.jar -> 1571250418676, spark://10.139.64.182:33603/files/addedFile6883373321297042917org_rogach_scallop_2_11_3_3_1-ecc25.jar -> 1571250417952, spark://10.139.64.182:33603/files/addedFile6674954176252367935org_scalaj_scalaj_http_2_11_2_3_0-d69e4.jar -> 1571250418848)
19/10/16 18:30:55 INFO FairSchedulableBuilder: Added task set TaskSet_1.0 tasks to pool 2598616896940891466
19/10/16 18:30:55 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, 10.139.64.183, executor 0, partition 0, PROCESS_LOCAL, 6056 bytes)
19/10/16 18:30:55 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.139.64.183:34813 (size: 9.0 KB, free: 50.8 GB)
19/10/16 18:30:57 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.139.64.183:34813 (size: 46.9 KB, free: 50.8 GB)
19/10/16 18:30:57 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 2066 ms on 10.139.64.183 (executor 0) (1/1)
19/10/16 18:30:57 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 2598616896940891466
19/10/16 18:30:57 INFO DAGScheduler: ResultStage 1 (csv at FileToDimension.scala:30) finished in 2.077 s
19/10/16 18:30:57 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
19/10/16 18:30:57 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
19/10/16 18:30:57 INFO DAGScheduler: Job 1 finished: csv at FileToDimension.scala:30, took 2.085815 s
19/10/16 18:30:57 INFO FileToDimension: >>>>>>>>>> Writing table RMS_1000255_PQM.MRKT_DIM Started.
19/10/16 18:30:57 INFO HiveMetaStore: 1: get_table : db=rms_1000255_pqm tbl=mrkt_dim
19/10/16 18:30:57 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=rms_1000255_pqm tbl=mrkt_dim	
19/10/16 18:30:57 WARN Utils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.debug.maxToStringFields' in SparkEnv.conf.
19/10/16 18:30:57 INFO FileSourceStrategy: Pruning directories with: 
19/10/16 18:30:57 INFO FileSourceStrategy: Post-Scan Filters: 
19/10/16 18:30:57 INFO FileSourceStrategy: Output Data Schema: struct<CLIENT_ID: int, MRKT_DSC_SHRT: string, MRKT_DSC_LNG: string, SAMS_RSTR_IND: int, CSL_BLK_DT: timestamp ... 62 more fields>
19/10/16 18:30:57 INFO FileSourceScanExec: Pushed Filters: 
19/10/16 18:30:57 INFO HiveMetaStore: 1: get_table : db=rms_1000255_pqm tbl=mrkt_dim
19/10/16 18:30:57 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=rms_1000255_pqm tbl=mrkt_dim	
19/10/16 18:30:57 INFO HiveMetaStore: 1: get_database: rms_1000255_pqm
19/10/16 18:30:57 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: rms_1000255_pqm	
19/10/16 18:30:57 INFO HiveMetaStore: 1: get_database: rms_1000255_pqm
19/10/16 18:30:57 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: rms_1000255_pqm	
19/10/16 18:30:57 DEBUG HttpTransport: HTTPRequest,Failed,cReqId:625b99f9-ab13-4222-a905-0cd47f56386d.0,lat:26,err:HTTP404(FileNotFoundException),Reqlen:0,Resplen:268,token_ns:4900,sReqId:f78cf0d3-c161-4fea-88e2-2ca577adcc13,path:/user/hive/warehouse/RMS_1000255_PQM.db/mrkt_dim,qp:op=GETFILESTATUS&tooid=true&api-version=2017-08-01
19/10/16 18:30:57 INFO HiveMetaStore: 1: get_database: rms_1000255_pqm
19/10/16 18:30:57 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: rms_1000255_pqm	
19/10/16 18:30:57 INFO HiveMetaStore: 1: get_database: rms_1000255_pqm
19/10/16 18:30:57 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: rms_1000255_pqm	
19/10/16 18:30:57 DEBUG HttpTransport: HTTPRequest,Failed,cReqId:013c85ee-5ce5-420a-b719-97138afab6c1.0,lat:17,err:HTTP404(FileNotFoundException),Reqlen:0,Resplen:268,token_ns:7100,sReqId:342c3846-30a2-4337-938e-84f8024dfd55,path:/user/hive/warehouse/RMS_1000255_PQM.db/mrkt_dim,qp:op=GETFILESTATUS&tooid=true&api-version=2017-08-01
19/10/16 18:30:58 INFO ParquetFileFormat: Using user defined output committer for Parquet: org.apache.spark.sql.parquet.DirectParquetOutputCommitter
19/10/16 18:30:58 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 649.5 KB, free 54.2 GB)
19/10/16 18:30:58 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 46.8 KB, free 54.2 GB)
19/10/16 18:30:58 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.139.64.182:46123 (size: 46.8 KB, free: 54.2 GB)
19/10/16 18:30:58 INFO SparkContext: Created broadcast 4 from saveAsTable at DataFrameDecorator.scala:17
19/10/16 18:30:58 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
19/10/16 18:30:58 INFO SparkContext: Starting job: saveAsTable at DataFrameDecorator.scala:17
19/10/16 18:30:58 INFO DAGScheduler: Got job 2 (saveAsTable at DataFrameDecorator.scala:17) with 1 output partitions
19/10/16 18:30:58 INFO DAGScheduler: Final stage: ResultStage 2 (saveAsTable at DataFrameDecorator.scala:17)
19/10/16 18:30:58 INFO DAGScheduler: Parents of final stage: List()
19/10/16 18:30:58 INFO DAGScheduler: Missing parents: List()
19/10/16 18:30:58 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[8] at saveAsTable at DataFrameDecorator.scala:17), which has no missing parents
19/10/16 18:30:58 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 325.1 KB, free 54.2 GB)
19/10/16 18:30:58 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 106.6 KB, free 54.2 GB)
19/10/16 18:30:58 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 10.139.64.182:46123 (size: 106.6 KB, free: 54.2 GB)
19/10/16 18:30:58 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1316
19/10/16 18:30:58 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[8] at saveAsTable at DataFrameDecorator.scala:17) (first 15 tasks are for partitions Vector(0))
19/10/16 18:30:58 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks
19/10/16 18:30:58 INFO TaskSetManager: Jars for session None: Map(spark://10.139.64.182:33603/jars/addedFile5681071070702781128dbfs__FileStore_saak9001_transengine_1_0_SNAPSHOT-ab9b9.jar -> 1571250418761, spark://10.139.64.182:33603/jars/addedFile2365597673424539553dbfs__FileStore_saak9001_pqm_core_models_1_0-b0ccd.jar -> 1571250418581, spark://10.139.64.182:33603/jars/addedFile6883373321297042917org_rogach_scallop_2_11_3_3_1-ecc25.jar -> 1571250417966, spark://10.139.64.182:33603/jars/addedFile8624879695200027925xml_apis_xml_apis_1_0_b2-167e6.jar -> 1571250418245, spark://10.139.64.182:33603/jars/addedFile5162712074542881962com_typesafe_config_1_3_1-e3e42.jar -> 1571250418145, spark://10.139.64.182:33603/jars/addedFile6674954176252367935org_scalaj_scalaj_http_2_11_2_3_0-d69e4.jar -> 1571250418854, spark://10.139.64.182:33603/jars/addedFile236936972251169783dom4j_dom4j_1_6_1-3b35e.jar -> 1571250418356, spark://10.139.64.182:33603/jars/addedFile5541486089544592835io_spray_spray_json_2_11_1_3_5-d5f78.jar -> 1571250418683)
19/10/16 18:30:58 INFO TaskSetManager: Files for session None: Map(spark://10.139.64.182:33603/files/addedFile8624879695200027925xml_apis_xml_apis_1_0_b2-167e6.jar -> 1571250418235, spark://10.139.64.182:33603/files/addedFile5162712074542881962com_typesafe_config_1_3_1-e3e42.jar -> 1571250418138, spark://10.139.64.182:33603/files/addedFile2365597673424539553dbfs__FileStore_saak9001_pqm_core_models_1_0-b0ccd.jar -> 1571250418573, spark://10.139.64.182:33603/files/addedFile5681071070702781128dbfs__FileStore_saak9001_transengine_1_0_SNAPSHOT-ab9b9.jar -> 1571250418753, spark://10.139.64.182:33603/files/addedFile236936972251169783dom4j_dom4j_1_6_1-3b35e.jar -> 1571250418346, spark://10.139.64.182:33603/files/addedFile5541486089544592835io_spray_spray_json_2_11_1_3_5-d5f78.jar -> 1571250418676, spark://10.139.64.182:33603/files/addedFile6883373321297042917org_rogach_scallop_2_11_3_3_1-ecc25.jar -> 1571250417952, spark://10.139.64.182:33603/files/addedFile6674954176252367935org_scalaj_scalaj_http_2_11_2_3_0-d69e4.jar -> 1571250418848)
19/10/16 18:30:58 INFO FairSchedulableBuilder: Added task set TaskSet_2.0 tasks to pool 2598616896940891466
19/10/16 18:30:58 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, 10.139.64.183, executor 0, partition 0, PROCESS_LOCAL, 6056 bytes)
19/10/16 18:30:58 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 10.139.64.183:34813 (size: 106.6 KB, free: 50.8 GB)
19/10/16 18:30:59 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.139.64.183:34813 (size: 46.8 KB, free: 50.8 GB)
19/10/16 18:31:00 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 2545 ms on 10.139.64.183 (executor 0) (1/1)
19/10/16 18:31:00 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 2598616896940891466
19/10/16 18:31:00 INFO DAGScheduler: ResultStage 2 (saveAsTable at DataFrameDecorator.scala:17) finished in 2.581 s
19/10/16 18:31:00 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
19/10/16 18:31:00 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
19/10/16 18:31:00 INFO DAGScheduler: Job 2 finished: saveAsTable at DataFrameDecorator.scala:17, took 2.596116 s
19/10/16 18:31:00 INFO DirectoryAtomicCommitProtocol: Committing job 08fba3a2-ebe5-4481-944f-e969ba5cc8eb
19/10/16 18:31:00 INFO FileSystem: FS_OP_CREATE FILE[adl://connectedsystemsuatdl.azuredatalakestore.net/user/hive/warehouse/RMS_1000255_PQM.db/mrkt_dim/_committed_824934570570732327] Creating output stream; permission: rw-r--r--, overwrite: true, bufferSize: 65536
19/10/16 18:31:00 INFO AdlFsOutputStream: FS_OP_CREATE FILE[/user/hive/warehouse/RMS_1000255_PQM.db/mrkt_dim/_committed_824934570570732327] Closing stream; size: 119
19/10/16 18:31:00 INFO AdlFsOutputStream: FS_OP_CREATE FILE[/user/hive/warehouse/RMS_1000255_PQM.db/mrkt_dim/_committed_824934570570732327] Upload complete; size: 119
19/10/16 18:31:00 INFO AdlFsOutputStream: FS_OP_CREATE FILE[/user/hive/warehouse/RMS_1000255_PQM.db/mrkt_dim/_committed_824934570570732327] Closing stream; size: 119
19/10/16 18:31:00 INFO AdlFsOutputStream: FS_OP_CREATE FILE[/user/hive/warehouse/RMS_1000255_PQM.db/mrkt_dim/_committed_824934570570732327] Upload complete; size: 119
19/10/16 18:31:00 INFO FileSystem: FS_OP_CREATE FILE[adl://connectedsystemsuatdl.azuredatalakestore.net/user/hive/warehouse/RMS_1000255_PQM.db/mrkt_dim/_SUCCESS] Creating output stream; permission: rw-r--r--, overwrite: true, bufferSize: 65536
19/10/16 18:31:01 INFO AdlFsOutputStream: FS_OP_CREATE FILE[/user/hive/warehouse/RMS_1000255_PQM.db/mrkt_dim/_SUCCESS] Closing stream; size: 0
19/10/16 18:31:01 INFO AdlFsOutputStream: FS_OP_CREATE FILE[/user/hive/warehouse/RMS_1000255_PQM.db/mrkt_dim/_SUCCESS] Upload complete; size: 0
19/10/16 18:31:01 INFO FileSystem: FS_OP_CREATE FILE[adl://connectedsystemsuatdl.azuredatalakestore.net/user/hive/warehouse/RMS_1000255_PQM.db/mrkt_dim/_SUCCESS] Creating output stream; permission: rw-r--r--, overwrite: true, bufferSize: 65536
19/10/16 18:31:01 INFO AdlFsOutputStream: FS_OP_CREATE FILE[/user/hive/warehouse/RMS_1000255_PQM.db/mrkt_dim/_SUCCESS] Closing stream; size: 0
19/10/16 18:31:01 INFO AdlFsOutputStream: FS_OP_CREATE FILE[/user/hive/warehouse/RMS_1000255_PQM.db/mrkt_dim/_SUCCESS] Upload complete; size: 0
19/10/16 18:31:01 INFO DirectoryAtomicCommitProtocol: Job commit completed for 08fba3a2-ebe5-4481-944f-e969ba5cc8eb
19/10/16 18:31:01 INFO DirectoryAtomicCommitProtocol: Auto-vacuuming directories updated by 08fba3a2-ebe5-4481-944f-e969ba5cc8eb
19/10/16 18:31:01 INFO DirectoryAtomicCommitProtocol: Started VACUUM on List(adl://connectedsystemsuatdl.azuredatalakestore.net/user/hive/warehouse/RMS_1000255_PQM.db/mrkt_dim) with data horizon 1571077861205 (now - 48.0 hours), metadata horizon 1571248861205 (now - 0.5 hours)
19/10/16 18:31:01 WARN FileOperations: method deleteMultiple doesn't exist. Falling back to individual deletion.
19/10/16 18:31:01 INFO FileFormatWriter: Write Job ea35da7a-39a0-4191-bb48-d0b7fde8f737 committed.
19/10/16 18:31:01 INFO FileFormatWriter: Finished processing stats for write job ea35da7a-39a0-4191-bb48-d0b7fde8f737.
19/10/16 18:31:01 DEBUG HttpTransport: HTTPRequest,Failed,cReqId:b0bff525-e0f0-4324-bfc0-8a96360b13b5.0,lat:7,err:HTTP404(FileNotFoundException),Reqlen:0,Resplen:284,token_ns:4800,sReqId:e185e3a4-b174-414f-89d1-f5d447fe24d2,path:/user/hive/warehouse/RMS_1000255_PQM.db/mrkt_dim/_spark_metadata,qp:op=GETFILESTATUS&tooid=true&api-version=2017-08-01
19/10/16 18:31:01 INFO InMemoryFileIndex: Start listing leaf files and directories. Size of Paths: 1; threshold: 32
19/10/16 18:31:01 INFO InMemoryFileIndex: Start listing leaf files and directories. Size of Paths: 0; threshold: 32
19/10/16 18:31:01 INFO HiveMetaStore: 1: get_database: rms_1000255_pqm
19/10/16 18:31:01 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: rms_1000255_pqm	
19/10/16 18:31:01 INFO HiveMetaStore: 1: get_table : db=rms_1000255_pqm tbl=mrkt_dim
19/10/16 18:31:01 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=rms_1000255_pqm tbl=mrkt_dim	
19/10/16 18:31:01 INFO HiveMetaStore: 1: get_database: rms_1000255_pqm
19/10/16 18:31:01 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: rms_1000255_pqm	
19/10/16 18:31:01 INFO HiveMetaStore: 1: get_table : db=rms_1000255_pqm tbl=mrkt_dim
19/10/16 18:31:01 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=rms_1000255_pqm tbl=mrkt_dim	
19/10/16 18:31:01 INFO SecuredHiveExternalCatalog: Persisting file based data source table `rms_1000255_pqm`.`mrkt_dim` into Hive metastore in Hive compatible format.
19/10/16 18:31:01 INFO HiveMetaStore: 1: create_table: Table(tableName:mrkt_dim, dbName:rms_1000255_pqm, owner:root, createTime:1571250657, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:CLIENT_ID, type:int, comment:null), FieldSchema(name:MRKT_DSC_SHRT, type:string, comment:null), FieldSchema(name:MRKT_DSC_LNG, type:string, comment:null), FieldSchema(name:SAMS_RSTR_IND, type:int, comment:null), FieldSchema(name:CSL_BLK_DT, type:timestamp, comment:null), FieldSchema(name:POD_RLS_IND, type:string, comment:null), FieldSchema(name:PRJCTD_IND, type:int, comment:null), FieldSchema(name:MRKT_TYP, type:string, comment:null), FieldSchema(name:MRKT_RPTBL_STRT_DT, type:timestamp, comment:null), FieldSchema(name:MRKT_RPTBL_STOP_DT, type:timestamp, comment:null), FieldSchema(name:TAGRS, type:string, comment:null), FieldSchema(name:MRKT_CD, type:string, comment:null), FieldSchema(name:ACV_ADJ_IND, type:int, comment:null), FieldSchema(name:MKT_DISP_NAME, type:string, comment:null), FieldSchema(name:MRKT_DRILL_IND, type:string, comment:null), FieldSchema(name:STR_PL_RSTR, type:int, comment:null), FieldSchema(name:WGHT_RULE_NUM, type:int, comment:null), FieldSchema(name:XDIM_MRKT2STR, type:string, comment:null), FieldSchema(name:XAOC_COMP_MKT_KEY, type:int, comment:null), FieldSchema(name:COMP_MKT_KEY, type:int, comment:null), FieldSchema(name:MRKT_KEY, type:int, comment:null), FieldSchema(name:COMP_MKT, type:string, comment:null), FieldSchema(name:XAOC_COMP_MKT, type:string, comment:null), FieldSchema(name:MARKET_TYPE, type:string, comment:null), FieldSchema(name:MRKT_RSTR_ID, type:int, comment:null), FieldSchema(name:MRKT_RSTR_POS, type:int, comment:null), FieldSchema(name:MRKT_IND, type:string, comment:null), FieldSchema(name:CHANNEL, type:string, comment:null), FieldSchema(name:SHR_MRKT_KEY, type:string, comment:null), FieldSchema(name:ETH_MKT_TYP, type:string, comment:null), FieldSchema(name:MRKT_AGG_FLG, type:int, comment:null), FieldSchema(name:DT_SRC_DTL_NM, type:string, comment:null), FieldSchema(name:SUPUP, type:string, comment:null), FieldSchema(name:COUNTRY, type:string, comment:null), FieldSchema(name:CS_MARKET_TYPE, type:string, comment:null), FieldSchema(name:CS_MARKET_SET, type:string, comment:null), FieldSchema(name:SUPPRESSION, type:string, comment:null), FieldSchema(name:DEF_MRKT_FLG, type:string, comment:null), FieldSchema(name:GEOGRAPHY, type:string, comment:null), FieldSchema(name:CS_CHANNEL, type:string, comment:null), FieldSchema(name:CS_MARKET_SET_DETAIL, type:string, comment:null), FieldSchema(name:CS_MARKET_TYPE_SET, type:string, comment:null), FieldSchema(name:CS_TA_CHANNEL, type:string, comment:null), FieldSchema(name:MRKT_TT_DMG_CD, type:int, comment:null), FieldSchema(name:MRKT_TYPE_HS, type:string, comment:null), FieldSchema(name:TA_REM, type:string, comment:null), FieldSchema(name:XAOC_TA_REM, type:string, comment:null), FieldSchema(name:KO_MARKET_PARENT_CHANNEL, type:string, comment:null), FieldSchema(name:KO_MARKET_PARENT_1, type:string, comment:null), FieldSchema(name:KO_MARKET_PARENT_2, type:string, comment:null), FieldSchema(name:KO_MARKET_PARENT_3, type:string, comment:null), FieldSchema(name:KO_MARKET_PARENT_4, type:string, comment:null), FieldSchema(name:KO_MARKET_LEVEL_CHANNEL, type:string, comment:null), FieldSchema(name:KO_MARKET_LEVEL_1, type:string, comment:null), FieldSchema(name:KO_MARKET_LEVEL_2, type:string, comment:null), FieldSchema(name:KO_MARKET_LEVEL_3, type:string, comment:null), FieldSchema(name:KO_MARKET_LEVEL_4, type:string, comment:null), FieldSchema(name:KO_MARKET_ALTERNATE_MKTS, type:string, comment:null), FieldSchema(name:KO_MARKET_ALL_CHILD_MKTS, type:string, comment:null), FieldSchema(name:CM_TA_RM_ALL, type:string, comment:null), FieldSchema(name:XAOC_CM_TA_RM_ALL, type:string, comment:null), FieldSchema(name:ODMP_DISP_NAME, type:string, comment:null), FieldSchema(name:MARKET_DISPLAY_NAME_KO, type:string, comment:null), FieldSchema(name:DYN_MRKT_FLG, type:string, comment:null)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, parameters:{path=adl://connectedsystemsuatdl.azuredatalakestore.net/user/hive/warehouse/RMS_1000255_PQM.db/mrkt_dim, serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{spark.sql.sources.schema.part.1=ta":{}},{"name":"KO_MARKET_LEVEL_4","type":"string","nullable":true,"metadata":{}},{"name":"KO_MARKET_ALTERNATE_MKTS","type":"string","nullable":true,"metadata":{}},{"name":"KO_MARKET_ALL_CHILD_MKTS","type":"string","nullable":true,"metadata":{}},{"name":"CM_TA_RM_ALL","type":"string","nullable":true,"metadata":{}},{"name":"XAOC_CM_TA_RM_ALL","type":"string","nullable":true,"metadata":{}},{"name":"ODMP_DISP_NAME","type":"string","nullable":true,"metadata":{}},{"name":"MARKET_DISPLAY_NAME_KO","type":"string","nullable":true,"metadata":{}},{"name":"DYN_MRKT_FLG","type":"string","nullable":true,"metadata":{}}]}, spark.sql.sources.schema.part.0={"type":"struct","fields":[{"name":"CLIENT_ID","type":"integer","nullable":true,"metadata":{}},{"name":"MRKT_DSC_SHRT","type":"string","nullable":true,"metadata":{}},{"name":"MRKT_DSC_LNG","type":"string","nullable":true,"metadata":{}},{"name":"SAMS_RSTR_IND","type":"integer","nullable":true,"metadata":{}},{"name":"CSL_BLK_DT","type":"timestamp","nullable":true,"metadata":{}},{"name":"POD_RLS_IND","type":"string","nullable":true,"metadata":{}},{"name":"PRJCTD_IND","type":"integer","nullable":true,"metadata":{}},{"name":"MRKT_TYP","type":"string","nullable":true,"metadata":{}},{"name":"MRKT_RPTBL_STRT_DT","type":"timestamp","nullable":true,"metadata":{}},{"name":"MRKT_RPTBL_STOP_DT","type":"timestamp","nullable":true,"metadata":{}},{"name":"TAGRS","type":"string","nullable":true,"metadata":{}},{"name":"MRKT_CD","type":"string","nullable":true,"metadata":{}},{"name":"ACV_ADJ_IND","type":"integer","nullable":true,"metadata":{}},{"name":"MKT_DISP_NAME","type":"string","nullable":true,"metadata":{}},{"name":"MRKT_DRILL_IND","type":"string","nullable":true,"metadata":{}},{"name":"STR_PL_RSTR","type":"integer","nullable":true,"metadata":{}},{"name":"WGHT_RULE_NUM","type":"integer","nullable":true,"metadata":{}},{"name":"XDIM_MRKT2STR","type":"string","nullable":true,"metadata":{}},{"name":"XAOC_COMP_MKT_KEY","type":"integer","nullable":true,"metadata":{}},{"name":"COMP_MKT_KEY","type":"integer","nullable":true,"metadata":{}},{"name":"MRKT_KEY","type":"integer","nullable":true,"metadata":{}},{"name":"COMP_MKT","type":"string","nullable":true,"metadata":{}},{"name":"XAOC_COMP_MKT","type":"string","nullable":true,"metadata":{}},{"name":"MARKET_TYPE","type":"string","nullable":true,"metadata":{}},{"name":"MRKT_RSTR_ID","type":"integer","nullable":true,"metadata":{}},{"name":"MRKT_RSTR_POS","type":"integer","nullable":true,"metadata":{}},{"name":"MRKT_IND","type":"string","nullable":true,"metadata":{}},{"name":"CHANNEL","type":"string","nullable":true,"metadata":{}},{"name":"SHR_MRKT_KEY","type":"string","nullable":true,"metadata":{}},{"name":"ETH_MKT_TYP","type":"string","nullable":true,"metadata":{}},{"name":"MRKT_AGG_FLG","type":"integer","nullable":true,"metadata":{}},{"name":"DT_SRC_DTL_NM","type":"string","nullable":true,"metadata":{}},{"name":"SUPUP","type":"string","nullable":true,"metadata":{}},{"name":"COUNTRY","type":"string","nullable":true,"metadata":{}},{"name":"CS_MARKET_TYPE","type":"string","nullable":true,"metadata":{}},{"name":"CS_MARKET_SET","type":"string","nullable":true,"metadata":{}},{"name":"SUPPRESSION","type":"string","nullable":true,"metadata":{}},{"name":"DEF_MRKT_FLG","type":"string","nullable":true,"metadata":{}},{"name":"GEOGRAPHY","type":"string","nullable":true,"metadata":{}},{"name":"CS_CHANNEL","type":"string","nullable":true,"metadata":{}},{"name":"CS_MARKET_SET_DETAIL","type":"string","nullable":true,"metadata":{}},{"name":"CS_MARKET_TYPE_SET","type":"string","nullable":true,"metadata":{}},{"name":"CS_TA_CHANNEL","type":"string","nullable":true,"metadata":{}},{"name":"MRKT_TT_DMG_CD","type":"integer","nullable":true,"metadata":{}},{"name":"MRKT_TYPE_HS","type":"string","nullable":true,"metadata":{}},{"name":"TA_REM","type":"string","nullable":true,"metadata":{}},{"name":"XAOC_TA_REM","type":"string","nullable":true,"metadata":{}},{"name":"KO_MARKET_PARENT_CHANNEL","type":"string","nullable":true,"metadata":{}},{"name":"KO_MARKET_PARENT_1","type":"string","nullable":true,"metadata":{}},{"name":"KO_MARKET_PARENT_2","type":"string","nullable":true,"metadata":{}},{"name":"KO_MARKET_PARENT_3","type":"string","nullable":true,"metadata":{}},{"name":"KO_MARKET_PARENT_4","type":"string","nullable":true,"metadata":{}},{"name":"KO_MARKET_LEVEL_CHANNEL","type":"string","nullable":true,"metadata":{}},{"name":"KO_MARKET_LEVEL_1","type":"string","nullable":true,"metadata":{}},{"name":"KO_MARKET_LEVEL_2","type":"string","nullable":true,"metadata":{}},{"name":"KO_MARKET_LEVEL_3","type":"string","nullable":true,"metada, spark.sql.sources.schema.numParts=2, spark.sql.sources.provider=parquet, spark.sql.create.version=2.4.0}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{root=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:root, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:root, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:root, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:root, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null))
19/10/16 18:31:01 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=create_table: Table(tableName:mrkt_dim, dbName:rms_1000255_pqm, owner:root, createTime:1571250657, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:CLIENT_ID, type:int, comment:null), FieldSchema(name:MRKT_DSC_SHRT, type:string, comment:null), FieldSchema(name:MRKT_DSC_LNG, type:string, comment:null), FieldSchema(name:SAMS_RSTR_IND, type:int, comment:null), FieldSchema(name:CSL_BLK_DT, type:timestamp, comment:null), FieldSchema(name:POD_RLS_IND, type:string, comment:null), FieldSchema(name:PRJCTD_IND, type:int, comment:null), FieldSchema(name:MRKT_TYP, type:string, comment:null), FieldSchema(name:MRKT_RPTBL_STRT_DT, type:timestamp, comment:null), FieldSchema(name:MRKT_RPTBL_STOP_DT, type:timestamp, comment:null), FieldSchema(name:TAGRS, type:string, comment:null), FieldSchema(name:MRKT_CD, type:string, comment:null), FieldSchema(name:ACV_ADJ_IND, type:int, comment:null), FieldSchema(name:MKT_DISP_NAME, type:string, comment:null), FieldSchema(name:MRKT_DRILL_IND, type:string, comment:null), FieldSchema(name:STR_PL_RSTR, type:int, comment:null), FieldSchema(name:WGHT_RULE_NUM, type:int, comment:null), FieldSchema(name:XDIM_MRKT2STR, type:string, comment:null), FieldSchema(name:XAOC_COMP_MKT_KEY, type:int, comment:null), FieldSchema(name:COMP_MKT_KEY, type:int, comment:null), FieldSchema(name:MRKT_KEY, type:int, comment:null), FieldSchema(name:COMP_MKT, type:string, comment:null), FieldSchema(name:XAOC_COMP_MKT, type:string, comment:null), FieldSchema(name:MARKET_TYPE, type:string, comment:null), FieldSchema(name:MRKT_RSTR_ID, type:int, comment:null), FieldSchema(name:MRKT_RSTR_POS, type:int, comment:null), FieldSchema(name:MRKT_IND, type:string, comment:null), FieldSchema(name:CHANNEL, type:string, comment:null), FieldSchema(name:SHR_MRKT_KEY, type:string, comment:null), FieldSchema(name:ETH_MKT_TYP, type:string, comment:null), FieldSchema(name:MRKT_AGG_FLG, type:int, comment:null), FieldSchema(name:DT_SRC_DTL_NM, type:string, comment:null), FieldSchema(name:SUPUP, type:string, comment:null), FieldSchema(name:COUNTRY, type:string, comment:null), FieldSchema(name:CS_MARKET_TYPE, type:string, comment:null), FieldSchema(name:CS_MARKET_SET, type:string, comment:null), FieldSchema(name:SUPPRESSION, type:string, comment:null), FieldSchema(name:DEF_MRKT_FLG, type:string, comment:null), FieldSchema(name:GEOGRAPHY, type:string, comment:null), FieldSchema(name:CS_CHANNEL, type:string, comment:null), FieldSchema(name:CS_MARKET_SET_DETAIL, type:string, comment:null), FieldSchema(name:CS_MARKET_TYPE_SET, type:string, comment:null), FieldSchema(name:CS_TA_CHANNEL, type:string, comment:null), FieldSchema(name:MRKT_TT_DMG_CD, type:int, comment:null), FieldSchema(name:MRKT_TYPE_HS, type:string, comment:null), FieldSchema(name:TA_REM, type:string, comment:null), FieldSchema(name:XAOC_TA_REM, type:string, comment:null), FieldSchema(name:KO_MARKET_PARENT_CHANNEL, type:string, comment:null), FieldSchema(name:KO_MARKET_PARENT_1, type:string, comment:null), FieldSchema(name:KO_MARKET_PARENT_2, type:string, comment:null), FieldSchema(name:KO_MARKET_PARENT_3, type:string, comment:null), FieldSchema(name:KO_MARKET_PARENT_4, type:string, comment:null), FieldSchema(name:KO_MARKET_LEVEL_CHANNEL, type:string, comment:null), FieldSchema(name:KO_MARKET_LEVEL_1, type:string, comment:null), FieldSchema(name:KO_MARKET_LEVEL_2, type:string, comment:null), FieldSchema(name:KO_MARKET_LEVEL_3, type:string, comment:null), FieldSchema(name:KO_MARKET_LEVEL_4, type:string, comment:null), FieldSchema(name:KO_MARKET_ALTERNATE_MKTS, type:string, comment:null), FieldSchema(name:KO_MARKET_ALL_CHILD_MKTS, type:string, comment:null), FieldSchema(name:CM_TA_RM_ALL, type:string, comment:null), FieldSchema(name:XAOC_CM_TA_RM_ALL, type:string, comment:null), FieldSchema(name:ODMP_DISP_NAME, type:string, comment:null), FieldSchema(name:MARKET_DISPLAY_NAME_KO, type:string, comment:null), FieldSchema(name:DYN_MRKT_FLG, type:string, comment:null)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, parameters:{path=adl://connectedsystemsuatdl.azuredatalakestore.net/user/hive/warehouse/RMS_1000255_PQM.db/mrkt_dim, serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{spark.sql.sources.schema.part.1=ta":{}},{"name":"KO_MARKET_LEVEL_4","type":"string","nullable":true,"metadata":{}},{"name":"KO_MARKET_ALTERNATE_MKTS","type":"string","nullable":true,"metadata":{}},{"name":"KO_MARKET_ALL_CHILD_MKTS","type":"string","nullable":true,"metadata":{}},{"name":"CM_TA_RM_ALL","type":"string","nullable":true,"metadata":{}},{"name":"XAOC_CM_TA_RM_ALL","type":"string","nullable":true,"metadata":{}},{"name":"ODMP_DISP_NAME","type":"string","nullable":true,"metadata":{}},{"name":"MARKET_DISPLAY_NAME_KO","type":"string","nullable":true,"metadata":{}},{"name":"DYN_MRKT_FLG","type":"string","nullable":true,"metadata":{}}]}, spark.sql.sources.schema.part.0={"type":"struct","fields":[{"name":"CLIENT_ID","type":"integer","nullable":true,"metadata":{}},{"name":"MRKT_DSC_SHRT","type":"string","nullable":true,"metadata":{}},{"name":"MRKT_DSC_LNG","type":"string","nullable":true,"metadata":{}},{"name":"SAMS_RSTR_IND","type":"integer","nullable":true,"metadata":{}},{"name":"CSL_BLK_DT","type":"timestamp","nullable":true,"metadata":{}},{"name":"POD_RLS_IND","type":"string","nullable":true,"metadata":{}},{"name":"PRJCTD_IND","type":"integer","nullable":true,"metadata":{}},{"name":"MRKT_TYP","type":"string","nullable":true,"metadata":{}},{"name":"MRKT_RPTBL_STRT_DT","type":"timestamp","nullable":true,"metadata":{}},{"name":"MRKT_RPTBL_STOP_DT","type":"timestamp","nullable":true,"metadata":{}},{"name":"TAGRS","type":"string","nullable":true,"metadata":{}},{"name":"MRKT_CD","type":"string","nullable":true,"metadata":{}},{"name":"ACV_ADJ_IND","type":"integer","nullable":true,"metadata":{}},{"name":"MKT_DISP_NAME","type":"string","nullable":true,"metadata":{}},{"name":"MRKT_DRILL_IND","type":"string","nullable":true,"metadata":{}},{"name":"STR_PL_RSTR","type":"integer","nullable":true,"metadata":{}},{"name":"WGHT_RULE_NUM","type":"integer","nullable":true,"metadata":{}},{"name":"XDIM_MRKT2STR","type":"string","nullable":true,"metadata":{}},{"name":"XAOC_COMP_MKT_KEY","type":"integer","nullable":true,"metadata":{}},{"name":"COMP_MKT_KEY","type":"integer","nullable":true,"metadata":{}},{"name":"MRKT_KEY","type":"integer","nullable":true,"metadata":{}},{"name":"COMP_MKT","type":"string","nullable":true,"metadata":{}},{"name":"XAOC_COMP_MKT","type":"string","nullable":true,"metadata":{}},{"name":"MARKET_TYPE","type":"string","nullable":true,"metadata":{}},{"name":"MRKT_RSTR_ID","type":"integer","nullable":true,"metadata":{}},{"name":"MRKT_RSTR_POS","type":"integer","nullable":true,"metadata":{}},{"name":"MRKT_IND","type":"string","nullable":true,"metadata":{}},{"name":"CHANNEL","type":"string","nullable":true,"metadata":{}},{"name":"SHR_MRKT_KEY","type":"string","nullable":true,"metadata":{}},{"name":"ETH_MKT_TYP","type":"string","nullable":true,"metadata":{}},{"name":"MRKT_AGG_FLG","type":"integer","nullable":true,"metadata":{}},{"name":"DT_SRC_DTL_NM","type":"string","nullable":true,"metadata":{}},{"name":"SUPUP","type":"string","nullable":true,"metadata":{}},{"name":"COUNTRY","type":"string","nullable":true,"metadata":{}},{"name":"CS_MARKET_TYPE","type":"string","nullable":true,"metadata":{}},{"name":"CS_MARKET_SET","type":"string","nullable":true,"metadata":{}},{"name":"SUPPRESSION","type":"string","nullable":true,"metadata":{}},{"name":"DEF_MRKT_FLG","type":"string","nullable":true,"metadata":{}},{"name":"GEOGRAPHY","type":"string","nullable":true,"metadata":{}},{"name":"CS_CHANNEL","type":"string","nullable":true,"metadata":{}},{"name":"CS_MARKET_SET_DETAIL","type":"string","nullable":true,"metadata":{}},{"name":"CS_MARKET_TYPE_SET","type":"string","nullable":true,"metadata":{}},{"name":"CS_TA_CHANNEL","type":"string","nullable":true,"metadata":{}},{"name":"MRKT_TT_DMG_CD","type":"integer","nullable":true,"metadata":{}},{"name":"MRKT_TYPE_HS","type":"string","nullable":true,"metadata":{}},{"name":"TA_REM","type":"string","nullable":true,"metadata":{}},{"name":"XAOC_TA_REM","type":"string","nullable":true,"metadata":{}},{"name":"KO_MARKET_PARENT_CHANNEL","type":"string","nullable":true,"metadata":{}},{"name":"KO_MARKET_PARENT_1","type":"string","nullable":true,"metadata":{}},{"name":"KO_MARKET_PARENT_2","type":"string","nullable":true,"metadata":{}},{"name":"KO_MARKET_PARENT_3","type":"string","nullable":true,"metadata":{}},{"name":"KO_MARKET_PARENT_4","type":"string","nullable":true,"metadata":{}},{"name":"KO_MARKET_LEVEL_CHANNEL","type":"string","nullable":true,"metadata":{}},{"name":"KO_MARKET_LEVEL_1","type":"string","nullable":true,"metadata":{}},{"name":"KO_MARKET_LEVEL_2","type":"string","nullable":true,"metadata":{}},{"name":"KO_MARKET_LEVEL_3","type":"string","nullable":true,"metada, spark.sql.sources.schema.numParts=2, spark.sql.sources.provider=parquet, spark.sql.create.version=2.4.0}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{root=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:root, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:root, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:root, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:root, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null))	
19/10/16 18:31:01 INFO FileToDimension: >>>>>>>>>> Writing table RMS_1000255_PQM.MRKT_DIM Completed.
19/10/16 18:31:01 INFO FileToDimension: FileToDimension Some(MarketCSV) for MRKT_DIM input file wasbs://teuatcontainer01@teuatstorage.blob.core.windows.net//RMS_1000255_PQM/MRKT_DIM.csv>>>>> Completed
19/10/16 18:31:01 INFO FileToDimension: FileToDimension Some(PeriodCSV) for PRD_DIM input file wasbs://teuatcontainer01@teuatstorage.blob.core.windows.net//RMS_1000255_PQM/PRD_DIM.csv>>>>> Started
19/10/16 18:31:01 INFO SQLAppStatusListener: Execution ID: 19 Total Executor Run Time: 2451
19/10/16 18:31:01 INFO InMemoryFileIndex: Start listing leaf files and directories. Size of Paths: 1; threshold: 32
19/10/16 18:31:01 INFO InMemoryFileIndex: Start listing leaf files and directories. Size of Paths: 0; threshold: 32
19/10/16 18:31:01 INFO InMemoryFileIndex: Start listing leaf files and directories. Size of Paths: 1; threshold: 32
19/10/16 18:31:01 INFO InMemoryFileIndex: Start listing leaf files and directories. Size of Paths: 0; threshold: 32
19/10/16 18:31:02 INFO FileSourceStrategy: Pruning directories with: 
19/10/16 18:31:02 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#428, None)) > 0)
19/10/16 18:31:02 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
19/10/16 18:31:02 INFO FileSourceScanExec: Pushed Filters: 
19/10/16 18:31:02 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 649.6 KB, free 54.2 GB)
19/10/16 18:31:02 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 46.9 KB, free 54.2 GB)
19/10/16 18:31:02 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 10.139.64.182:46123 (size: 46.9 KB, free: 54.2 GB)
19/10/16 18:31:02 INFO SparkContext: Created broadcast 6 from csv at FileToDimension.scala:30
19/10/16 18:31:02 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
19/10/16 18:31:02 INFO SparkContext: Starting job: csv at FileToDimension.scala:30
19/10/16 18:31:02 INFO DAGScheduler: Got job 3 (csv at FileToDimension.scala:30) with 1 output partitions
19/10/16 18:31:02 INFO DAGScheduler: Final stage: ResultStage 3 (csv at FileToDimension.scala:30)
19/10/16 18:31:02 INFO DAGScheduler: Parents of final stage: List()
19/10/16 18:31:02 INFO DAGScheduler: Missing parents: List()
19/10/16 18:31:02 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[11] at csv at FileToDimension.scala:30), which has no missing parents
19/10/16 18:31:02 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 14.1 KB, free 54.2 GB)
19/10/16 18:31:02 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 7.0 KB, free 54.2 GB)
19/10/16 18:31:02 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 10.139.64.182:46123 (size: 7.0 KB, free: 54.2 GB)
19/10/16 18:31:02 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1316
19/10/16 18:31:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[11] at csv at FileToDimension.scala:30) (first 15 tasks are for partitions Vector(0))
19/10/16 18:31:02 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks
19/10/16 18:31:02 INFO TaskSetManager: Jars for session None: Map(spark://10.139.64.182:33603/jars/addedFile5681071070702781128dbfs__FileStore_saak9001_transengine_1_0_SNAPSHOT-ab9b9.jar -> 1571250418761, spark://10.139.64.182:33603/jars/addedFile2365597673424539553dbfs__FileStore_saak9001_pqm_core_models_1_0-b0ccd.jar -> 1571250418581, spark://10.139.64.182:33603/jars/addedFile6883373321297042917org_rogach_scallop_2_11_3_3_1-ecc25.jar -> 1571250417966, spark://10.139.64.182:33603/jars/addedFile8624879695200027925xml_apis_xml_apis_1_0_b2-167e6.jar -> 1571250418245, spark://10.139.64.182:33603/jars/addedFile5162712074542881962com_typesafe_config_1_3_1-e3e42.jar -> 1571250418145, spark://10.139.64.182:33603/jars/addedFile6674954176252367935org_scalaj_scalaj_http_2_11_2_3_0-d69e4.jar -> 1571250418854, spark://10.139.64.182:33603/jars/addedFile236936972251169783dom4j_dom4j_1_6_1-3b35e.jar -> 1571250418356, spark://10.139.64.182:33603/jars/addedFile5541486089544592835io_spray_spray_json_2_11_1_3_5-d5f78.jar -> 1571250418683)
19/10/16 18:31:02 INFO TaskSetManager: Files for session None: Map(spark://10.139.64.182:33603/files/addedFile8624879695200027925xml_apis_xml_apis_1_0_b2-167e6.jar -> 1571250418235, spark://10.139.64.182:33603/files/addedFile5162712074542881962com_typesafe_config_1_3_1-e3e42.jar -> 1571250418138, spark://10.139.64.182:33603/files/addedFile2365597673424539553dbfs__FileStore_saak9001_pqm_core_models_1_0-b0ccd.jar -> 1571250418573, spark://10.139.64.182:33603/files/addedFile5681071070702781128dbfs__FileStore_saak9001_transengine_1_0_SNAPSHOT-ab9b9.jar -> 1571250418753, spark://10.139.64.182:33603/files/addedFile236936972251169783dom4j_dom4j_1_6_1-3b35e.jar -> 1571250418346, spark://10.139.64.182:33603/files/addedFile5541486089544592835io_spray_spray_json_2_11_1_3_5-d5f78.jar -> 1571250418676, spark://10.139.64.182:33603/files/addedFile6883373321297042917org_rogach_scallop_2_11_3_3_1-ecc25.jar -> 1571250417952, spark://10.139.64.182:33603/files/addedFile6674954176252367935org_scalaj_scalaj_http_2_11_2_3_0-d69e4.jar -> 1571250418848)
19/10/16 18:31:02 INFO FairSchedulableBuilder: Added task set TaskSet_3.0 tasks to pool 2598616896940891466
19/10/16 18:31:02 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3, 10.139.64.183, executor 0, partition 0, PROCESS_LOCAL, 6055 bytes)
19/10/16 18:31:02 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 10.139.64.183:34813 (size: 7.0 KB, free: 50.8 GB)
19/10/16 18:31:02 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 10.139.64.183:34813 (size: 46.9 KB, free: 50.8 GB)
19/10/16 18:31:02 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 183 ms on 10.139.64.183 (executor 0) (1/1)
19/10/16 18:31:02 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 2598616896940891466
19/10/16 18:31:02 INFO DAGScheduler: ResultStage 3 (csv at FileToDimension.scala:30) finished in 0.195 s
19/10/16 18:31:02 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
19/10/16 18:31:02 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
19/10/16 18:31:02 INFO DAGScheduler: Job 3 finished: csv at FileToDimension.scala:30, took 0.203243 s
19/10/16 18:31:02 INFO FileSourceStrategy: Pruning directories with: 
19/10/16 18:31:02 INFO FileSourceStrategy: Post-Scan Filters: 
19/10/16 18:31:02 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
19/10/16 18:31:02 INFO FileSourceScanExec: Pushed Filters: 
19/10/16 18:31:02 INFO SQLAppStatusListener: Execution ID: 20 Total Executor Run Time: 158
19/10/16 18:31:02 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 649.6 KB, free 54.2 GB)
19/10/16 18:31:02 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 46.9 KB, free 54.2 GB)
19/10/16 18:31:02 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 10.139.64.182:46123 (size: 46.9 KB, free: 54.2 GB)
19/10/16 18:31:02 INFO SparkContext: Created broadcast 8 from csv at FileToDimension.scala:30
19/10/16 18:31:02 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
19/10/16 18:31:02 INFO SparkContext: Starting job: csv at FileToDimension.scala:30
19/10/16 18:31:02 INFO DAGScheduler: Got job 4 (csv at FileToDimension.scala:30) with 1 output partitions
19/10/16 18:31:02 INFO DAGScheduler: Final stage: ResultStage 4 (csv at FileToDimension.scala:30)
19/10/16 18:31:02 INFO DAGScheduler: Parents of final stage: List()
19/10/16 18:31:02 INFO DAGScheduler: Missing parents: List()
19/10/16 18:31:02 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[16] at csv at FileToDimension.scala:30), which has no missing parents
19/10/16 18:31:02 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 17.9 KB, free 54.2 GB)
19/10/16 18:31:02 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 9.0 KB, free 54.2 GB)
19/10/16 18:31:02 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 10.139.64.182:46123 (size: 9.0 KB, free: 54.2 GB)
19/10/16 18:31:02 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1316
19/10/16 18:31:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[16] at csv at FileToDimension.scala:30) (first 15 tasks are for partitions Vector(0))
19/10/16 18:31:02 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks
19/10/16 18:31:02 INFO TaskSetManager: Jars for session None: Map(spark://10.139.64.182:33603/jars/addedFile5681071070702781128dbfs__FileStore_saak9001_transengine_1_0_SNAPSHOT-ab9b9.jar -> 1571250418761, spark://10.139.64.182:33603/jars/addedFile2365597673424539553dbfs__FileStore_saak9001_pqm_core_models_1_0-b0ccd.jar -> 1571250418581, spark://10.139.64.182:33603/jars/addedFile6883373321297042917org_rogach_scallop_2_11_3_3_1-ecc25.jar -> 1571250417966, spark://10.139.64.182:33603/jars/addedFile8624879695200027925xml_apis_xml_apis_1_0_b2-167e6.jar -> 1571250418245, spark://10.139.64.182:33603/jars/addedFile5162712074542881962com_typesafe_config_1_3_1-e3e42.jar -> 1571250418145, spark://10.139.64.182:33603/jars/addedFile6674954176252367935org_scalaj_scalaj_http_2_11_2_3_0-d69e4.jar -> 1571250418854, spark://10.139.64.182:33603/jars/addedFile236936972251169783dom4j_dom4j_1_6_1-3b35e.jar -> 1571250418356, spark://10.139.64.182:33603/jars/addedFile5541486089544592835io_spray_spray_json_2_11_1_3_5-d5f78.jar -> 1571250418683)
19/10/16 18:31:02 INFO TaskSetManager: Files for session None: Map(spark://10.139.64.182:33603/files/addedFile8624879695200027925xml_apis_xml_apis_1_0_b2-167e6.jar -> 1571250418235, spark://10.139.64.182:33603/files/addedFile5162712074542881962com_typesafe_config_1_3_1-e3e42.jar -> 1571250418138, spark://10.139.64.182:33603/files/addedFile2365597673424539553dbfs__FileStore_saak9001_pqm_core_models_1_0-b0ccd.jar -> 1571250418573, spark://10.139.64.182:33603/files/addedFile5681071070702781128dbfs__FileStore_saak9001_transengine_1_0_SNAPSHOT-ab9b9.jar -> 1571250418753, spark://10.139.64.182:33603/files/addedFile236936972251169783dom4j_dom4j_1_6_1-3b35e.jar -> 1571250418346, spark://10.139.64.182:33603/files/addedFile5541486089544592835io_spray_spray_json_2_11_1_3_5-d5f78.jar -> 1571250418676, spark://10.139.64.182:33603/files/addedFile6883373321297042917org_rogach_scallop_2_11_3_3_1-ecc25.jar -> 1571250417952, spark://10.139.64.182:33603/files/addedFile6674954176252367935org_scalaj_scalaj_http_2_11_2_3_0-d69e4.jar -> 1571250418848)
19/10/16 18:31:02 INFO FairSchedulableBuilder: Added task set TaskSet_4.0 tasks to pool 2598616896940891466
19/10/16 18:31:02 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4, 10.139.64.183, executor 0, partition 0, PROCESS_LOCAL, 6055 bytes)
19/10/16 18:31:02 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 10.139.64.183:34813 (size: 9.0 KB, free: 50.8 GB)
19/10/16 18:31:02 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 10.139.64.183:34813 (size: 46.9 KB, free: 50.8 GB)
19/10/16 18:31:02 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 196 ms on 10.139.64.183 (executor 0) (1/1)
19/10/16 18:31:02 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 2598616896940891466
19/10/16 18:31:02 INFO DAGScheduler: ResultStage 4 (csv at FileToDimension.scala:30) finished in 0.203 s
19/10/16 18:31:02 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
19/10/16 18:31:02 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
19/10/16 18:31:02 INFO DAGScheduler: Job 4 finished: csv at FileToDimension.scala:30, took 0.210491 s
19/10/16 18:31:02 INFO FileToDimension: >>>>>>>>>> Writing table RMS_1000255_PQM.PRD_DIM Started.
19/10/16 18:31:02 INFO HiveMetaStore: 1: get_table : db=rms_1000255_pqm tbl=prd_dim
19/10/16 18:31:02 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=rms_1000255_pqm tbl=prd_dim	
19/10/16 18:31:02 INFO FileSourceStrategy: Pruning directories with: 
19/10/16 18:31:02 INFO FileSourceStrategy: Post-Scan Filters: 
19/10/16 18:31:02 INFO FileSourceStrategy: Output Data Schema: struct<QUARTERS: string, QUARTER_IN_YEAR_DSC: string, HYTD: string, PRD_SP: int, WEEKS12: string ... 69 more fields>
19/10/16 18:31:02 INFO FileSourceScanExec: Pushed Filters: 
19/10/16 18:31:02 INFO HiveMetaStore: 1: get_table : db=rms_1000255_pqm tbl=prd_dim
19/10/16 18:31:02 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=rms_1000255_pqm tbl=prd_dim	
19/10/16 18:31:02 INFO HiveMetaStore: 1: get_database: rms_1000255_pqm
19/10/16 18:31:02 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: rms_1000255_pqm	
19/10/16 18:31:02 INFO HiveMetaStore: 1: get_database: rms_1000255_pqm
19/10/16 18:31:02 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: rms_1000255_pqm	
19/10/16 18:31:02 DEBUG HttpTransport: HTTPRequest,Failed,cReqId:ac3ffe1b-33bb-4d4c-9bc9-b89f56644d26.0,lat:8,err:HTTP404(FileNotFoundException),Reqlen:0,Resplen:267,token_ns:5100,sReqId:593d2ccf-e95a-47a2-aa63-89cc0b68e1e0,path:/user/hive/warehouse/RMS_1000255_PQM.db/prd_dim,qp:op=GETFILESTATUS&tooid=true&api-version=2017-08-01
19/10/16 18:31:02 INFO HiveMetaStore: 1: get_database: rms_1000255_pqm
19/10/16 18:31:02 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: rms_1000255_pqm	
19/10/16 18:31:02 INFO HiveMetaStore: 1: get_database: rms_1000255_pqm
19/10/16 18:31:02 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: rms_1000255_pqm	
19/10/16 18:31:02 DEBUG HttpTransport: HTTPRequest,Failed,cReqId:f16bbc3c-4c0a-4b63-9198-aa2e27b52359.0,lat:6,err:HTTP404(FileNotFoundException),Reqlen:0,Resplen:267,token_ns:4500,sReqId:dd02b6bc-6a1b-426d-88f0-664dae991aee,path:/user/hive/warehouse/RMS_1000255_PQM.db/prd_dim,qp:op=GETFILESTATUS&tooid=true&api-version=2017-08-01
19/10/16 18:31:02 INFO ParquetFileFormat: Using user defined output committer for Parquet: org.apache.spark.sql.parquet.DirectParquetOutputCommitter
19/10/16 18:31:02 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 649.5 KB, free 54.2 GB)
19/10/16 18:31:02 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 46.8 KB, free 54.2 GB)
19/10/16 18:31:02 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 10.139.64.182:46123 (size: 46.8 KB, free: 54.2 GB)
19/10/16 18:31:02 INFO SparkContext: Created broadcast 10 from saveAsTable at DataFrameDecorator.scala:17
19/10/16 18:31:02 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
19/10/16 18:31:02 INFO SparkContext: Starting job: saveAsTable at DataFrameDecorator.scala:17
19/10/16 18:31:02 INFO DAGScheduler: Got job 5 (saveAsTable at DataFrameDecorator.scala:17) with 1 output partitions
19/10/16 18:31:02 INFO DAGScheduler: Final stage: ResultStage 5 (saveAsTable at DataFrameDecorator.scala:17)
19/10/16 18:31:02 INFO DAGScheduler: Parents of final stage: List()
19/10/16 18:31:02 INFO DAGScheduler: Missing parents: List()
19/10/16 18:31:02 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[18] at saveAsTable at DataFrameDecorator.scala:17), which has no missing parents
19/10/16 18:31:02 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 326.3 KB, free 54.2 GB)
19/10/16 18:31:02 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 106.7 KB, free 54.2 GB)
19/10/16 18:31:02 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 10.139.64.182:46123 (size: 106.7 KB, free: 54.2 GB)
19/10/16 18:31:02 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1316
19/10/16 18:31:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[18] at saveAsTable at DataFrameDecorator.scala:17) (first 15 tasks are for partitions Vector(0))
19/10/16 18:31:02 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks
19/10/16 18:31:02 INFO TaskSetManager: Jars for session None: Map(spark://10.139.64.182:33603/jars/addedFile5681071070702781128dbfs__FileStore_saak9001_transengine_1_0_SNAPSHOT-ab9b9.jar -> 1571250418761, spark://10.139.64.182:33603/jars/addedFile2365597673424539553dbfs__FileStore_saak9001_pqm_core_models_1_0-b0ccd.jar -> 1571250418581, spark://10.139.64.182:33603/jars/addedFile6883373321297042917org_rogach_scallop_2_11_3_3_1-ecc25.jar -> 1571250417966, spark://10.139.64.182:33603/jars/addedFile8624879695200027925xml_apis_xml_apis_1_0_b2-167e6.jar -> 1571250418245, spark://10.139.64.182:33603/jars/addedFile5162712074542881962com_typesafe_config_1_3_1-e3e42.jar -> 1571250418145, spark://10.139.64.182:33603/jars/addedFile6674954176252367935org_scalaj_scalaj_http_2_11_2_3_0-d69e4.jar -> 1571250418854, spark://10.139.64.182:33603/jars/addedFile236936972251169783dom4j_dom4j_1_6_1-3b35e.jar -> 1571250418356, spark://10.139.64.182:33603/jars/addedFile5541486089544592835io_spray_spray_json_2_11_1_3_5-d5f78.jar -> 1571250418683)
19/10/16 18:31:02 INFO TaskSetManager: Files for session None: Map(spark://10.139.64.182:33603/files/addedFile8624879695200027925xml_apis_xml_apis_1_0_b2-167e6.jar -> 1571250418235, spark://10.139.64.182:33603/files/addedFile5162712074542881962com_typesafe_config_1_3_1-e3e42.jar -> 1571250418138, spark://10.139.64.182:33603/files/addedFile2365597673424539553dbfs__FileStore_saak9001_pqm_core_models_1_0-b0ccd.jar -> 1571250418573, spark://10.139.64.182:33603/files/addedFile5681071070702781128dbfs__FileStore_saak9001_transengine_1_0_SNAPSHOT-ab9b9.jar -> 1571250418753, spark://10.139.64.182:33603/files/addedFile236936972251169783dom4j_dom4j_1_6_1-3b35e.jar -> 1571250418346, spark://10.139.64.182:33603/files/addedFile5541486089544592835io_spray_spray_json_2_11_1_3_5-d5f78.jar -> 1571250418676, spark://10.139.64.182:33603/files/addedFile6883373321297042917org_rogach_scallop_2_11_3_3_1-ecc25.jar -> 1571250417952, spark://10.139.64.182:33603/files/addedFile6674954176252367935org_scalaj_scalaj_http_2_11_2_3_0-d69e4.jar -> 1571250418848)
19/10/16 18:31:02 INFO FairSchedulableBuilder: Added task set TaskSet_5.0 tasks to pool 2598616896940891466
19/10/16 18:31:02 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5, 10.139.64.183, executor 0, partition 0, PROCESS_LOCAL, 6056 bytes)
19/10/16 18:31:02 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 10.139.64.183:34813 (size: 106.7 KB, free: 50.8 GB)
19/10/16 18:31:03 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 10.139.64.183:34813 (size: 46.8 KB, free: 50.8 GB)
19/10/16 18:31:03 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 964 ms on 10.139.64.183 (executor 0) (1/1)
19/10/16 18:31:03 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 2598616896940891466
19/10/16 18:31:03 INFO DAGScheduler: ResultStage 5 (saveAsTable at DataFrameDecorator.scala:17) finished in 1.002 s
19/10/16 18:31:03 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
19/10/16 18:31:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
19/10/16 18:31:03 INFO DAGScheduler: Job 5 finished: saveAsTable at DataFrameDecorator.scala:17, took 1.008077 s
19/10/16 18:31:03 INFO DirectoryAtomicCommitProtocol: Committing job fda19bfb-4918-4708-994b-89a0dcc0cc75
19/10/16 18:31:03 INFO FileSystem: FS_OP_CREATE FILE[adl://connectedsystemsuatdl.azuredatalakestore.net/user/hive/warehouse/RMS_1000255_PQM.db/prd_dim/_committed_3871000950718496626] Creating output stream; permission: rw-r--r--, overwrite: true, bufferSize: 65536
19/10/16 18:31:03 INFO AdlFsOutputStream: FS_OP_CREATE FILE[/user/hive/warehouse/RMS_1000255_PQM.db/prd_dim/_committed_3871000950718496626] Closing stream; size: 120
19/10/16 18:31:03 INFO AdlFsOutputStream: FS_OP_CREATE FILE[/user/hive/warehouse/RMS_1000255_PQM.db/prd_dim/_committed_3871000950718496626] Upload complete; size: 120
19/10/16 18:31:03 INFO AdlFsOutputStream: FS_OP_CREATE FILE[/user/hive/warehouse/RMS_1000255_PQM.db/prd_dim/_committed_3871000950718496626] Closing stream; size: 120
19/10/16 18:31:03 INFO AdlFsOutputStream: FS_OP_CREATE FILE[/user/hive/warehouse/RMS_1000255_PQM.db/prd_dim/_committed_3871000950718496626] Upload complete; size: 120
19/10/16 18:31:03 INFO FileSystem: FS_OP_CREATE FILE[adl://connectedsystemsuatdl.azuredatalakestore.net/user/hive/warehouse/RMS_1000255_PQM.db/prd_dim/_SUCCESS] Creating output stream; permission: rw-r--r--, overwrite: true, bufferSize: 65536
19/10/16 18:31:03 INFO AdlFsOutputStream: FS_OP_CREATE FILE[/user/hive/warehouse/RMS_1000255_PQM.db/prd_dim/_SUCCESS] Closing stream; size: 0
19/10/16 18:31:03 INFO AdlFsOutputStream: FS_OP_CREATE FILE[/user/hive/warehouse/RMS_1000255_PQM.db/prd_dim/_SUCCESS] Upload complete; size: 0
19/10/16 18:31:03 INFO FileSystem: FS_OP_CREATE FILE[adl://connectedsystemsuatdl.azuredatalakestore.net/user/hive/warehouse/RMS_1000255_PQM.db/prd_dim/_SUCCESS] Creating output stream; permission: rw-r--r--, overwrite: true, bufferSize: 65536
19/10/16 18:31:03 INFO AdlFsOutputStream: FS_OP_CREATE FILE[/user/hive/warehouse/RMS_1000255_PQM.db/prd_dim/_SUCCESS] Closing stream; size: 0
19/10/16 18:31:04 INFO AdlFsOutputStream: FS_OP_CREATE FILE[/user/hive/warehouse/RMS_1000255_PQM.db/prd_dim/_SUCCESS] Upload complete; size: 0
19/10/16 18:31:04 INFO DirectoryAtomicCommitProtocol: Job commit completed for fda19bfb-4918-4708-994b-89a0dcc0cc75
19/10/16 18:31:04 INFO DirectoryAtomicCommitProtocol: Auto-vacuuming directories updated by fda19bfb-4918-4708-994b-89a0dcc0cc75
19/10/16 18:31:04 INFO DirectoryAtomicCommitProtocol: Started VACUUM on List(adl://connectedsystemsuatdl.azuredatalakestore.net/user/hive/warehouse/RMS_1000255_PQM.db/prd_dim) with data horizon 1571077864010 (now - 48.0 hours), metadata horizon 1571248864010 (now - 0.5 hours)
19/10/16 18:31:04 WARN FileOperations: method deleteMultiple doesn't exist. Falling back to individual deletion.
19/10/16 18:31:04 INFO FileFormatWriter: Write Job b1e31115-89b9-40df-bfc5-a5d9c0f37eee committed.
19/10/16 18:31:04 INFO FileFormatWriter: Finished processing stats for write job b1e31115-89b9-40df-bfc5-a5d9c0f37eee.
19/10/16 18:31:04 DEBUG HttpTransport: HTTPRequest,Failed,cReqId:9e62b114-38f9-4838-8156-fcde1ede0e96.0,lat:8,err:HTTP404(FileNotFoundException),Reqlen:0,Resplen:283,token_ns:3500,sReqId:88a504e8-fe84-446c-b052-65138de26453,path:/user/hive/warehouse/RMS_1000255_PQM.db/prd_dim/_spark_metadata,qp:op=GETFILESTATUS&tooid=true&api-version=2017-08-01
19/10/16 18:31:04 INFO InMemoryFileIndex: Start listing leaf files and directories. Size of Paths: 1; threshold: 32
19/10/16 18:31:04 INFO InMemoryFileIndex: Start listing leaf files and directories. Size of Paths: 0; threshold: 32
19/10/16 18:31:04 INFO HiveMetaStore: 1: get_database: rms_1000255_pqm
19/10/16 18:31:04 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: rms_1000255_pqm	
19/10/16 18:31:04 INFO HiveMetaStore: 1: get_table : db=rms_1000255_pqm tbl=prd_dim
19/10/16 18:31:04 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=rms_1000255_pqm tbl=prd_dim	
19/10/16 18:31:04 INFO HiveMetaStore: 1: get_database: rms_1000255_pqm
19/10/16 18:31:04 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: rms_1000255_pqm	
19/10/16 18:31:04 INFO HiveMetaStore: 1: get_table : db=rms_1000255_pqm tbl=prd_dim
19/10/16 18:31:04 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=rms_1000255_pqm tbl=prd_dim	
19/10/16 18:31:04 INFO SecuredHiveExternalCatalog: Persisting file based data source table `rms_1000255_pqm`.`prd_dim` into Hive metastore in Hive compatible format.
19/10/16 18:31:04 INFO HiveMetaStore: 1: create_table: Table(tableName:prd_dim, dbName:rms_1000255_pqm, owner:root, createTime:1571250662, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:QUARTERS, type:string, comment:null), FieldSchema(name:QUARTER_IN_YEAR_DSC, type:string, comment:null), FieldSchema(name:HYTD, type:string, comment:null), FieldSchema(name:PRD_SP, type:int, comment:null), FieldSchema(name:WEEKS12, type:string, comment:null), FieldSchema(name:QTD, type:string, comment:null), FieldSchema(name:PRD_DSP_ORDER, type:int, comment:null), FieldSchema(name:MONTHS6, type:string, comment:null), FieldSchema(name:PRD_RLS_IND, type:string, comment:null), FieldSchema(name:HOLIDAY_PERIODS, type:string, comment:null), FieldSchema(name:WEEKS4, type:string, comment:null), FieldSchema(name:YEAR_ORD, type:int, comment:null), FieldSchema(name:YTD, type:string, comment:null), FieldSchema(name:WEEKS24, type:string, comment:null), FieldSchema(name:PMONTH_DSC, type:string, comment:null), FieldSchema(name:FQUARTER_IN_YEAR_DSC, type:string, comment:null), FieldSchema(name:PYEAR_ORD, type:int, comment:null), FieldSchema(name:PWEEK_ORD, type:int, comment:null), FieldSchema(name:PRD_KEY, type:int, comment:null), FieldSchema(name:WEEK_ORD, type:int, comment:null), FieldSchema(name:MONTHS3, type:string, comment:null), FieldSchema(name:CUME_RUL_TYP, type:string, comment:null), FieldSchema(name:PRD_ID, type:int, comment:null), FieldSchema(name:MONTHLY, type:string, comment:null), FieldSchema(name:MONTHS12, type:string, comment:null), FieldSchema(name:RPT_MONTHS, type:string, comment:null), FieldSchema(name:PYEAR_DSC, type:string, comment:null), FieldSchema(name:HALF_YEARS_RTA, type:string, comment:null), FieldSchema(name:WK_ENDING_DSC, type:string, comment:null), FieldSchema(name:MONTHS, type:string, comment:null), FieldSchema(name:PRD_DSC_SHORT, type:string, comment:null), FieldSchema(name:SQUARTER_IN_YEAR_DSC, type:string, comment:null), FieldSchema(name:PRD_DSC_LONG, type:string, comment:null), FieldSchema(name:PPERIOD_DSC, type:string, comment:null), FieldSchema(name:PCKG_ID, type:int, comment:null), FieldSchema(name:PWEEK_DSC, type:string, comment:null), FieldSchema(name:PPERIOD_ORD, type:int, comment:null), FieldSchema(name:SWEEK_IN_YEAR_DSC, type:string, comment:null), FieldSchema(name:CUME_END_DT, type:timestamp, comment:null), FieldSchema(name:MONTH_IN_QUARTER_DSC, type:string, comment:null), FieldSchema(name:PRD_DET_LVL, type:string, comment:null), FieldSchema(name:SMONTH_IN_QUARTER_DSC, type:string, comment:null), FieldSchema(name:PRD_ID_YAGO, type:int, comment:null), FieldSchema(name:WEEKS52, type:string, comment:null), FieldSchema(name:CURRENT_PERIODS, type:string, comment:null), FieldSchema(name:WEEK_IN_MONTH_DSC, type:string, comment:null), FieldSchema(name:WEEKS26, type:string, comment:null), FieldSchema(name:MONTH_ORD, type:int, comment:null), FieldSchema(name:PQUARTER_DSC, type:string, comment:null), FieldSchema(name:FILTER_NAME, type:string, comment:null), FieldSchema(name:PRD_DSP_FOLDER, type:string, comment:null), FieldSchema(name:PRD_SRC_ID, type:int, comment:null), FieldSchema(name:WEEKS4_ORD, type:int, comment:null), FieldSchema(name:DIM_TYP, type:string, comment:null), FieldSchema(name:HALF_YEARS, type:string, comment:null), FieldSchema(name:PQUARTER_ORD, type:int, comment:null), FieldSchema(name:MONTHS1, type:string, comment:null), FieldSchema(name:HOLIDAYS, type:string, comment:null), FieldSchema(name:FYEAR_ORD, type:int, comment:null), FieldSchema(name:PRD_TAG, type:string, comment:null), FieldSchema(name:WEEKS13, type:string, comment:null), FieldSchema(name:SYEAR_DSC, type:string, comment:null), FieldSchema(name:SPERIOD_DSC, type:string, comment:null), FieldSchema(name:NUM_OF_WKS, type:int, comment:null), FieldSchema(name:WK_ENDING_DT, type:timestamp, comment:null), FieldSchema(name:YEAR_DSC, type:string, comment:null), FieldSchema(name:QUARTER_ORD, type:int, comment:null), FieldSchema(name:PERIOD_TO_DATE, type:string, comment:null), FieldSchema(name:DSP_FOLDER_AD, type:string, comment:null), FieldSchema(name:FYEAR_DSC, type:string, comment:null), FieldSchema(name:QUARTERS_RTA, type:string, comment:null)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, parameters:{path=adl://connectedsystemsuatdl.azuredatalakestore.net/user/hive/warehouse/RMS_1000255_PQM.db/prd_dim, serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{spark.sql.sources.schema.part.1=adata":{}},{"name":"FYEAR_ORD","type":"integer","nullable":true,"metadata":{}},{"name":"PRD_TAG","type":"string","nullable":true,"metadata":{}},{"name":"WEEKS13","type":"string","nullable":true,"metadata":{}},{"name":"SYEAR_DSC","type":"string","nullable":true,"metadata":{}},{"name":"SPERIOD_DSC","type":"string","nullable":true,"metadata":{}},{"name":"NUM_OF_WKS","type":"integer","nullable":true,"metadata":{}},{"name":"WK_ENDING_DT","type":"timestamp","nullable":true,"metadata":{}},{"name":"YEAR_DSC","type":"string","nullable":true,"metadata":{}},{"name":"QUARTER_ORD","type":"integer","nullable":true,"metadata":{}},{"name":"PERIOD_TO_DATE","type":"string","nullable":true,"metadata":{}},{"name":"DSP_FOLDER_AD","type":"string","nullable":true,"metadata":{}},{"name":"FYEAR_DSC","type":"string","nullable":true,"metadata":{}},{"name":"QUARTERS_RTA","type":"string","nullable":true,"metadata":{}}]}, spark.sql.sources.schema.part.0={"type":"struct","fields":[{"name":"QUARTERS","type":"string","nullable":true,"metadata":{}},{"name":"QUARTER_IN_YEAR_DSC","type":"string","nullable":true,"metadata":{}},{"name":"HYTD","type":"string","nullable":true,"metadata":{}},{"name":"PRD_SP","type":"integer","nullable":true,"metadata":{}},{"name":"WEEKS12","type":"string","nullable":true,"metadata":{}},{"name":"QTD","type":"string","nullable":true,"metadata":{}},{"name":"PRD_DSP_ORDER","type":"integer","nullable":true,"metadata":{}},{"name":"MONTHS6","type":"string","nullable":true,"metadata":{}},{"name":"PRD_RLS_IND","type":"string","nullable":true,"metadata":{}},{"name":"HOLIDAY_PERIODS","type":"string","nullable":true,"metadata":{}},{"name":"WEEKS4","type":"string","nullable":true,"metadata":{}},{"name":"YEAR_ORD","type":"integer","nullable":true,"metadata":{}},{"name":"YTD","type":"string","nullable":true,"metadata":{}},{"name":"WEEKS24","type":"string","nullable":true,"metadata":{}},{"name":"PMONTH_DSC","type":"string","nullable":true,"metadata":{}},{"name":"FQUARTER_IN_YEAR_DSC","type":"string","nullable":true,"metadata":{}},{"name":"PYEAR_ORD","type":"integer","nullable":true,"metadata":{}},{"name":"PWEEK_ORD","type":"integer","nullable":true,"metadata":{}},{"name":"PRD_KEY","type":"integer","nullable":true,"metadata":{}},{"name":"WEEK_ORD","type":"integer","nullable":true,"metadata":{}},{"name":"MONTHS3","type":"string","nullable":true,"metadata":{}},{"name":"CUME_RUL_TYP","type":"string","nullable":true,"metadata":{}},{"name":"PRD_ID","type":"integer","nullable":true,"metadata":{}},{"name":"MONTHLY","type":"string","nullable":true,"metadata":{}},{"name":"MONTHS12","type":"string","nullable":true,"metadata":{}},{"name":"RPT_MONTHS","type":"string","nullable":true,"metadata":{}},{"name":"PYEAR_DSC","type":"string","nullable":true,"metadata":{}},{"name":"HALF_YEARS_RTA","type":"string","nullable":true,"metadata":{}},{"name":"WK_ENDING_DSC","type":"string","nullable":true,"metadata":{}},{"name":"MONTHS","type":"string","nullable":true,"metadata":{}},{"name":"PRD_DSC_SHORT","type":"string","nullable":true,"metadata":{}},{"name":"SQUARTER_IN_YEAR_DSC","type":"string","nullable":true,"metadata":{}},{"name":"PRD_DSC_LONG","type":"string","nullable":true,"metadata":{}},{"name":"PPERIOD_DSC","type":"string","nullable":true,"metadata":{}},{"name":"PCKG_ID","type":"integer","nullable":true,"metadata":{}},{"name":"PWEEK_DSC","type":"string","nullable":true,"metadata":{}},{"name":"PPERIOD_ORD","type":"integer","nullable":true,"metadata":{}},{"name":"SWEEK_IN_YEAR_DSC","type":"string","nullable":true,"metadata":{}},{"name":"CUME_END_DT","type":"timestamp","nullable":true,"metadata":{}},{"name":"MONTH_IN_QUARTER_DSC","type":"string","nullable":true,"metadata":{}},{"name":"PRD_DET_LVL","type":"string","nullable":true,"metadata":{}},{"name":"SMONTH_IN_QUARTER_DSC","type":"string","nullable":true,"metadata":{}},{"name":"PRD_ID_YAGO","type":"integer","nullable":true,"metadata":{}},{"name":"WEEKS52","type":"string","nullable":true,"metadata":{}},{"name":"CURRENT_PERIODS","type":"string","nullable":true,"metadata":{}},{"name":"WEEK_IN_MONTH_DSC","type":"string","nullable":true,"metadata":{}},{"name":"WEEKS26","type":"string","nullable":true,"metadata":{}},{"name":"MONTH_ORD","type":"integer","nullable":true,"metadata":{}},{"name":"PQUARTER_DSC","type":"string","nullable":true,"metadata":{}},{"name":"FILTER_NAME","type":"string","nullable":true,"metadata":{}},{"name":"PRD_DSP_FOLDER","type":"string","nullable":true,"metadata":{}},{"name":"PRD_SRC_ID","type":"integer","nullable":true,"metadata":{}},{"name":"WEEKS4_ORD","type":"integer","nullable":true,"metadata":{}},{"name":"DIM_TYP","type":"string","nullable":true,"metadata":{}},{"name":"HALF_YEARS","type":"string","nullable":true,"metadata":{}},{"name":"PQUARTER_ORD","type":"integer","nullable":true,"metadata":{}},{"name":"MONTHS1","type":"string","nullable":true,"metadata":{}},{"name":"HOLIDAYS","type":"string","nullable":true,"met, spark.sql.sources.schema.numParts=2, spark.sql.sources.provider=parquet, spark.sql.create.version=2.4.0}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{root=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:root, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:root, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:root, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:root, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null))
19/10/16 18:31:04 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=create_table: Table(tableName:prd_dim, dbName:rms_1000255_pqm, owner:root, createTime:1571250662, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:QUARTERS, type:string, comment:null), FieldSchema(name:QUARTER_IN_YEAR_DSC, type:string, comment:null), FieldSchema(name:HYTD, type:string, comment:null), FieldSchema(name:PRD_SP, type:int, comment:null), FieldSchema(name:WEEKS12, type:string, comment:null), FieldSchema(name:QTD, type:string, comment:null), FieldSchema(name:PRD_DSP_ORDER, type:int, comment:null), FieldSchema(name:MONTHS6, type:string, comment:null), FieldSchema(name:PRD_RLS_IND, type:string, comment:null), FieldSchema(name:HOLIDAY_PERIODS, type:string, comment:null), FieldSchema(name:WEEKS4, type:string, comment:null), FieldSchema(name:YEAR_ORD, type:int, comment:null), FieldSchema(name:YTD, type:string, comment:null), FieldSchema(name:WEEKS24, type:string, comment:null), FieldSchema(name:PMONTH_DSC, type:string, comment:null), FieldSchema(name:FQUARTER_IN_YEAR_DSC, type:string, comment:null), FieldSchema(name:PYEAR_ORD, type:int, comment:null), FieldSchema(name:PWEEK_ORD, type:int, comment:null), FieldSchema(name:PRD_KEY, type:int, comment:null), FieldSchema(name:WEEK_ORD, type:int, comment:null), FieldSchema(name:MONTHS3, type:string, comment:null), FieldSchema(name:CUME_RUL_TYP, type:string, comment:null), FieldSchema(name:PRD_ID, type:int, comment:null), FieldSchema(name:MONTHLY, type:string, comment:null), FieldSchema(name:MONTHS12, type:string, comment:null), FieldSchema(name:RPT_MONTHS, type:string, comment:null), FieldSchema(name:PYEAR_DSC, type:string, comment:null), FieldSchema(name:HALF_YEARS_RTA, type:string, comment:null), FieldSchema(name:WK_ENDING_DSC, type:string, comment:null), FieldSchema(name:MONTHS, type:string, comment:null), FieldSchema(name:PRD_DSC_SHORT, type:string, comment:null), FieldSchema(name:SQUARTER_IN_YEAR_DSC, type:string, comment:null), FieldSchema(name:PRD_DSC_LONG, type:string, comment:null), FieldSchema(name:PPERIOD_DSC, type:string, comment:null), FieldSchema(name:PCKG_ID, type:int, comment:null), FieldSchema(name:PWEEK_DSC, type:string, comment:null), FieldSchema(name:PPERIOD_ORD, type:int, comment:null), FieldSchema(name:SWEEK_IN_YEAR_DSC, type:string, comment:null), FieldSchema(name:CUME_END_DT, type:timestamp, comment:null), FieldSchema(name:MONTH_IN_QUARTER_DSC, type:string, comment:null), FieldSchema(name:PRD_DET_LVL, type:string, comment:null), FieldSchema(name:SMONTH_IN_QUARTER_DSC, type:string, comment:null), FieldSchema(name:PRD_ID_YAGO, type:int, comment:null), FieldSchema(name:WEEKS52, type:string, comment:null), FieldSchema(name:CURRENT_PERIODS, type:string, comment:null), FieldSchema(name:WEEK_IN_MONTH_DSC, type:string, comment:null), FieldSchema(name:WEEKS26, type:string, comment:null), FieldSchema(name:MONTH_ORD, type:int, comment:null), FieldSchema(name:PQUARTER_DSC, type:string, comment:null), FieldSchema(name:FILTER_NAME, type:string, comment:null), FieldSchema(name:PRD_DSP_FOLDER, type:string, comment:null), FieldSchema(name:PRD_SRC_ID, type:int, comment:null), FieldSchema(name:WEEKS4_ORD, type:int, comment:null), FieldSchema(name:DIM_TYP, type:string, comment:null), FieldSchema(name:HALF_YEARS, type:string, comment:null), FieldSchema(name:PQUARTER_ORD, type:int, comment:null), FieldSchema(name:MONTHS1, type:string, comment:null), FieldSchema(name:HOLIDAYS, type:string, comment:null), FieldSchema(name:FYEAR_ORD, type:int, comment:null), FieldSchema(name:PRD_TAG, type:string, comment:null), FieldSchema(name:WEEKS13, type:string, comment:null), FieldSchema(name:SYEAR_DSC, type:string, comment:null), FieldSchema(name:SPERIOD_DSC, type:string, comment:null), FieldSchema(name:NUM_OF_WKS, type:int, comment:null), FieldSchema(name:WK_ENDING_DT, type:timestamp, comment:null), FieldSchema(name:YEAR_DSC, type:string, comment:null), FieldSchema(name:QUARTER_ORD, type:int, comment:null), FieldSchema(name:PERIOD_TO_DATE, type:string, comment:null), FieldSchema(name:DSP_FOLDER_AD, type:string, comment:null), FieldSchema(name:FYEAR_DSC, type:string, comment:null), FieldSchema(name:QUARTERS_RTA, type:string, comment:null)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, parameters:{path=adl://connectedsystemsuatdl.azuredatalakestore.net/user/hive/warehouse/RMS_1000255_PQM.db/prd_dim, serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{spark.sql.sources.schema.part.1=adata":{}},{"name":"FYEAR_ORD","type":"integer","nullable":true,"metadata":{}},{"name":"PRD_TAG","type":"string","nullable":true,"metadata":{}},{"name":"WEEKS13","type":"string","nullable":true,"metadata":{}},{"name":"SYEAR_DSC","type":"string","nullable":true,"metadata":{}},{"name":"SPERIOD_DSC","type":"string","nullable":true,"metadata":{}},{"name":"NUM_OF_WKS","type":"integer","nullable":true,"metadata":{}},{"name":"WK_ENDING_DT","type":"timestamp","nullable":true,"metadata":{}},{"name":"YEAR_DSC","type":"string","nullable":true,"metadata":{}},{"name":"QUARTER_ORD","type":"integer","nullable":true,"metadata":{}},{"name":"PERIOD_TO_DATE","type":"string","nullable":true,"metadata":{}},{"name":"DSP_FOLDER_AD","type":"string","nullable":true,"metadata":{}},{"name":"FYEAR_DSC","type":"string","nullable":true,"metadata":{}},{"name":"QUARTERS_RTA","type":"string","nullable":true,"metadata":{}}]}, spark.sql.sources.schema.part.0={"type":"struct","fields":[{"name":"QUARTERS","type":"string","nullable":true,"metadata":{}},{"name":"QUARTER_IN_YEAR_DSC","type":"string","nullable":true,"metadata":{}},{"name":"HYTD","type":"string","nullable":true,"metadata":{}},{"name":"PRD_SP","type":"integer","nullable":true,"metadata":{}},{"name":"WEEKS12","type":"string","nullable":true,"metadata":{}},{"name":"QTD","type":"string","nullable":true,"metadata":{}},{"name":"PRD_DSP_ORDER","type":"integer","nullable":true,"metadata":{}},{"name":"MONTHS6","type":"string","nullable":true,"metadata":{}},{"name":"PRD_RLS_IND","type":"string","nullable":true,"metadata":{}},{"name":"HOLIDAY_PERIODS","type":"string","nullable":true,"metadata":{}},{"name":"WEEKS4","type":"string","nullable":true,"metadata":{}},{"name":"YEAR_ORD","type":"integer","nullable":true,"metadata":{}},{"name":"YTD","type":"string","nullable":true,"metadata":{}},{"name":"WEEKS24","type":"string","nullable":true,"metadata":{}},{"name":"PMONTH_DSC","type":"string","nullable":true,"metadata":{}},{"name":"FQUARTER_IN_YEAR_DSC","type":"string","nullable":true,"metadata":{}},{"name":"PYEAR_ORD","type":"integer","nullable":true,"metadata":{}},{"name":"PWEEK_ORD","type":"integer","nullable":true,"metadata":{}},{"name":"PRD_KEY","type":"integer","nullable":true,"metadata":{}},{"name":"WEEK_ORD","type":"integer","nullable":true,"metadata":{}},{"name":"MONTHS3","type":"string","nullable":true,"metadata":{}},{"name":"CUME_RUL_TYP","type":"string","nullable":true,"metadata":{}},{"name":"PRD_ID","type":"integer","nullable":true,"metadata":{}},{"name":"MONTHLY","type":"string","nullable":true,"metadata":{}},{"name":"MONTHS12","type":"string","nullable":true,"metadata":{}},{"name":"RPT_MONTHS","type":"string","nullable":true,"metadata":{}},{"name":"PYEAR_DSC","type":"string","nullable":true,"metadata":{}},{"name":"HALF_YEARS_RTA","type":"string","nullable":true,"metadata":{}},{"name":"WK_ENDING_DSC","type":"string","nullable":true,"metadata":{}},{"name":"MONTHS","type":"string","nullable":true,"metadata":{}},{"name":"PRD_DSC_SHORT","type":"string","nullable":true,"metadata":{}},{"name":"SQUARTER_IN_YEAR_DSC","type":"string","nullable":true,"metadata":{}},{"name":"PRD_DSC_LONG","type":"string","nullable":true,"metadata":{}},{"name":"PPERIOD_DSC","type":"string","nullable":true,"metadata":{}},{"name":"PCKG_ID","type":"integer","nullable":true,"metadata":{}},{"name":"PWEEK_DSC","type":"string","nullable":true,"metadata":{}},{"name":"PPERIOD_ORD","type":"integer","nullable":true,"metadata":{}},{"name":"SWEEK_IN_YEAR_DSC","type":"string","nullable":true,"metadata":{}},{"name":"CUME_END_DT","type":"timestamp","nullable":true,"metadata":{}},{"name":"MONTH_IN_QUARTER_DSC","type":"string","nullable":true,"metadata":{}},{"name":"PRD_DET_LVL","type":"string","nullable":true,"metadata":{}},{"name":"SMONTH_IN_QUARTER_DSC","type":"string","nullable":true,"metadata":{}},{"name":"PRD_ID_YAGO","type":"integer","nullable":true,"metadata":{}},{"name":"WEEKS52","type":"string","nullable":true,"metadata":{}},{"name":"CURRENT_PERIODS","type":"string","nullable":true,"metadata":{}},{"name":"WEEK_IN_MONTH_DSC","type":"string","nullable":true,"metadata":{}},{"name":"WEEKS26","type":"string","nullable":true,"metadata":{}},{"name":"MONTH_ORD","type":"integer","nullable":true,"metadata":{}},{"name":"PQUARTER_DSC","type":"string","nullable":true,"metadata":{}},{"name":"FILTER_NAME","type":"string","nullable":true,"metadata":{}},{"name":"PRD_DSP_FOLDER","type":"string","nullable":true,"metadata":{}},{"name":"PRD_SRC_ID","type":"integer","nullable":true,"metadata":{}},{"name":"WEEKS4_ORD","type":"integer","nullable":true,"metadata":{}},{"name":"DIM_TYP","type":"string","nullable":true,"metadata":{}},{"name":"HALF_YEARS","type":"string","nullable":true,"metadata":{}},{"name":"PQUARTER_ORD","type":"integer","nullable":true,"metadata":{}},{"name":"MONTHS1","type":"string","nullable":true,"metadata":{}},{"name":"HOLIDAYS","type":"string","nullable":true,"met, spark.sql.sources.schema.numParts=2, spark.sql.sources.provider=parquet, spark.sql.create.version=2.4.0}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{root=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:root, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:root, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:root, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:root, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null))	
19/10/16 18:31:04 INFO FileToDimension: >>>>>>>>>> Writing table RMS_1000255_PQM.PRD_DIM Completed.
19/10/16 18:31:04 INFO FileToDimension: FileToDimension Some(PeriodCSV) for PRD_DIM input file wasbs://teuatcontainer01@teuatstorage.blob.core.windows.net//RMS_1000255_PQM/PRD_DIM.csv>>>>> Completed
19/10/16 18:31:04 INFO ProductExecutor: Executing step..Steps(Some(1),Some(ProductCSV),Some(Product CSV Build),Some(List()),Some(Parquet),Some(reads csv and builds Product file),Some(aaa),Some(library),Some(String),Some(String),Some(Table),Some(PRDC_DIM),Some(StepLibraryDetails(Some(abc.jar),Some(com.nielsen.te.v3.core.executors.ProductExecutor))),Some(String),None)
19/10/16 18:31:04 INFO SQLAppStatusListener: Execution ID: 21 Total Executor Run Time: 929
19/10/16 18:31:04 INFO HiveMetaStore: 1: get_database: abd_1000255
19/10/16 18:31:04 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: abd_1000255	
19/10/16 18:31:04 INFO HiveMetaStore: 1: get_table : db=abd_1000255 tbl=trag_node_hie_prd
19/10/16 18:31:04 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=abd_1000255 tbl=trag_node_hie_prd	
19/10/16 18:31:04 ERROR ScalaDriverLocal: User Code Stack Trace: 
org.apache.spark.sql.AnalysisException: Table or view not found: `abd_1000255`.`trag_node_hie_prd`;;
'UnresolvedRelation `abd_1000255`.`trag_node_hie_prd`

	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:89)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:84)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:148)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:84)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:102)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:116)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:113)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:113)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$analyzed$1.apply(QueryExecution.scala:82)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$analyzed$1.apply(QueryExecution.scala:80)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:80)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:80)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:72)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:80)
	at org.apache.spark.sql.SparkSession.table(SparkSession.scala:677)
	at org.apache.spark.sql.SparkSession.table(SparkSession.scala:673)
	at com.nielsen.te.v3.core.executors.explodeProcess$$anonfun$mergeDbsTblData$1.apply(ProductExecutor.scala:50)
	at com.nielsen.te.v3.core.executors.explodeProcess$$anonfun$mergeDbsTblData$1.apply(ProductExecutor.scala:49)
	at scala.collection.immutable.List.foreach(List.scala:381)
	at com.nielsen.te.v3.core.executors.explodeProcess$class.mergeDbsTblData(ProductExecutor.scala:49)
	at com.nielsen.te.v3.core.executors.ProductExecutor.mergeDbsTblData(ProductExecutor.scala:63)
	at com.nielsen.te.v3.core.executors.ProductExecutor.executeStep(ProductExecutor.scala:77)
	at com.nielsen.te.v3.executor.LibraryStepExecutor.executeStep(StepExecutor.scala:56)
	at com.nielsen.te.v3.executor.StepProcessor$$anonfun$stepProcessor$1.apply(StepProcessor.scala:129)
	at com.nielsen.te.v3.executor.StepProcessor$$anonfun$stepProcessor$1.apply(StepProcessor.scala:127)
	at scala.collection.immutable.List.foreach(List.scala:381)
	at com.nielsen.te.v3.executor.StepProcessor$.stepProcessor(StepProcessor.scala:127)
	at com.nielsen.te.v3.TEv3Main$.appMain(TEv3Main.scala:47)
	at com.nielsen.te.v3.TEv3Main$.main(TEv3Main.scala:51)
	at lined5ae43a8bacb48d1b9d367f68ba7987225.$read$$iw$$iw$$iw$$iw$$iw$$iw.<init>(command--1:1)
	at lined5ae43a8bacb48d1b9d367f68ba7987225.$read$$iw$$iw$$iw$$iw$$iw.<init>(command--1:44)
	at lined5ae43a8bacb48d1b9d367f68ba7987225.$read$$iw$$iw$$iw$$iw.<init>(command--1:46)
	at lined5ae43a8bacb48d1b9d367f68ba7987225.$read$$iw$$iw$$iw.<init>(command--1:48)
	at lined5ae43a8bacb48d1b9d367f68ba7987225.$read$$iw$$iw.<init>(command--1:50)
	at lined5ae43a8bacb48d1b9d367f68ba7987225.$read$$iw.<init>(command--1:52)
	at lined5ae43a8bacb48d1b9d367f68ba7987225.$read.<init>(command--1:54)
	at lined5ae43a8bacb48d1b9d367f68ba7987225.$read$.<init>(command--1:58)
	at lined5ae43a8bacb48d1b9d367f68ba7987225.$read$.<clinit>(command--1)
	at lined5ae43a8bacb48d1b9d367f68ba7987225.$eval$.$print$lzycompute(<notebook>:7)
	at lined5ae43a8bacb48d1b9d367f68ba7987225.$eval$.$print(<notebook>:6)
	at lined5ae43a8bacb48d1b9d367f68ba7987225.$eval.$print(<notebook>)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:786)
	at scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:1047)
	at scala.tools.nsc.interpreter.IMain$WrappedRequest$$anonfun$loadAndRunReq$1.apply(IMain.scala:638)
	at scala.tools.nsc.interpreter.IMain$WrappedRequest$$anonfun$loadAndRunReq$1.apply(IMain.scala:637)
	at scala.reflect.internal.util.ScalaClassLoader$class.asContext(ScalaClassLoader.scala:31)
	at scala.reflect.internal.util.AbstractFileClassLoader.asContext(AbstractFileClassLoader.scala:19)
	at scala.tools.nsc.interpreter.IMain$WrappedRequest.loadAndRunReq(IMain.scala:637)
	at scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:569)
	at scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:565)
	at com.databricks.backend.daemon.driver.DriverILoop.execute(DriverILoop.scala:199)
	at com.databricks.backend.daemon.driver.ScalaDriverLocal$$anonfun$repl$1.apply$mcV$sp(ScalaDriverLocal.scala:189)
	at com.databricks.backend.daemon.driver.ScalaDriverLocal$$anonfun$repl$1.apply(ScalaDriverLocal.scala:189)
	at com.databricks.backend.daemon.driver.ScalaDriverLocal$$anonfun$repl$1.apply(ScalaDriverLocal.scala:189)
	at com.databricks.backend.daemon.driver.DriverLocal$TrapExitInternal$.trapExit(DriverLocal.scala:587)
	at com.databricks.backend.daemon.driver.DriverLocal$TrapExit$.apply(DriverLocal.scala:542)
	at com.databricks.backend.daemon.driver.ScalaDriverLocal.repl(ScalaDriverLocal.scala:189)
	at com.databricks.backend.daemon.driver.DriverLocal$$anonfun$execute$7.apply(DriverLocal.scala:324)
	at com.databricks.backend.daemon.driver.DriverLocal$$anonfun$execute$7.apply(DriverLocal.scala:304)
	at com.databricks.logging.UsageLogging$$anonfun$withAttributionContext$1.apply(UsageLogging.scala:235)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at com.databricks.logging.UsageLogging$class.withAttributionContext(UsageLogging.scala:230)
	at com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:45)
	at com.databricks.logging.UsageLogging$class.withAttributionTags(UsageLogging.scala:268)
	at com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:45)
	at com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:304)
	at com.databricks.backend.daemon.driver.DriverWrapper$$anonfun$tryExecutingCommand$2.apply(DriverWrapper.scala:589)
	at com.databricks.backend.daemon.driver.DriverWrapper$$anonfun$tryExecutingCommand$2.apply(DriverWrapper.scala:589)
	at scala.util.Try$.apply(Try.scala:192)
	at com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:584)
	at com.databricks.backend.daemon.driver.DriverWrapper.getCommandOutputAndError(DriverWrapper.scala:475)
	at com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:542)
	at com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:381)
	at com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:328)
	at com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:215)
	at java.lang.Thread.run(Thread.java:748)
19/10/16 18:31:04 INFO ProgressReporter$: Removed result fetcher for 2598616896940891466_6784036610922160267_job-67852-run-1-action-72028
19/10/16 18:31:05 INFO DriverCorral$: Cleaning the wrapper ReplId-24102-44cb9-7d5d4-a (currently in status Idle(ReplId-24102-44cb9-7d5d4-a))
19/10/16 18:31:05 INFO DriverCorral$: sending shutdown signal for REPL ReplId-24102-44cb9-7d5d4-a
19/10/16 18:31:05 INFO DriverCorral$: sending the interrupt signal for REPL ReplId-24102-44cb9-7d5d4-a
19/10/16 18:31:05 INFO DriverCorral$: waiting for localThread to stop for REPL ReplId-24102-44cb9-7d5d4-a
19/10/16 18:31:05 INFO DriverCorral$: ReplId-24102-44cb9-7d5d4-a successfully discarded
19/10/16 18:31:07 ERROR TaskSchedulerImpl: Lost executor 0 on 10.139.64.183: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
19/10/16 18:31:07 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20191016182633-0000/0 is now LOST (worker lost)
19/10/16 18:31:07 INFO StandaloneSchedulerBackend: Executor app-20191016182633-0000/0 removed: worker lost
19/10/16 18:31:07 INFO DAGScheduler: Executor lost: 0 (epoch 0)
19/10/16 18:31:07 INFO BlockManagerMasterEndpoint: Trying to remove executor 0 from BlockManagerMaster.
19/10/16 18:31:07 INFO StandaloneAppClient$ClientEndpoint: Master removed worker worker-20191016182632-10.139.64.183-38329: 10.139.64.183:38329 got disassociated
19/10/16 18:31:07 INFO BlockManagerMasterEndpoint: Removing block manager BlockManagerId(0, 10.139.64.183, 34813, None)
19/10/16 18:31:07 INFO BlockManagerMaster: Removed 0 successfully in removeExecutor
19/10/16 18:31:07 INFO StandaloneSchedulerBackend: Worker worker-20191016182632-10.139.64.183-38329 removed: 10.139.64.183:38329 got disassociated
19/10/16 18:31:07 INFO BlockManagerMasterEndpoint: Trying to remove executor 0 from BlockManagerMaster.
19/10/16 18:31:07 INFO BlockManagerMaster: Removal of executor 0 requested
19/10/16 18:31:07 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asked to remove non-existent executor 0
19/10/16 18:31:07 INFO TaskSchedulerImpl: Handle removed worker worker-20191016182632-10.139.64.183-38329: 10.139.64.183:38329 got disassociated
19/10/16 18:31:07 INFO DAGScheduler: Shuffle files lost for worker worker-20191016182632-10.139.64.183-38329 on host 10.139.64.183
