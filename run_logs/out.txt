19/10/16 17:01:50 INFO DriverCorral: Starting repl ReplId-4e683-ed4de-84cd6-6
19/10/16 17:01:50 INFO LogicalPlanStats: Setting LogicalPlanStats visitor to com.databricks.sql.optimizer.statsEstimation.DatabricksLogicalPlanStatsVisitor$
19/10/16 17:01:50 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.0 using maven.
19/10/16 17:01:50 INFO IsolatedClientLoader: Initiating download of metastore jars from maven. This may take a while and is not recommended for production use. Please follow the instructions here: https://docs.databricks.com/user-guide/advanced/external-hive-metastore.html#spark-options on how to download the jars just once and use them in your cluster configuration. A log message beginning with 'Downloaded metastore jars' will print once the download is complete.
19/10/16 17:01:50 INFO DBCEventLoggingListener: Rolling event log; numTimesRolledOver = 1
19/10/16 17:01:50 INFO DBCEventLoggingListener: Rolled active log file /databricks/driver/eventlogs/5370616172338932195/eventlog to /databricks/driver/eventlogs/5370616172338932195/eventlog-2019-10-16--17-00
19/10/16 17:01:50 INFO DBCEventLoggingListener: Logging events to eventlogs/5370616172338932195/eventlog
19/10/16 17:01:50 INFO DBCEventLoggingListener: Compressed rolled file /databricks/driver/eventlogs/5370616172338932195/eventlog-2019-10-16--17-00 to /databricks/driver/eventlogs/5370616172338932195/eventlog-2019-10-16--17-00.gz
19/10/16 17:02:43 INFO IsolatedClientLoader: Downloaded metastore jars to /local_disk0/tmp/hive-v2_3-2cbc5e07-1f82-42f9-aa22-f99083d605be
19/10/16 17:02:43 INFO HiveConf: Found configuration file null
19/10/16 17:02:43 INFO SessionState: Created HDFS directory: /tmp/hive/root/a1c39014-69da-4fd1-b4dd-7bbab7963555
19/10/16 17:02:43 INFO SessionState: Created local directory: /local_disk0/tmp/root/a1c39014-69da-4fd1-b4dd-7bbab7963555
19/10/16 17:02:43 INFO SessionState: Created HDFS directory: /tmp/hive/root/a1c39014-69da-4fd1-b4dd-7bbab7963555/_tmp_space.db
19/10/16 17:02:43 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.3) is adl://connectedsystemsuatdl.azuredatalakestore.net/user/hive/warehouse
19/10/16 17:02:44 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
19/10/16 17:02:44 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
19/10/16 17:02:44 INFO HiveMetaStore: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
19/10/16 17:02:44 INFO ObjectStore: ObjectStore, initialize called
19/10/16 17:02:44 INFO Persistence: Property spark.hadoop.datanucleus.connectionPool.maxIdle unknown - will be ignored
19/10/16 17:02:44 INFO Persistence: Property spark.hadoop.datanucleus.connectionPool.maxPoolSize unknown - will be ignored
19/10/16 17:02:44 INFO Persistence: Property datanucleus.fixedDatastore unknown - will be ignored
19/10/16 17:02:44 INFO Persistence: Property spark.hadoop.datanucleus.connectionPool.minPoolSize unknown - will be ignored
19/10/16 17:02:44 INFO Persistence: Property spark.hadoop.javax.jdo.option.ConnectionDriverName unknown - will be ignored
19/10/16 17:02:44 INFO Persistence: Property spark.hadoop.javax.jdo.option.ConnectionURL unknown - will be ignored
19/10/16 17:02:44 INFO Persistence: Property spark.hadoop.javax.jdo.option.ConnectionUserName unknown - will be ignored
19/10/16 17:02:44 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
19/10/16 17:02:44 INFO Persistence: Property spark.hadoop.javax.jdo.option.ConnectionPassword unknown - will be ignored
19/10/16 17:02:44 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
19/10/16 17:02:45 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
19/10/16 17:02:46 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is POSTGRES
19/10/16 17:02:46 INFO ObjectStore: Initialized ObjectStore
19/10/16 17:02:46 INFO HiveMetaStore: Added admin role in metastore
19/10/16 17:02:46 INFO HiveMetaStore: Added public role in metastore
19/10/16 17:02:46 INFO HiveMetaStore: No user is added in admin role, since config is empty
19/10/16 17:02:46 INFO HiveMetaStore: 0: get_all_functions
19/10/16 17:02:46 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_all_functions	
19/10/16 17:02:46 INFO Hive: Registering function collect brickhouse.udf.collect.CollectUDAF
19/10/16 17:02:46 INFO Hive: Registering function collect brickhouse.udf.collect.CollectUDAF
19/10/16 17:02:46 INFO HiveMetaStore: 0: get_database: default
19/10/16 17:02:46 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
19/10/16 17:02:46 INFO SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=a1c39014-69da-4fd1-b4dd-7bbab7963555, clientType=HIVECLI]
19/10/16 17:02:46 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.
19/10/16 17:02:46 INFO metastore: Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
19/10/16 17:02:46 INFO HiveMetaStore: 0: Cleaning up thread local RawStore...
19/10/16 17:02:46 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
19/10/16 17:02:46 INFO HiveMetaStore: 0: Done cleaning up thread local RawStore
19/10/16 17:02:46 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
19/10/16 17:02:46 INFO SessionState: Added [/local_disk0/tmp/addedFile3353797914854493852xml_apis_xml_apis_1_0_b2-167e6.jar] to class path
19/10/16 17:02:46 INFO SessionState: Added resources: [/local_disk0/tmp/addedFile3353797914854493852xml_apis_xml_apis_1_0_b2-167e6.jar]
19/10/16 17:02:46 WARN SparkContext: The jar /local_disk0/tmp/addedFile3353797914854493852xml_apis_xml_apis_1_0_b2-167e6.jar has been added already. Overwriting of added jars is not supported in the current version.
19/10/16 17:02:46 INFO SessionState: Added [/local_disk0/tmp/addedFile8222637637957951783org_scalaj_scalaj_http_2_11_2_3_0-d69e4.jar] to class path
19/10/16 17:02:46 INFO SessionState: Added resources: [/local_disk0/tmp/addedFile8222637637957951783org_scalaj_scalaj_http_2_11_2_3_0-d69e4.jar]
19/10/16 17:02:46 WARN SparkContext: The jar /local_disk0/tmp/addedFile8222637637957951783org_scalaj_scalaj_http_2_11_2_3_0-d69e4.jar has been added already. Overwriting of added jars is not supported in the current version.
19/10/16 17:02:46 INFO SessionState: Added [/local_disk0/tmp/addedFile7597737351700963968dbfs__FileStore_saak9001_transengine_1_0_SNAPSHOT-ab9b9.jar] to class path
19/10/16 17:02:46 INFO SessionState: Added resources: [/local_disk0/tmp/addedFile7597737351700963968dbfs__FileStore_saak9001_transengine_1_0_SNAPSHOT-ab9b9.jar]
19/10/16 17:02:46 WARN SparkContext: The jar /local_disk0/tmp/addedFile7597737351700963968dbfs__FileStore_saak9001_transengine_1_0_SNAPSHOT-ab9b9.jar has been added already. Overwriting of added jars is not supported in the current version.
19/10/16 17:02:46 INFO SessionState: Added [/local_disk0/tmp/addedFile8210562299627401364org_rogach_scallop_2_11_3_3_1-ecc25.jar] to class path
19/10/16 17:02:46 INFO SessionState: Added resources: [/local_disk0/tmp/addedFile8210562299627401364org_rogach_scallop_2_11_3_3_1-ecc25.jar]
19/10/16 17:02:46 WARN SparkContext: The jar /local_disk0/tmp/addedFile8210562299627401364org_rogach_scallop_2_11_3_3_1-ecc25.jar has been added already. Overwriting of added jars is not supported in the current version.
19/10/16 17:02:46 INFO SessionState: Added [/local_disk0/tmp/addedFile1442731520669452270dbfs__FileStore_saak9001_pqm_core_models_1_0-b0ccd.jar] to class path
19/10/16 17:02:46 INFO SessionState: Added resources: [/local_disk0/tmp/addedFile1442731520669452270dbfs__FileStore_saak9001_pqm_core_models_1_0-b0ccd.jar]
19/10/16 17:02:46 WARN SparkContext: The jar /local_disk0/tmp/addedFile1442731520669452270dbfs__FileStore_saak9001_pqm_core_models_1_0-b0ccd.jar has been added already. Overwriting of added jars is not supported in the current version.
19/10/16 17:02:46 INFO SessionState: Added [/local_disk0/tmp/addedFile6473220421637755275com_typesafe_config_1_3_1-e3e42.jar] to class path
19/10/16 17:02:46 INFO SessionState: Added resources: [/local_disk0/tmp/addedFile6473220421637755275com_typesafe_config_1_3_1-e3e42.jar]
19/10/16 17:02:46 WARN SparkContext: The jar /local_disk0/tmp/addedFile6473220421637755275com_typesafe_config_1_3_1-e3e42.jar has been added already. Overwriting of added jars is not supported in the current version.
19/10/16 17:02:46 INFO SessionState: Added [/local_disk0/tmp/addedFile2283635635480536819io_spray_spray_json_2_11_1_3_5-d5f78.jar] to class path
19/10/16 17:02:46 INFO SessionState: Added resources: [/local_disk0/tmp/addedFile2283635635480536819io_spray_spray_json_2_11_1_3_5-d5f78.jar]
19/10/16 17:02:46 WARN SparkContext: The jar /local_disk0/tmp/addedFile2283635635480536819io_spray_spray_json_2_11_1_3_5-d5f78.jar has been added already. Overwriting of added jars is not supported in the current version.
19/10/16 17:02:46 INFO SessionState: Added [/local_disk0/tmp/addedFile495216498108986397dom4j_dom4j_1_6_1-3b35e.jar] to class path
19/10/16 17:02:46 INFO SessionState: Added resources: [/local_disk0/tmp/addedFile495216498108986397dom4j_dom4j_1_6_1-3b35e.jar]
19/10/16 17:02:46 WARN SparkContext: The jar /local_disk0/tmp/addedFile495216498108986397dom4j_dom4j_1_6_1-3b35e.jar has been added already. Overwriting of added jars is not supported in the current version.
19/10/16 17:02:46 INFO ScalaDriverWrapper: setupRepl:ReplId-4e683-ed4de-84cd6-6: finished to load
19/10/16 17:02:46 WARN ScalaDriverLocal: User entity information is incomplete, missing userId,orgId,user.
19/10/16 17:02:46 INFO ProgressReporter$: Added result fetcher for 5649834816523521382_7575602824216384756_3155cb1c-f4ac-4283-bbb1-8345bcdf9d4c
19/10/16 17:02:46 INFO SignalUtils: Registered signal handler for INT
19/10/16 17:02:47 INFO SQLAppStatusListener: Execution ID: 0 Total Executor Run Time: 0
19/10/16 17:02:47 INFO SQLAppStatusListener: Execution ID: 1 Total Executor Run Time: 0
19/10/16 17:02:47 INFO SQLAppStatusListener: Execution ID: 2 Total Executor Run Time: 0
19/10/16 17:02:47 INFO SQLAppStatusListener: Execution ID: 3 Total Executor Run Time: 0
19/10/16 17:02:47 INFO SQLAppStatusListener: Execution ID: 4 Total Executor Run Time: 0
19/10/16 17:02:47 INFO SQLAppStatusListener: Execution ID: 5 Total Executor Run Time: 0
19/10/16 17:02:47 INFO SQLAppStatusListener: Execution ID: 6 Total Executor Run Time: 0
19/10/16 17:02:47 INFO SQLAppStatusListener: Execution ID: 7 Total Executor Run Time: 0
19/10/16 17:02:48 INFO DriverILoop: Set class prefix to: line319986a979e1413c813ad8ed67b3ac97
19/10/16 17:02:48 INFO DriverILoop: set ContextClassLoader
19/10/16 17:02:48 INFO DriverILoop: initialized intp
19/10/16 17:02:51 INFO ProgressReporter$: Removed result fetcher for 5649834816523521382_7575602824216384756_3155cb1c-f4ac-4283-bbb1-8345bcdf9d4c
19/10/16 17:02:59 INFO DriverCorral: Starting repl ReplId-55d9c-75f21-2d5d9-7
19/10/16 17:02:59 INFO LogicalPlanStats: Setting LogicalPlanStats visitor to com.databricks.sql.optimizer.statsEstimation.DatabricksLogicalPlanStatsVisitor$
19/10/16 17:02:59 INFO SessionState: Added [/local_disk0/tmp/addedFile3353797914854493852xml_apis_xml_apis_1_0_b2-167e6.jar] to class path
19/10/16 17:02:59 INFO SessionState: Added resources: [/local_disk0/tmp/addedFile3353797914854493852xml_apis_xml_apis_1_0_b2-167e6.jar]
19/10/16 17:02:59 WARN SparkContext: The jar /local_disk0/tmp/addedFile3353797914854493852xml_apis_xml_apis_1_0_b2-167e6.jar has been added already. Overwriting of added jars is not supported in the current version.
19/10/16 17:02:59 INFO SQLAppStatusListener: Execution ID: 8 Total Executor Run Time: 0
19/10/16 17:02:59 INFO SessionState: Added [/local_disk0/tmp/addedFile8222637637957951783org_scalaj_scalaj_http_2_11_2_3_0-d69e4.jar] to class path
19/10/16 17:02:59 INFO SessionState: Added resources: [/local_disk0/tmp/addedFile8222637637957951783org_scalaj_scalaj_http_2_11_2_3_0-d69e4.jar]
19/10/16 17:02:59 WARN SparkContext: The jar /local_disk0/tmp/addedFile8222637637957951783org_scalaj_scalaj_http_2_11_2_3_0-d69e4.jar has been added already. Overwriting of added jars is not supported in the current version.
19/10/16 17:02:59 INFO SQLAppStatusListener: Execution ID: 9 Total Executor Run Time: 0
19/10/16 17:02:59 INFO SessionState: Added [/local_disk0/tmp/addedFile7597737351700963968dbfs__FileStore_saak9001_transengine_1_0_SNAPSHOT-ab9b9.jar] to class path
19/10/16 17:02:59 INFO SessionState: Added resources: [/local_disk0/tmp/addedFile7597737351700963968dbfs__FileStore_saak9001_transengine_1_0_SNAPSHOT-ab9b9.jar]
19/10/16 17:02:59 WARN SparkContext: The jar /local_disk0/tmp/addedFile7597737351700963968dbfs__FileStore_saak9001_transengine_1_0_SNAPSHOT-ab9b9.jar has been added already. Overwriting of added jars is not supported in the current version.
19/10/16 17:02:59 INFO SQLAppStatusListener: Execution ID: 10 Total Executor Run Time: 0
19/10/16 17:02:59 INFO SessionState: Added [/local_disk0/tmp/addedFile8210562299627401364org_rogach_scallop_2_11_3_3_1-ecc25.jar] to class path
19/10/16 17:02:59 INFO SessionState: Added resources: [/local_disk0/tmp/addedFile8210562299627401364org_rogach_scallop_2_11_3_3_1-ecc25.jar]
19/10/16 17:02:59 WARN SparkContext: The jar /local_disk0/tmp/addedFile8210562299627401364org_rogach_scallop_2_11_3_3_1-ecc25.jar has been added already. Overwriting of added jars is not supported in the current version.
19/10/16 17:02:59 INFO SQLAppStatusListener: Execution ID: 11 Total Executor Run Time: 0
19/10/16 17:02:59 INFO SessionState: Added [/local_disk0/tmp/addedFile1442731520669452270dbfs__FileStore_saak9001_pqm_core_models_1_0-b0ccd.jar] to class path
19/10/16 17:02:59 INFO SessionState: Added resources: [/local_disk0/tmp/addedFile1442731520669452270dbfs__FileStore_saak9001_pqm_core_models_1_0-b0ccd.jar]
19/10/16 17:02:59 WARN SparkContext: The jar /local_disk0/tmp/addedFile1442731520669452270dbfs__FileStore_saak9001_pqm_core_models_1_0-b0ccd.jar has been added already. Overwriting of added jars is not supported in the current version.
19/10/16 17:02:59 INFO SQLAppStatusListener: Execution ID: 12 Total Executor Run Time: 0
19/10/16 17:02:59 INFO SessionState: Added [/local_disk0/tmp/addedFile6473220421637755275com_typesafe_config_1_3_1-e3e42.jar] to class path
19/10/16 17:02:59 INFO SessionState: Added resources: [/local_disk0/tmp/addedFile6473220421637755275com_typesafe_config_1_3_1-e3e42.jar]
19/10/16 17:02:59 WARN SparkContext: The jar /local_disk0/tmp/addedFile6473220421637755275com_typesafe_config_1_3_1-e3e42.jar has been added already. Overwriting of added jars is not supported in the current version.
19/10/16 17:02:59 INFO SQLAppStatusListener: Execution ID: 13 Total Executor Run Time: 0
19/10/16 17:02:59 INFO SessionState: Added [/local_disk0/tmp/addedFile2283635635480536819io_spray_spray_json_2_11_1_3_5-d5f78.jar] to class path
19/10/16 17:02:59 INFO SessionState: Added resources: [/local_disk0/tmp/addedFile2283635635480536819io_spray_spray_json_2_11_1_3_5-d5f78.jar]
19/10/16 17:02:59 WARN SparkContext: The jar /local_disk0/tmp/addedFile2283635635480536819io_spray_spray_json_2_11_1_3_5-d5f78.jar has been added already. Overwriting of added jars is not supported in the current version.
19/10/16 17:02:59 INFO SQLAppStatusListener: Execution ID: 14 Total Executor Run Time: 0
19/10/16 17:02:59 INFO SessionState: Added [/local_disk0/tmp/addedFile495216498108986397dom4j_dom4j_1_6_1-3b35e.jar] to class path
19/10/16 17:02:59 INFO SessionState: Added resources: [/local_disk0/tmp/addedFile495216498108986397dom4j_dom4j_1_6_1-3b35e.jar]
19/10/16 17:02:59 WARN SparkContext: The jar /local_disk0/tmp/addedFile495216498108986397dom4j_dom4j_1_6_1-3b35e.jar has been added already. Overwriting of added jars is not supported in the current version.
19/10/16 17:02:59 INFO ScalaDriverWrapper: setupRepl:ReplId-55d9c-75f21-2d5d9-7: finished to load
19/10/16 17:02:59 INFO SQLAppStatusListener: Execution ID: 15 Total Executor Run Time: 0
19/10/16 17:02:59 WARN ScalaDriverLocal: User entity information is incomplete, missing userId,user.
19/10/16 17:02:59 INFO ProgressReporter$: Added result fetcher for 6186194774562528663_8903127354785209889_job-67849-run-1-action-72025
19/10/16 17:02:59 INFO DriverILoop: Set class prefix to: line85237363e5484a9a98c17c2f3e11d1cc
19/10/16 17:02:59 INFO DriverILoop: set ContextClassLoader
19/10/16 17:02:59 INFO DriverILoop: initialized intp
19/10/16 17:03:02 WARN SparkContext: Using an existing SparkContext; some configuration may not take effect.
19/10/16 17:03:02 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
19/10/16 17:03:02 INFO Instructions: Instruction File has been Read Successfully.>>>>>>>>>>>
19/10/16 17:03:03 INFO HiveMetaStore: 1: get_database: global_temp
19/10/16 17:03:03 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: global_temp	
19/10/16 17:03:03 WARN HiveConf: HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist
19/10/16 17:03:03 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
19/10/16 17:03:03 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
19/10/16 17:03:03 INFO HiveMetaStore: 1: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
19/10/16 17:03:03 INFO ObjectStore: ObjectStore, initialize called
19/10/16 17:03:03 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is POSTGRES
19/10/16 17:03:03 INFO ObjectStore: Initialized ObjectStore
19/10/16 17:03:03 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException
19/10/16 17:03:03 INFO HiveMetaStore: 1: get_table : db=abd_1000255 tbl=bd_output_1000255
19/10/16 17:03:03 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=abd_1000255 tbl=bd_output_1000255	
19/10/16 17:03:03 INFO HiveMetaStore: 1: get_database: rms_1000255_pqm
19/10/16 17:03:03 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: rms_1000255_pqm	
19/10/16 17:03:03 WARN ObjectStore: Failed to get database rms_1000255_pqm, returning NoSuchObjectException
19/10/16 17:03:03 INFO HiveMetaStore: 1: get_database: rms_1000255_pqm
19/10/16 17:03:03 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: rms_1000255_pqm	
19/10/16 17:03:03 WARN ObjectStore: Failed to get database rms_1000255_pqm, returning NoSuchObjectException
19/10/16 17:03:03 INFO SQLAppStatusListener: Execution ID: 16 Total Executor Run Time: 0
19/10/16 17:03:03 INFO HiveMetaStore: 1: create_database: Database(name:rms_1000255_pqm, description:, locationUri:adl://connectedsystemsuatdl.azuredatalakestore.net/user/hive/warehouse/RMS_1000255_PQM.db, parameters:{})
19/10/16 17:03:03 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=create_database: Database(name:rms_1000255_pqm, description:, locationUri:adl://connectedsystemsuatdl.azuredatalakestore.net/user/hive/warehouse/RMS_1000255_PQM.db, parameters:{})	
19/10/16 17:03:03 WARN ObjectStore: Failed to get database rms_1000255_pqm, returning NoSuchObjectException
19/10/16 17:03:03 DEBUG AccessTokenProvider: AADToken: no token. Returning expiring=true
19/10/16 17:03:03 DEBUG AccessTokenProvider: AAD Token is missing or expired: Calling refresh-token from abstract base class
19/10/16 17:03:03 DEBUG ClientCredsTokenProvider: AADToken: refreshing client-credential based token
19/10/16 17:03:03 DEBUG AzureADAuthenticator: AADToken: starting to fetch token using client creds for client ID 003106c3-ff8c-4c62-a060-24b8b32ea512
19/10/16 17:03:03 DEBUG AzureADAuthenticator: AADToken: fetched token with expiry Thu Oct 17 01:03:03 UTC 2019
19/10/16 17:03:03 DEBUG HttpTransport: HTTPRequest,Failed,cReqId:08eede64-2a8b-4ec8-a7fa-7088095691cb.0,lat:331,err:HTTP404(FileNotFoundException),Reqlen:0,Resplen:259,token_ns:165294288,sReqId:13f5dd2a-ff27-4af8-a8b6-f2e9be8be0db,path:/user/hive/warehouse/RMS_1000255_PQM.db,qp:op=GETFILESTATUS&tooid=true&api-version=2017-08-01
19/10/16 17:03:03 INFO FileUtils: Creating directory if it doesn't exist: adl://connectedsystemsuatdl.azuredatalakestore.net/user/hive/warehouse/RMS_1000255_PQM.db
19/10/16 17:03:03 INFO SQLAppStatusListener: Execution ID: 17 Total Executor Run Time: 0
19/10/16 17:03:03 INFO FileToDimension: FileToDimension Some(MarketCSV) for MRKT_DIM input file wasbs://teuatcontainer01@teuatstorage.blob.core.windows.net//RMS_1000255_PQM/MRKT_DIM.csv>>>>> Started
19/10/16 17:03:04 INFO InMemoryFileIndex: Start listing leaf files and directories. Size of Paths: 1; threshold: 32
19/10/16 17:03:04 INFO InMemoryFileIndex: Start listing leaf files and directories. Size of Paths: 0; threshold: 32
19/10/16 17:03:04 INFO InMemoryFileIndex: Start listing leaf files and directories. Size of Paths: 1; threshold: 32
19/10/16 17:03:04 INFO InMemoryFileIndex: Start listing leaf files and directories. Size of Paths: 0; threshold: 32
19/10/16 17:03:05 INFO FileSourceStrategy: Pruning directories with: 
19/10/16 17:03:05 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#34, None)) > 0)
19/10/16 17:03:05 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
19/10/16 17:03:05 INFO FileSourceScanExec: Pushed Filters: 
19/10/16 17:03:05 INFO CodeGenerator: Code generated in 317.441686 ms
19/10/16 17:03:05 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 649.6 KB, free 54.2 GB)
19/10/16 17:03:05 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 47.0 KB, free 54.2 GB)
19/10/16 17:03:05 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.139.64.54:39703 (size: 47.0 KB, free: 54.2 GB)
19/10/16 17:03:05 INFO SparkContext: Created broadcast 0 from csv at FileToDimension.scala:30
19/10/16 17:03:05 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
19/10/16 17:03:05 INFO SparkContext: Starting job: csv at FileToDimension.scala:30
19/10/16 17:03:05 INFO DAGScheduler: Got job 0 (csv at FileToDimension.scala:30) with 1 output partitions
19/10/16 17:03:05 INFO DAGScheduler: Final stage: ResultStage 0 (csv at FileToDimension.scala:30)
19/10/16 17:03:05 INFO DAGScheduler: Parents of final stage: List()
19/10/16 17:03:05 INFO DAGScheduler: Missing parents: List()
19/10/16 17:03:05 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at csv at FileToDimension.scala:30), which has no missing parents
19/10/16 17:03:06 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 14.1 KB, free 54.2 GB)
19/10/16 17:03:06 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.0 KB, free 54.2 GB)
19/10/16 17:03:06 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.139.64.54:39703 (size: 7.0 KB, free: 54.2 GB)
19/10/16 17:03:06 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1316
19/10/16 17:03:06 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at csv at FileToDimension.scala:30) (first 15 tasks are for partitions Vector(0))
19/10/16 17:03:06 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
19/10/16 17:03:06 INFO TaskSetManager: Jars for session None: Map(spark://10.139.64.54:42381/jars/addedFile2283635635480536819io_spray_spray_json_2_11_1_3_5-d5f78.jar -> 1571245130561, spark://10.139.64.54:42381/jars/addedFile1442731520669452270dbfs__FileStore_saak9001_pqm_core_models_1_0-b0ccd.jar -> 1571245130466, spark://10.139.64.54:42381/jars/addedFile8210562299627401364org_rogach_scallop_2_11_3_3_1-ecc25.jar -> 1571245130071, spark://10.139.64.54:42381/jars/addedFile3353797914854493852xml_apis_xml_apis_1_0_b2-167e6.jar -> 1571245130253, spark://10.139.64.54:42381/jars/addedFile6473220421637755275com_typesafe_config_1_3_1-e3e42.jar -> 1571245130174, spark://10.139.64.54:42381/jars/addedFile495216498108986397dom4j_dom4j_1_6_1-3b35e.jar -> 1571245130329, spark://10.139.64.54:42381/jars/addedFile7597737351700963968dbfs__FileStore_saak9001_transengine_1_0_SNAPSHOT-ab9b9.jar -> 1571245130726, spark://10.139.64.54:42381/jars/addedFile8222637637957951783org_scalaj_scalaj_http_2_11_2_3_0-d69e4.jar -> 1571245130806)
19/10/16 17:03:06 INFO TaskSetManager: Files for session None: Map(spark://10.139.64.54:42381/files/addedFile495216498108986397dom4j_dom4j_1_6_1-3b35e.jar -> 1571245130322, spark://10.139.64.54:42381/files/addedFile6473220421637755275com_typesafe_config_1_3_1-e3e42.jar -> 1571245130165, spark://10.139.64.54:42381/files/addedFile1442731520669452270dbfs__FileStore_saak9001_pqm_core_models_1_0-b0ccd.jar -> 1571245130458, spark://10.139.64.54:42381/files/addedFile2283635635480536819io_spray_spray_json_2_11_1_3_5-d5f78.jar -> 1571245130552, spark://10.139.64.54:42381/files/addedFile8210562299627401364org_rogach_scallop_2_11_3_3_1-ecc25.jar -> 1571245130055, spark://10.139.64.54:42381/files/addedFile7597737351700963968dbfs__FileStore_saak9001_transengine_1_0_SNAPSHOT-ab9b9.jar -> 1571245130717, spark://10.139.64.54:42381/files/addedFile3353797914854493852xml_apis_xml_apis_1_0_b2-167e6.jar -> 1571245130246, spark://10.139.64.54:42381/files/addedFile8222637637957951783org_scalaj_scalaj_http_2_11_2_3_0-d69e4.jar -> 1571245130798)
19/10/16 17:03:06 WARN FairSchedulableBuilder: A job was submitted with scheduler pool 6186194774562528663, which has not been configured. This can happen when the file that pools are read from isn't set, or when that file doesn't contain 6186194774562528663. Created 6186194774562528663 with default configuration (schedulingMode: FIFO, minShare: 0, weight: 1)
19/10/16 17:03:06 INFO FairSchedulableBuilder: Added task set TaskSet_0.0 tasks to pool 6186194774562528663
19/10/16 17:03:06 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, 10.139.64.53, executor 0, partition 0, PROCESS_LOCAL, 6056 bytes)
19/10/16 17:03:06 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.139.64.53:43111 (size: 7.0 KB, free: 50.8 GB)
19/10/16 17:03:07 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.139.64.53:43111 (size: 47.0 KB, free: 50.8 GB)
19/10/16 17:03:08 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2560 ms on 10.139.64.53 (executor 0) (1/1)
19/10/16 17:03:08 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 6186194774562528663
19/10/16 17:03:08 INFO DAGScheduler: ResultStage 0 (csv at FileToDimension.scala:30) finished in 2.692 s
19/10/16 17:03:08 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
19/10/16 17:03:08 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
19/10/16 17:03:08 INFO DAGScheduler: Job 0 finished: csv at FileToDimension.scala:30, took 2.781369 s
19/10/16 17:03:08 INFO CodeGenerator: Code generated in 17.372644 ms
19/10/16 17:03:08 INFO FileSourceStrategy: Pruning directories with: 
19/10/16 17:03:08 INFO FileSourceStrategy: Post-Scan Filters: 
19/10/16 17:03:08 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
19/10/16 17:03:08 INFO FileSourceScanExec: Pushed Filters: 
19/10/16 17:03:08 INFO SQLAppStatusListener: Execution ID: 18 Total Executor Run Time: 1807
19/10/16 17:03:08 INFO CodeGenerator: Code generated in 52.642634 ms
19/10/16 17:03:08 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 649.6 KB, free 54.2 GB)
19/10/16 17:03:08 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 47.0 KB, free 54.2 GB)
19/10/16 17:03:08 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.139.64.54:39703 (size: 47.0 KB, free: 54.2 GB)
19/10/16 17:03:08 INFO SparkContext: Created broadcast 2 from csv at FileToDimension.scala:30
19/10/16 17:03:08 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
19/10/16 17:03:08 INFO SparkContext: Starting job: csv at FileToDimension.scala:30
19/10/16 17:03:08 INFO DAGScheduler: Got job 1 (csv at FileToDimension.scala:30) with 1 output partitions
19/10/16 17:03:08 INFO DAGScheduler: Final stage: ResultStage 1 (csv at FileToDimension.scala:30)
19/10/16 17:03:08 INFO DAGScheduler: Parents of final stage: List()
19/10/16 17:03:08 INFO DAGScheduler: Missing parents: List()
19/10/16 17:03:08 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at csv at FileToDimension.scala:30), which has no missing parents
19/10/16 17:03:08 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 18.0 KB, free 54.2 GB)
19/10/16 17:03:08 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 9.0 KB, free 54.2 GB)
19/10/16 17:03:08 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.139.64.54:39703 (size: 9.0 KB, free: 54.2 GB)
19/10/16 17:03:08 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1316
19/10/16 17:03:08 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at csv at FileToDimension.scala:30) (first 15 tasks are for partitions Vector(0))
19/10/16 17:03:08 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks
19/10/16 17:03:08 INFO TaskSetManager: Jars for session None: Map(spark://10.139.64.54:42381/jars/addedFile2283635635480536819io_spray_spray_json_2_11_1_3_5-d5f78.jar -> 1571245130561, spark://10.139.64.54:42381/jars/addedFile1442731520669452270dbfs__FileStore_saak9001_pqm_core_models_1_0-b0ccd.jar -> 1571245130466, spark://10.139.64.54:42381/jars/addedFile8210562299627401364org_rogach_scallop_2_11_3_3_1-ecc25.jar -> 1571245130071, spark://10.139.64.54:42381/jars/addedFile3353797914854493852xml_apis_xml_apis_1_0_b2-167e6.jar -> 1571245130253, spark://10.139.64.54:42381/jars/addedFile6473220421637755275com_typesafe_config_1_3_1-e3e42.jar -> 1571245130174, spark://10.139.64.54:42381/jars/addedFile495216498108986397dom4j_dom4j_1_6_1-3b35e.jar -> 1571245130329, spark://10.139.64.54:42381/jars/addedFile7597737351700963968dbfs__FileStore_saak9001_transengine_1_0_SNAPSHOT-ab9b9.jar -> 1571245130726, spark://10.139.64.54:42381/jars/addedFile8222637637957951783org_scalaj_scalaj_http_2_11_2_3_0-d69e4.jar -> 1571245130806)
19/10/16 17:03:08 INFO TaskSetManager: Files for session None: Map(spark://10.139.64.54:42381/files/addedFile495216498108986397dom4j_dom4j_1_6_1-3b35e.jar -> 1571245130322, spark://10.139.64.54:42381/files/addedFile6473220421637755275com_typesafe_config_1_3_1-e3e42.jar -> 1571245130165, spark://10.139.64.54:42381/files/addedFile1442731520669452270dbfs__FileStore_saak9001_pqm_core_models_1_0-b0ccd.jar -> 1571245130458, spark://10.139.64.54:42381/files/addedFile2283635635480536819io_spray_spray_json_2_11_1_3_5-d5f78.jar -> 1571245130552, spark://10.139.64.54:42381/files/addedFile8210562299627401364org_rogach_scallop_2_11_3_3_1-ecc25.jar -> 1571245130055, spark://10.139.64.54:42381/files/addedFile7597737351700963968dbfs__FileStore_saak9001_transengine_1_0_SNAPSHOT-ab9b9.jar -> 1571245130717, spark://10.139.64.54:42381/files/addedFile3353797914854493852xml_apis_xml_apis_1_0_b2-167e6.jar -> 1571245130246, spark://10.139.64.54:42381/files/addedFile8222637637957951783org_scalaj_scalaj_http_2_11_2_3_0-d69e4.jar -> 1571245130798)
19/10/16 17:03:08 WARN FairSchedulableBuilder: A job was submitted with scheduler pool 6186194774562528663, which has not been configured. This can happen when the file that pools are read from isn't set, or when that file doesn't contain 6186194774562528663. Created 6186194774562528663 with default configuration (schedulingMode: FIFO, minShare: 0, weight: 1)
19/10/16 17:03:08 INFO FairSchedulableBuilder: Added task set TaskSet_1.0 tasks to pool 6186194774562528663
19/10/16 17:03:08 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, 10.139.64.53, executor 0, partition 0, PROCESS_LOCAL, 6056 bytes)
19/10/16 17:03:09 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.139.64.53:43111 (size: 9.0 KB, free: 50.8 GB)
19/10/16 17:03:10 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.139.64.53:43111 (size: 47.0 KB, free: 50.8 GB)
19/10/16 17:03:10 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1999 ms on 10.139.64.53 (executor 0) (1/1)
19/10/16 17:03:10 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 6186194774562528663
19/10/16 17:03:10 INFO DAGScheduler: ResultStage 1 (csv at FileToDimension.scala:30) finished in 2.009 s
19/10/16 17:03:10 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
19/10/16 17:03:10 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
19/10/16 17:03:10 INFO DAGScheduler: Job 1 finished: csv at FileToDimension.scala:30, took 2.020517 s
19/10/16 17:03:11 INFO FileToDimension: >>>>>>>>>> Writing table RMS_1000255_PQM.MRKT_DIM Started.
19/10/16 17:03:11 INFO HiveMetaStore: 1: get_table : db=rms_1000255_pqm tbl=mrkt_dim
19/10/16 17:03:11 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=rms_1000255_pqm tbl=mrkt_dim	
19/10/16 17:03:11 WARN Utils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.debug.maxToStringFields' in SparkEnv.conf.
19/10/16 17:03:11 INFO FileSourceStrategy: Pruning directories with: 
19/10/16 17:03:11 INFO FileSourceStrategy: Post-Scan Filters: 
19/10/16 17:03:11 INFO FileSourceStrategy: Output Data Schema: struct<CLIENT_ID: int, MRKT_DSC_SHRT: string, MRKT_DSC_LNG: string, SAMS_RSTR_IND: int, CSL_BLK_DT: timestamp ... 62 more fields>
19/10/16 17:03:11 INFO FileSourceScanExec: Pushed Filters: 
19/10/16 17:03:11 INFO HiveMetaStore: 1: get_table : db=rms_1000255_pqm tbl=mrkt_dim
19/10/16 17:03:11 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=rms_1000255_pqm tbl=mrkt_dim	
19/10/16 17:03:11 INFO HiveMetaStore: 1: get_database: rms_1000255_pqm
19/10/16 17:03:11 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: rms_1000255_pqm	
19/10/16 17:03:11 INFO HiveMetaStore: 1: get_database: rms_1000255_pqm
19/10/16 17:03:11 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: rms_1000255_pqm	
19/10/16 17:03:11 DEBUG HttpTransport: HTTPRequest,Failed,cReqId:4e91a060-5272-4ff5-bf4c-a250daa51464.0,lat:28,err:HTTP404(FileNotFoundException),Reqlen:0,Resplen:268,token_ns:6700,sReqId:5e32c844-7cde-4d7b-b650-5c44f2811c2d,path:/user/hive/warehouse/RMS_1000255_PQM.db/mrkt_dim,qp:op=GETFILESTATUS&tooid=true&api-version=2017-08-01
19/10/16 17:03:11 INFO HiveMetaStore: 1: get_database: rms_1000255_pqm
19/10/16 17:03:11 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: rms_1000255_pqm	
19/10/16 17:03:11 INFO HiveMetaStore: 1: get_database: rms_1000255_pqm
19/10/16 17:03:11 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: rms_1000255_pqm	
19/10/16 17:03:11 DEBUG HttpTransport: HTTPRequest,Failed,cReqId:a72ef990-8f43-45bb-b5cf-bc965d6f46e9.0,lat:7,err:HTTP404(FileNotFoundException),Reqlen:0,Resplen:268,token_ns:6000,sReqId:f1efce7c-6cf7-4f7d-b690-87a4c9af4361,path:/user/hive/warehouse/RMS_1000255_PQM.db/mrkt_dim,qp:op=GETFILESTATUS&tooid=true&api-version=2017-08-01
19/10/16 17:03:11 INFO ParquetFileFormat: Using user defined output committer for Parquet: org.apache.spark.sql.parquet.DirectParquetOutputCommitter
19/10/16 17:03:11 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 649.4 KB, free 54.2 GB)
19/10/16 17:03:11 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 46.9 KB, free 54.2 GB)
19/10/16 17:03:11 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.139.64.54:39703 (size: 46.9 KB, free: 54.2 GB)
19/10/16 17:03:11 INFO SparkContext: Created broadcast 4 from saveAsTable at DataFrameDecorator.scala:17
19/10/16 17:03:11 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
19/10/16 17:03:11 INFO SparkContext: Starting job: saveAsTable at DataFrameDecorator.scala:17
19/10/16 17:03:11 INFO DAGScheduler: Got job 2 (saveAsTable at DataFrameDecorator.scala:17) with 1 output partitions
19/10/16 17:03:11 INFO DAGScheduler: Final stage: ResultStage 2 (saveAsTable at DataFrameDecorator.scala:17)
19/10/16 17:03:11 INFO DAGScheduler: Parents of final stage: List()
19/10/16 17:03:11 INFO DAGScheduler: Missing parents: List()
19/10/16 17:03:11 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[8] at saveAsTable at DataFrameDecorator.scala:17), which has no missing parents
19/10/16 17:03:11 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 325.1 KB, free 54.2 GB)
19/10/16 17:03:11 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 106.5 KB, free 54.2 GB)
19/10/16 17:03:11 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 10.139.64.54:39703 (size: 106.5 KB, free: 54.2 GB)
19/10/16 17:03:11 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1316
19/10/16 17:03:11 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[8] at saveAsTable at DataFrameDecorator.scala:17) (first 15 tasks are for partitions Vector(0))
19/10/16 17:03:11 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks
19/10/16 17:03:11 INFO TaskSetManager: Jars for session None: Map(spark://10.139.64.54:42381/jars/addedFile2283635635480536819io_spray_spray_json_2_11_1_3_5-d5f78.jar -> 1571245130561, spark://10.139.64.54:42381/jars/addedFile1442731520669452270dbfs__FileStore_saak9001_pqm_core_models_1_0-b0ccd.jar -> 1571245130466, spark://10.139.64.54:42381/jars/addedFile8210562299627401364org_rogach_scallop_2_11_3_3_1-ecc25.jar -> 1571245130071, spark://10.139.64.54:42381/jars/addedFile3353797914854493852xml_apis_xml_apis_1_0_b2-167e6.jar -> 1571245130253, spark://10.139.64.54:42381/jars/addedFile6473220421637755275com_typesafe_config_1_3_1-e3e42.jar -> 1571245130174, spark://10.139.64.54:42381/jars/addedFile495216498108986397dom4j_dom4j_1_6_1-3b35e.jar -> 1571245130329, spark://10.139.64.54:42381/jars/addedFile7597737351700963968dbfs__FileStore_saak9001_transengine_1_0_SNAPSHOT-ab9b9.jar -> 1571245130726, spark://10.139.64.54:42381/jars/addedFile8222637637957951783org_scalaj_scalaj_http_2_11_2_3_0-d69e4.jar -> 1571245130806)
19/10/16 17:03:11 INFO TaskSetManager: Files for session None: Map(spark://10.139.64.54:42381/files/addedFile495216498108986397dom4j_dom4j_1_6_1-3b35e.jar -> 1571245130322, spark://10.139.64.54:42381/files/addedFile6473220421637755275com_typesafe_config_1_3_1-e3e42.jar -> 1571245130165, spark://10.139.64.54:42381/files/addedFile1442731520669452270dbfs__FileStore_saak9001_pqm_core_models_1_0-b0ccd.jar -> 1571245130458, spark://10.139.64.54:42381/files/addedFile2283635635480536819io_spray_spray_json_2_11_1_3_5-d5f78.jar -> 1571245130552, spark://10.139.64.54:42381/files/addedFile8210562299627401364org_rogach_scallop_2_11_3_3_1-ecc25.jar -> 1571245130055, spark://10.139.64.54:42381/files/addedFile7597737351700963968dbfs__FileStore_saak9001_transengine_1_0_SNAPSHOT-ab9b9.jar -> 1571245130717, spark://10.139.64.54:42381/files/addedFile3353797914854493852xml_apis_xml_apis_1_0_b2-167e6.jar -> 1571245130246, spark://10.139.64.54:42381/files/addedFile8222637637957951783org_scalaj_scalaj_http_2_11_2_3_0-d69e4.jar -> 1571245130798)
19/10/16 17:03:11 INFO FairSchedulableBuilder: Added task set TaskSet_2.0 tasks to pool 6186194774562528663
19/10/16 17:03:11 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, 10.139.64.53, executor 0, partition 0, PROCESS_LOCAL, 6056 bytes)
19/10/16 17:03:11 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 10.139.64.53:43111 (size: 106.5 KB, free: 50.8 GB)
19/10/16 17:03:12 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.139.64.53:43111 (size: 46.9 KB, free: 50.8 GB)
19/10/16 17:03:13 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 2209 ms on 10.139.64.53 (executor 0) (1/1)
19/10/16 17:03:13 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 6186194774562528663
19/10/16 17:03:13 INFO DAGScheduler: ResultStage 2 (saveAsTable at DataFrameDecorator.scala:17) finished in 2.247 s
19/10/16 17:03:13 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
19/10/16 17:03:13 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
19/10/16 17:03:13 INFO DAGScheduler: Job 2 finished: saveAsTable at DataFrameDecorator.scala:17, took 2.261118 s
19/10/16 17:03:13 INFO DirectoryAtomicCommitProtocol: Committing job 6438acb3-128c-4122-a301-39942b4204b0
19/10/16 17:03:13 INFO FileSystem: FS_OP_CREATE FILE[adl://connectedsystemsuatdl.azuredatalakestore.net/user/hive/warehouse/RMS_1000255_PQM.db/mrkt_dim/_committed_7973957760692700518] Creating output stream; permission: rw-r--r--, overwrite: true, bufferSize: 65536
19/10/16 17:03:13 INFO AdlFsOutputStream: FS_OP_CREATE FILE[/user/hive/warehouse/RMS_1000255_PQM.db/mrkt_dim/_committed_7973957760692700518] Closing stream; size: 120
19/10/16 17:03:13 INFO AdlFsOutputStream: FS_OP_CREATE FILE[/user/hive/warehouse/RMS_1000255_PQM.db/mrkt_dim/_committed_7973957760692700518] Upload complete; size: 120
19/10/16 17:03:13 INFO AdlFsOutputStream: FS_OP_CREATE FILE[/user/hive/warehouse/RMS_1000255_PQM.db/mrkt_dim/_committed_7973957760692700518] Closing stream; size: 120
19/10/16 17:03:13 INFO AdlFsOutputStream: FS_OP_CREATE FILE[/user/hive/warehouse/RMS_1000255_PQM.db/mrkt_dim/_committed_7973957760692700518] Upload complete; size: 120
19/10/16 17:03:13 INFO FileSystem: FS_OP_CREATE FILE[adl://connectedsystemsuatdl.azuredatalakestore.net/user/hive/warehouse/RMS_1000255_PQM.db/mrkt_dim/_SUCCESS] Creating output stream; permission: rw-r--r--, overwrite: true, bufferSize: 65536
19/10/16 17:03:13 INFO AdlFsOutputStream: FS_OP_CREATE FILE[/user/hive/warehouse/RMS_1000255_PQM.db/mrkt_dim/_SUCCESS] Closing stream; size: 0
19/10/16 17:03:13 INFO AdlFsOutputStream: FS_OP_CREATE FILE[/user/hive/warehouse/RMS_1000255_PQM.db/mrkt_dim/_SUCCESS] Upload complete; size: 0
19/10/16 17:03:13 INFO FileSystem: FS_OP_CREATE FILE[adl://connectedsystemsuatdl.azuredatalakestore.net/user/hive/warehouse/RMS_1000255_PQM.db/mrkt_dim/_SUCCESS] Creating output stream; permission: rw-r--r--, overwrite: true, bufferSize: 65536
19/10/16 17:03:14 INFO AdlFsOutputStream: FS_OP_CREATE FILE[/user/hive/warehouse/RMS_1000255_PQM.db/mrkt_dim/_SUCCESS] Closing stream; size: 0
19/10/16 17:03:14 INFO AdlFsOutputStream: FS_OP_CREATE FILE[/user/hive/warehouse/RMS_1000255_PQM.db/mrkt_dim/_SUCCESS] Upload complete; size: 0
19/10/16 17:03:14 INFO DirectoryAtomicCommitProtocol: Job commit completed for 6438acb3-128c-4122-a301-39942b4204b0
19/10/16 17:03:14 INFO DirectoryAtomicCommitProtocol: Auto-vacuuming directories updated by 6438acb3-128c-4122-a301-39942b4204b0
19/10/16 17:03:14 INFO DirectoryAtomicCommitProtocol: Started VACUUM on List(adl://connectedsystemsuatdl.azuredatalakestore.net/user/hive/warehouse/RMS_1000255_PQM.db/mrkt_dim) with data horizon 1571072594070 (now - 48.0 hours), metadata horizon 1571243594070 (now - 0.5 hours)
19/10/16 17:03:14 WARN FileOperations: method deleteMultiple doesn't exist. Falling back to individual deletion.
19/10/16 17:03:14 INFO FileFormatWriter: Write Job 000e87fb-e4b5-4d18-a623-0c951f2ffaa1 committed.
19/10/16 17:03:14 INFO FileFormatWriter: Finished processing stats for write job 000e87fb-e4b5-4d18-a623-0c951f2ffaa1.
19/10/16 17:03:14 DEBUG HttpTransport: HTTPRequest,Failed,cReqId:10ddf37a-5f61-4092-84dd-244ab42dcbc8.0,lat:7,err:HTTP404(FileNotFoundException),Reqlen:0,Resplen:284,token_ns:4000,sReqId:c1ee4220-bb97-4914-9d5c-2bd884eb784a,path:/user/hive/warehouse/RMS_1000255_PQM.db/mrkt_dim/_spark_metadata,qp:op=GETFILESTATUS&tooid=true&api-version=2017-08-01
19/10/16 17:03:14 INFO InMemoryFileIndex: Start listing leaf files and directories. Size of Paths: 1; threshold: 32
19/10/16 17:03:14 INFO InMemoryFileIndex: Start listing leaf files and directories. Size of Paths: 0; threshold: 32
19/10/16 17:03:14 INFO HiveMetaStore: 1: get_database: rms_1000255_pqm
19/10/16 17:03:14 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: rms_1000255_pqm	
19/10/16 17:03:14 INFO HiveMetaStore: 1: get_table : db=rms_1000255_pqm tbl=mrkt_dim
19/10/16 17:03:14 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=rms_1000255_pqm tbl=mrkt_dim	
19/10/16 17:03:14 INFO HiveMetaStore: 1: get_database: rms_1000255_pqm
19/10/16 17:03:14 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: rms_1000255_pqm	
19/10/16 17:03:14 INFO HiveMetaStore: 1: get_table : db=rms_1000255_pqm tbl=mrkt_dim
19/10/16 17:03:14 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=rms_1000255_pqm tbl=mrkt_dim	
19/10/16 17:03:14 INFO SecuredHiveExternalCatalog: Persisting file based data source table `rms_1000255_pqm`.`mrkt_dim` into Hive metastore in Hive compatible format.
19/10/16 17:03:14 INFO HiveMetaStore: 1: create_table: Table(tableName:mrkt_dim, dbName:rms_1000255_pqm, owner:root, createTime:1571245391, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:CLIENT_ID, type:int, comment:null), FieldSchema(name:MRKT_DSC_SHRT, type:string, comment:null), FieldSchema(name:MRKT_DSC_LNG, type:string, comment:null), FieldSchema(name:SAMS_RSTR_IND, type:int, comment:null), FieldSchema(name:CSL_BLK_DT, type:timestamp, comment:null), FieldSchema(name:POD_RLS_IND, type:string, comment:null), FieldSchema(name:PRJCTD_IND, type:int, comment:null), FieldSchema(name:MRKT_TYP, type:string, comment:null), FieldSchema(name:MRKT_RPTBL_STRT_DT, type:timestamp, comment:null), FieldSchema(name:MRKT_RPTBL_STOP_DT, type:timestamp, comment:null), FieldSchema(name:TAGRS, type:string, comment:null), FieldSchema(name:MRKT_CD, type:string, comment:null), FieldSchema(name:ACV_ADJ_IND, type:int, comment:null), FieldSchema(name:MKT_DISP_NAME, type:string, comment:null), FieldSchema(name:MRKT_DRILL_IND, type:string, comment:null), FieldSchema(name:STR_PL_RSTR, type:int, comment:null), FieldSchema(name:WGHT_RULE_NUM, type:int, comment:null), FieldSchema(name:XDIM_MRKT2STR, type:string, comment:null), FieldSchema(name:XAOC_COMP_MKT_KEY, type:int, comment:null), FieldSchema(name:COMP_MKT_KEY, type:int, comment:null), FieldSchema(name:MRKT_KEY, type:int, comment:null), FieldSchema(name:COMP_MKT, type:string, comment:null), FieldSchema(name:XAOC_COMP_MKT, type:string, comment:null), FieldSchema(name:MARKET_TYPE, type:string, comment:null), FieldSchema(name:MRKT_RSTR_ID, type:int, comment:null), FieldSchema(name:MRKT_RSTR_POS, type:int, comment:null), FieldSchema(name:MRKT_IND, type:string, comment:null), FieldSchema(name:CHANNEL, type:string, comment:null), FieldSchema(name:SHR_MRKT_KEY, type:string, comment:null), FieldSchema(name:ETH_MKT_TYP, type:string, comment:null), FieldSchema(name:MRKT_AGG_FLG, type:int, comment:null), FieldSchema(name:DT_SRC_DTL_NM, type:string, comment:null), FieldSchema(name:SUPUP, type:string, comment:null), FieldSchema(name:COUNTRY, type:string, comment:null), FieldSchema(name:CS_MARKET_TYPE, type:string, comment:null), FieldSchema(name:CS_MARKET_SET, type:string, comment:null), FieldSchema(name:SUPPRESSION, type:string, comment:null), FieldSchema(name:DEF_MRKT_FLG, type:string, comment:null), FieldSchema(name:GEOGRAPHY, type:string, comment:null), FieldSchema(name:CS_CHANNEL, type:string, comment:null), FieldSchema(name:CS_MARKET_SET_DETAIL, type:string, comment:null), FieldSchema(name:CS_MARKET_TYPE_SET, type:string, comment:null), FieldSchema(name:CS_TA_CHANNEL, type:string, comment:null), FieldSchema(name:MRKT_TT_DMG_CD, type:int, comment:null), FieldSchema(name:MRKT_TYPE_HS, type:string, comment:null), FieldSchema(name:TA_REM, type:string, comment:null), FieldSchema(name:XAOC_TA_REM, type:string, comment:null), FieldSchema(name:KO_MARKET_PARENT_CHANNEL, type:string, comment:null), FieldSchema(name:KO_MARKET_PARENT_1, type:string, comment:null), FieldSchema(name:KO_MARKET_PARENT_2, type:string, comment:null), FieldSchema(name:KO_MARKET_PARENT_3, type:string, comment:null), FieldSchema(name:KO_MARKET_PARENT_4, type:string, comment:null), FieldSchema(name:KO_MARKET_LEVEL_CHANNEL, type:string, comment:null), FieldSchema(name:KO_MARKET_LEVEL_1, type:string, comment:null), FieldSchema(name:KO_MARKET_LEVEL_2, type:string, comment:null), FieldSchema(name:KO_MARKET_LEVEL_3, type:string, comment:null), FieldSchema(name:KO_MARKET_LEVEL_4, type:string, comment:null), FieldSchema(name:KO_MARKET_ALTERNATE_MKTS, type:string, comment:null), FieldSchema(name:KO_MARKET_ALL_CHILD_MKTS, type:string, comment:null), FieldSchema(name:CM_TA_RM_ALL, type:string, comment:null), FieldSchema(name:XAOC_CM_TA_RM_ALL, type:string, comment:null), FieldSchema(name:ODMP_DISP_NAME, type:string, comment:null), FieldSchema(name:MARKET_DISPLAY_NAME_KO, type:string, comment:null), FieldSchema(name:DYN_MRKT_FLG, type:string, comment:null)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, parameters:{path=adl://connectedsystemsuatdl.azuredatalakestore.net/user/hive/warehouse/RMS_1000255_PQM.db/mrkt_dim, serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{spark.sql.sources.schema.part.1=ta":{}},{"name":"KO_MARKET_LEVEL_4","type":"string","nullable":true,"metadata":{}},{"name":"KO_MARKET_ALTERNATE_MKTS","type":"string","nullable":true,"metadata":{}},{"name":"KO_MARKET_ALL_CHILD_MKTS","type":"string","nullable":true,"metadata":{}},{"name":"CM_TA_RM_ALL","type":"string","nullable":true,"metadata":{}},{"name":"XAOC_CM_TA_RM_ALL","type":"string","nullable":true,"metadata":{}},{"name":"ODMP_DISP_NAME","type":"string","nullable":true,"metadata":{}},{"name":"MARKET_DISPLAY_NAME_KO","type":"string","nullable":true,"metadata":{}},{"name":"DYN_MRKT_FLG","type":"string","nullable":true,"metadata":{}}]}, spark.sql.sources.schema.part.0={"type":"struct","fields":[{"name":"CLIENT_ID","type":"integer","nullable":true,"metadata":{}},{"name":"MRKT_DSC_SHRT","type":"string","nullable":true,"metadata":{}},{"name":"MRKT_DSC_LNG","type":"string","nullable":true,"metadata":{}},{"name":"SAMS_RSTR_IND","type":"integer","nullable":true,"metadata":{}},{"name":"CSL_BLK_DT","type":"timestamp","nullable":true,"metadata":{}},{"name":"POD_RLS_IND","type":"string","nullable":true,"metadata":{}},{"name":"PRJCTD_IND","type":"integer","nullable":true,"metadata":{}},{"name":"MRKT_TYP","type":"string","nullable":true,"metadata":{}},{"name":"MRKT_RPTBL_STRT_DT","type":"timestamp","nullable":true,"metadata":{}},{"name":"MRKT_RPTBL_STOP_DT","type":"timestamp","nullable":true,"metadata":{}},{"name":"TAGRS","type":"string","nullable":true,"metadata":{}},{"name":"MRKT_CD","type":"string","nullable":true,"metadata":{}},{"name":"ACV_ADJ_IND","type":"integer","nullable":true,"metadata":{}},{"name":"MKT_DISP_NAME","type":"string","nullable":true,"metadata":{}},{"name":"MRKT_DRILL_IND","type":"string","nullable":true,"metadata":{}},{"name":"STR_PL_RSTR","type":"integer","nullable":true,"metadata":{}},{"name":"WGHT_RULE_NUM","type":"integer","nullable":true,"metadata":{}},{"name":"XDIM_MRKT2STR","type":"string","nullable":true,"metadata":{}},{"name":"XAOC_COMP_MKT_KEY","type":"integer","nullable":true,"metadata":{}},{"name":"COMP_MKT_KEY","type":"integer","nullable":true,"metadata":{}},{"name":"MRKT_KEY","type":"integer","nullable":true,"metadata":{}},{"name":"COMP_MKT","type":"string","nullable":true,"metadata":{}},{"name":"XAOC_COMP_MKT","type":"string","nullable":true,"metadata":{}},{"name":"MARKET_TYPE","type":"string","nullable":true,"metadata":{}},{"name":"MRKT_RSTR_ID","type":"integer","nullable":true,"metadata":{}},{"name":"MRKT_RSTR_POS","type":"integer","nullable":true,"metadata":{}},{"name":"MRKT_IND","type":"string","nullable":true,"metadata":{}},{"name":"CHANNEL","type":"string","nullable":true,"metadata":{}},{"name":"SHR_MRKT_KEY","type":"string","nullable":true,"metadata":{}},{"name":"ETH_MKT_TYP","type":"string","nullable":true,"metadata":{}},{"name":"MRKT_AGG_FLG","type":"integer","nullable":true,"metadata":{}},{"name":"DT_SRC_DTL_NM","type":"string","nullable":true,"metadata":{}},{"name":"SUPUP","type":"string","nullable":true,"metadata":{}},{"name":"COUNTRY","type":"string","nullable":true,"metadata":{}},{"name":"CS_MARKET_TYPE","type":"string","nullable":true,"metadata":{}},{"name":"CS_MARKET_SET","type":"string","nullable":true,"metadata":{}},{"name":"SUPPRESSION","type":"string","nullable":true,"metadata":{}},{"name":"DEF_MRKT_FLG","type":"string","nullable":true,"metadata":{}},{"name":"GEOGRAPHY","type":"string","nullable":true,"metadata":{}},{"name":"CS_CHANNEL","type":"string","nullable":true,"metadata":{}},{"name":"CS_MARKET_SET_DETAIL","type":"string","nullable":true,"metadata":{}},{"name":"CS_MARKET_TYPE_SET","type":"string","nullable":true,"metadata":{}},{"name":"CS_TA_CHANNEL","type":"string","nullable":true,"metadata":{}},{"name":"MRKT_TT_DMG_CD","type":"integer","nullable":true,"metadata":{}},{"name":"MRKT_TYPE_HS","type":"string","nullable":true,"metadata":{}},{"name":"TA_REM","type":"string","nullable":true,"metadata":{}},{"name":"XAOC_TA_REM","type":"string","nullable":true,"metadata":{}},{"name":"KO_MARKET_PARENT_CHANNEL","type":"string","nullable":true,"metadata":{}},{"name":"KO_MARKET_PARENT_1","type":"string","nullable":true,"metadata":{}},{"name":"KO_MARKET_PARENT_2","type":"string","nullable":true,"metadata":{}},{"name":"KO_MARKET_PARENT_3","type":"string","nullable":true,"metadata":{}},{"name":"KO_MARKET_PARENT_4","type":"string","nullable":true,"metadata":{}},{"name":"KO_MARKET_LEVEL_CHANNEL","type":"string","nullable":true,"metadata":{}},{"name":"KO_MARKET_LEVEL_1","type":"string","nullable":true,"metadata":{}},{"name":"KO_MARKET_LEVEL_2","type":"string","nullable":true,"metadata":{}},{"name":"KO_MARKET_LEVEL_3","type":"string","nullable":true,"metada, spark.sql.sources.schema.numParts=2, spark.sql.sources.provider=parquet, spark.sql.create.version=2.4.0}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{root=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:root, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:root, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:root, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:root, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null))
19/10/16 17:03:14 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=create_table: Table(tableName:mrkt_dim, dbName:rms_1000255_pqm, owner:root, createTime:1571245391, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:CLIENT_ID, type:int, comment:null), FieldSchema(name:MRKT_DSC_SHRT, type:string, comment:null), FieldSchema(name:MRKT_DSC_LNG, type:string, comment:null), FieldSchema(name:SAMS_RSTR_IND, type:int, comment:null), FieldSchema(name:CSL_BLK_DT, type:timestamp, comment:null), FieldSchema(name:POD_RLS_IND, type:string, comment:null), FieldSchema(name:PRJCTD_IND, type:int, comment:null), FieldSchema(name:MRKT_TYP, type:string, comment:null), FieldSchema(name:MRKT_RPTBL_STRT_DT, type:timestamp, comment:null), FieldSchema(name:MRKT_RPTBL_STOP_DT, type:timestamp, comment:null), FieldSchema(name:TAGRS, type:string, comment:null), FieldSchema(name:MRKT_CD, type:string, comment:null), FieldSchema(name:ACV_ADJ_IND, type:int, comment:null), FieldSchema(name:MKT_DISP_NAME, type:string, comment:null), FieldSchema(name:MRKT_DRILL_IND, type:string, comment:null), FieldSchema(name:STR_PL_RSTR, type:int, comment:null), FieldSchema(name:WGHT_RULE_NUM, type:int, comment:null), FieldSchema(name:XDIM_MRKT2STR, type:string, comment:null), FieldSchema(name:XAOC_COMP_MKT_KEY, type:int, comment:null), FieldSchema(name:COMP_MKT_KEY, type:int, comment:null), FieldSchema(name:MRKT_KEY, type:int, comment:null), FieldSchema(name:COMP_MKT, type:string, comment:null), FieldSchema(name:XAOC_COMP_MKT, type:string, comment:null), FieldSchema(name:MARKET_TYPE, type:string, comment:null), FieldSchema(name:MRKT_RSTR_ID, type:int, comment:null), FieldSchema(name:MRKT_RSTR_POS, type:int, comment:null), FieldSchema(name:MRKT_IND, type:string, comment:null), FieldSchema(name:CHANNEL, type:string, comment:null), FieldSchema(name:SHR_MRKT_KEY, type:string, comment:null), FieldSchema(name:ETH_MKT_TYP, type:string, comment:null), FieldSchema(name:MRKT_AGG_FLG, type:int, comment:null), FieldSchema(name:DT_SRC_DTL_NM, type:string, comment:null), FieldSchema(name:SUPUP, type:string, comment:null), FieldSchema(name:COUNTRY, type:string, comment:null), FieldSchema(name:CS_MARKET_TYPE, type:string, comment:null), FieldSchema(name:CS_MARKET_SET, type:string, comment:null), FieldSchema(name:SUPPRESSION, type:string, comment:null), FieldSchema(name:DEF_MRKT_FLG, type:string, comment:null), FieldSchema(name:GEOGRAPHY, type:string, comment:null), FieldSchema(name:CS_CHANNEL, type:string, comment:null), FieldSchema(name:CS_MARKET_SET_DETAIL, type:string, comment:null), FieldSchema(name:CS_MARKET_TYPE_SET, type:string, comment:null), FieldSchema(name:CS_TA_CHANNEL, type:string, comment:null), FieldSchema(name:MRKT_TT_DMG_CD, type:int, comment:null), FieldSchema(name:MRKT_TYPE_HS, type:string, comment:null), FieldSchema(name:TA_REM, type:string, comment:null), FieldSchema(name:XAOC_TA_REM, type:string, comment:null), FieldSchema(name:KO_MARKET_PARENT_CHANNEL, type:string, comment:null), FieldSchema(name:KO_MARKET_PARENT_1, type:string, comment:null), FieldSchema(name:KO_MARKET_PARENT_2, type:string, comment:null), FieldSchema(name:KO_MARKET_PARENT_3, type:string, comment:null), FieldSchema(name:KO_MARKET_PARENT_4, type:string, comment:null), FieldSchema(name:KO_MARKET_LEVEL_CHANNEL, type:string, comment:null), FieldSchema(name:KO_MARKET_LEVEL_1, type:string, comment:null), FieldSchema(name:KO_MARKET_LEVEL_2, type:string, comment:null), FieldSchema(name:KO_MARKET_LEVEL_3, type:string, comment:null), FieldSchema(name:KO_MARKET_LEVEL_4, type:string, comment:null), FieldSchema(name:KO_MARKET_ALTERNATE_MKTS, type:string, comment:null), FieldSchema(name:KO_MARKET_ALL_CHILD_MKTS, type:string, comment:null), FieldSchema(name:CM_TA_RM_ALL, type:string, comment:null), FieldSchema(name:XAOC_CM_TA_RM_ALL, type:string, comment:null), FieldSchema(name:ODMP_DISP_NAME, type:string, comment:null), FieldSchema(name:MARKET_DISPLAY_NAME_KO, type:string, comment:null), FieldSchema(name:DYN_MRKT_FLG, type:string, comment:null)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, parameters:{path=adl://connectedsystemsuatdl.azuredatalakestore.net/user/hive/warehouse/RMS_1000255_PQM.db/mrkt_dim, serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{spark.sql.sources.schema.part.1=ta":{}},{"name":"KO_MARKET_LEVEL_4","type":"string","nullable":true,"metadata":{}},{"name":"KO_MARKET_ALTERNATE_MKTS","type":"string","nullable":true,"metadata":{}},{"name":"KO_MARKET_ALL_CHILD_MKTS","type":"string","nullable":true,"metadata":{}},{"name":"CM_TA_RM_ALL","type":"string","nullable":true,"metadata":{}},{"name":"XAOC_CM_TA_RM_ALL","type":"string","nullable":true,"metadata":{}},{"name":"ODMP_DISP_NAME","type":"string","nullable":true,"metadata":{}},{"name":"MARKET_DISPLAY_NAME_KO","type":"string","nullable":true,"metadata":{}},{"name":"DYN_MRKT_FLG","type":"string","nullable":true,"metadata":{}}]}, spark.sql.sources.schema.part.0={"type":"struct","fields":[{"name":"CLIENT_ID","type":"integer","nullable":true,"metadata":{}},{"name":"MRKT_DSC_SHRT","type":"string","nullable":true,"metadata":{}},{"name":"MRKT_DSC_LNG","type":"string","nullable":true,"metadata":{}},{"name":"SAMS_RSTR_IND","type":"integer","nullable":true,"metadata":{}},{"name":"CSL_BLK_DT","type":"timestamp","nullable":true,"metadata":{}},{"name":"POD_RLS_IND","type":"string","nullable":true,"metadata":{}},{"name":"PRJCTD_IND","type":"integer","nullable":true,"metadata":{}},{"name":"MRKT_TYP","type":"string","nullable":true,"metadata":{}},{"name":"MRKT_RPTBL_STRT_DT","type":"timestamp","nullable":true,"metadata":{}},{"name":"MRKT_RPTBL_STOP_DT","type":"timestamp","nullable":true,"metadata":{}},{"name":"TAGRS","type":"string","nullable":true,"metadata":{}},{"name":"MRKT_CD","type":"string","nullable":true,"metadata":{}},{"name":"ACV_ADJ_IND","type":"integer","nullable":true,"metadata":{}},{"name":"MKT_DISP_NAME","type":"string","nullable":true,"metadata":{}},{"name":"MRKT_DRILL_IND","type":"string","nullable":true,"metadata":{}},{"name":"STR_PL_RSTR","type":"integer","nullable":true,"metadata":{}},{"name":"WGHT_RULE_NUM","type":"integer","nullable":true,"metadata":{}},{"name":"XDIM_MRKT2STR","type":"string","nullable":true,"metadata":{}},{"name":"XAOC_COMP_MKT_KEY","type":"integer","nullable":true,"metadata":{}},{"name":"COMP_MKT_KEY","type":"integer","nullable":true,"metadata":{}},{"name":"MRKT_KEY","type":"integer","nullable":true,"metadata":{}},{"name":"COMP_MKT","type":"string","nullable":true,"metadata":{}},{"name":"XAOC_COMP_MKT","type":"string","nullable":true,"metadata":{}},{"name":"MARKET_TYPE","type":"string","nullable":true,"metadata":{}},{"name":"MRKT_RSTR_ID","type":"integer","nullable":true,"metadata":{}},{"name":"MRKT_RSTR_POS","type":"integer","nullable":true,"metadata":{}},{"name":"MRKT_IND","type":"string","nullable":true,"metadata":{}},{"name":"CHANNEL","type":"string","nullable":true,"metadata":{}},{"name":"SHR_MRKT_KEY","type":"string","nullable":true,"metadata":{}},{"name":"ETH_MKT_TYP","type":"string","nullable":true,"metadata":{}},{"name":"MRKT_AGG_FLG","type":"integer","nullable":true,"metadata":{}},{"name":"DT_SRC_DTL_NM","type":"string","nullable":true,"metadata":{}},{"name":"SUPUP","type":"string","nullable":true,"metadata":{}},{"name":"COUNTRY","type":"string","nullable":true,"metadata":{}},{"name":"CS_MARKET_TYPE","type":"string","nullable":true,"metadata":{}},{"name":"CS_MARKET_SET","type":"string","nullable":true,"metadata":{}},{"name":"SUPPRESSION","type":"string","nullable":true,"metadata":{}},{"name":"DEF_MRKT_FLG","type":"string","nullable":true,"metadata":{}},{"name":"GEOGRAPHY","type":"string","nullable":true,"metadata":{}},{"name":"CS_CHANNEL","type":"string","nullable":true,"metadata":{}},{"name":"CS_MARKET_SET_DETAIL","type":"string","nullable":true,"metadata":{}},{"name":"CS_MARKET_TYPE_SET","type":"string","nullable":true,"metadata":{}},{"name":"CS_TA_CHANNEL","type":"string","nullable":true,"metadata":{}},{"name":"MRKT_TT_DMG_CD","type":"integer","nullable":true,"metadata":{}},{"name":"MRKT_TYPE_HS","type":"string","nullable":true,"metadata":{}},{"name":"TA_REM","type":"string","nullable":true,"metadata":{}},{"name":"XAOC_TA_REM","type":"string","nullable":true,"metadata":{}},{"name":"KO_MARKET_PARENT_CHANNEL","type":"string","nullable":true,"metadata":{}},{"name":"KO_MARKET_PARENT_1","type":"string","nullable":true,"metadata":{}},{"name":"KO_MARKET_PARENT_2","type":"string","nullable":true,"metadata":{}},{"name":"KO_MARKET_PARENT_3","type":"string","nullable":true,"metadata":{}},{"name":"KO_MARKET_PARENT_4","type":"string","nullable":true,"metadata":{}},{"name":"KO_MARKET_LEVEL_CHANNEL","type":"string","nullable":true,"metadata":{}},{"name":"KO_MARKET_LEVEL_1","type":"string","nullable":true,"metadata":{}},{"name":"KO_MARKET_LEVEL_2","type":"string","nullable":true,"metadata":{}},{"name":"KO_MARKET_LEVEL_3","type":"string","nullable":true,"metada, spark.sql.sources.schema.numParts=2, spark.sql.sources.provider=parquet, spark.sql.create.version=2.4.0}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{root=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:root, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:root, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:root, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:root, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null))	
19/10/16 17:03:14 INFO FileToDimension: >>>>>>>>>> Writing table RMS_1000255_PQM.MRKT_DIM Completed.
19/10/16 17:03:14 INFO FileToDimension: FileToDimension Some(MarketCSV) for MRKT_DIM input file wasbs://teuatcontainer01@teuatstorage.blob.core.windows.net//RMS_1000255_PQM/MRKT_DIM.csv>>>>> Completed
19/10/16 17:03:14 INFO FileToDimension: FileToDimension Some(PeriodCSV) for PRD_DIM input file wasbs://teuatcontainer01@teuatstorage.blob.core.windows.net//RMS_1000255_PQM/PRD_DIM.csv>>>>> Started
19/10/16 17:03:14 INFO SQLAppStatusListener: Execution ID: 19 Total Executor Run Time: 2122
19/10/16 17:03:14 INFO InMemoryFileIndex: Start listing leaf files and directories. Size of Paths: 1; threshold: 32
19/10/16 17:03:14 INFO InMemoryFileIndex: Start listing leaf files and directories. Size of Paths: 0; threshold: 32
19/10/16 17:03:14 INFO InMemoryFileIndex: Start listing leaf files and directories. Size of Paths: 1; threshold: 32
19/10/16 17:03:14 INFO InMemoryFileIndex: Start listing leaf files and directories. Size of Paths: 0; threshold: 32
19/10/16 17:03:14 INFO FileSourceStrategy: Pruning directories with: 
19/10/16 17:03:14 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#428, None)) > 0)
19/10/16 17:03:14 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
19/10/16 17:03:14 INFO FileSourceScanExec: Pushed Filters: 
19/10/16 17:03:14 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 649.6 KB, free 54.2 GB)
19/10/16 17:03:14 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 47.0 KB, free 54.2 GB)
19/10/16 17:03:14 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 10.139.64.54:39703 (size: 47.0 KB, free: 54.2 GB)
19/10/16 17:03:14 INFO SparkContext: Created broadcast 6 from csv at FileToDimension.scala:30
19/10/16 17:03:14 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
19/10/16 17:03:14 INFO SparkContext: Starting job: csv at FileToDimension.scala:30
19/10/16 17:03:14 INFO DAGScheduler: Got job 3 (csv at FileToDimension.scala:30) with 1 output partitions
19/10/16 17:03:14 INFO DAGScheduler: Final stage: ResultStage 3 (csv at FileToDimension.scala:30)
19/10/16 17:03:14 INFO DAGScheduler: Parents of final stage: List()
19/10/16 17:03:14 INFO DAGScheduler: Missing parents: List()
19/10/16 17:03:14 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[11] at csv at FileToDimension.scala:30), which has no missing parents
19/10/16 17:03:14 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 14.1 KB, free 54.2 GB)
19/10/16 17:03:14 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 7.0 KB, free 54.2 GB)
19/10/16 17:03:15 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 10.139.64.54:39703 (size: 7.0 KB, free: 54.2 GB)
19/10/16 17:03:15 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1316
19/10/16 17:03:15 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[11] at csv at FileToDimension.scala:30) (first 15 tasks are for partitions Vector(0))
19/10/16 17:03:15 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks
19/10/16 17:03:15 INFO TaskSetManager: Jars for session None: Map(spark://10.139.64.54:42381/jars/addedFile2283635635480536819io_spray_spray_json_2_11_1_3_5-d5f78.jar -> 1571245130561, spark://10.139.64.54:42381/jars/addedFile1442731520669452270dbfs__FileStore_saak9001_pqm_core_models_1_0-b0ccd.jar -> 1571245130466, spark://10.139.64.54:42381/jars/addedFile8210562299627401364org_rogach_scallop_2_11_3_3_1-ecc25.jar -> 1571245130071, spark://10.139.64.54:42381/jars/addedFile3353797914854493852xml_apis_xml_apis_1_0_b2-167e6.jar -> 1571245130253, spark://10.139.64.54:42381/jars/addedFile6473220421637755275com_typesafe_config_1_3_1-e3e42.jar -> 1571245130174, spark://10.139.64.54:42381/jars/addedFile495216498108986397dom4j_dom4j_1_6_1-3b35e.jar -> 1571245130329, spark://10.139.64.54:42381/jars/addedFile7597737351700963968dbfs__FileStore_saak9001_transengine_1_0_SNAPSHOT-ab9b9.jar -> 1571245130726, spark://10.139.64.54:42381/jars/addedFile8222637637957951783org_scalaj_scalaj_http_2_11_2_3_0-d69e4.jar -> 1571245130806)
19/10/16 17:03:15 INFO TaskSetManager: Files for session None: Map(spark://10.139.64.54:42381/files/addedFile495216498108986397dom4j_dom4j_1_6_1-3b35e.jar -> 1571245130322, spark://10.139.64.54:42381/files/addedFile6473220421637755275com_typesafe_config_1_3_1-e3e42.jar -> 1571245130165, spark://10.139.64.54:42381/files/addedFile1442731520669452270dbfs__FileStore_saak9001_pqm_core_models_1_0-b0ccd.jar -> 1571245130458, spark://10.139.64.54:42381/files/addedFile2283635635480536819io_spray_spray_json_2_11_1_3_5-d5f78.jar -> 1571245130552, spark://10.139.64.54:42381/files/addedFile8210562299627401364org_rogach_scallop_2_11_3_3_1-ecc25.jar -> 1571245130055, spark://10.139.64.54:42381/files/addedFile7597737351700963968dbfs__FileStore_saak9001_transengine_1_0_SNAPSHOT-ab9b9.jar -> 1571245130717, spark://10.139.64.54:42381/files/addedFile3353797914854493852xml_apis_xml_apis_1_0_b2-167e6.jar -> 1571245130246, spark://10.139.64.54:42381/files/addedFile8222637637957951783org_scalaj_scalaj_http_2_11_2_3_0-d69e4.jar -> 1571245130798)
19/10/16 17:03:15 WARN FairSchedulableBuilder: A job was submitted with scheduler pool 6186194774562528663, which has not been configured. This can happen when the file that pools are read from isn't set, or when that file doesn't contain 6186194774562528663. Created 6186194774562528663 with default configuration (schedulingMode: FIFO, minShare: 0, weight: 1)
19/10/16 17:03:15 INFO FairSchedulableBuilder: Added task set TaskSet_3.0 tasks to pool 6186194774562528663
19/10/16 17:03:15 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3, 10.139.64.53, executor 0, partition 0, PROCESS_LOCAL, 6055 bytes)
19/10/16 17:03:15 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 10.139.64.53:43111 (size: 7.0 KB, free: 50.8 GB)
19/10/16 17:03:15 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 10.139.64.53:43111 (size: 47.0 KB, free: 50.8 GB)
19/10/16 17:03:15 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 223 ms on 10.139.64.53 (executor 0) (1/1)
19/10/16 17:03:15 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 6186194774562528663
19/10/16 17:03:15 INFO DAGScheduler: ResultStage 3 (csv at FileToDimension.scala:30) finished in 0.236 s
19/10/16 17:03:15 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
19/10/16 17:03:15 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
19/10/16 17:03:15 INFO DAGScheduler: Job 3 finished: csv at FileToDimension.scala:30, took 0.245244 s
19/10/16 17:03:15 INFO FileSourceStrategy: Pruning directories with: 
19/10/16 17:03:15 INFO FileSourceStrategy: Post-Scan Filters: 
19/10/16 17:03:15 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
19/10/16 17:03:15 INFO FileSourceScanExec: Pushed Filters: 
19/10/16 17:03:15 INFO SQLAppStatusListener: Execution ID: 20 Total Executor Run Time: 200
19/10/16 17:03:15 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 649.6 KB, free 54.2 GB)
19/10/16 17:03:15 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 47.0 KB, free 54.2 GB)
19/10/16 17:03:15 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 10.139.64.54:39703 (size: 47.0 KB, free: 54.2 GB)
19/10/16 17:03:15 INFO SparkContext: Created broadcast 8 from csv at FileToDimension.scala:30
19/10/16 17:03:15 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
19/10/16 17:03:15 INFO SparkContext: Starting job: csv at FileToDimension.scala:30
19/10/16 17:03:15 INFO DAGScheduler: Got job 4 (csv at FileToDimension.scala:30) with 1 output partitions
19/10/16 17:03:15 INFO DAGScheduler: Final stage: ResultStage 4 (csv at FileToDimension.scala:30)
19/10/16 17:03:15 INFO DAGScheduler: Parents of final stage: List()
19/10/16 17:03:15 INFO DAGScheduler: Missing parents: List()
19/10/16 17:03:15 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[16] at csv at FileToDimension.scala:30), which has no missing parents
19/10/16 17:03:15 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 17.9 KB, free 54.2 GB)
19/10/16 17:03:15 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 9.0 KB, free 54.2 GB)
19/10/16 17:03:15 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 10.139.64.54:39703 (size: 9.0 KB, free: 54.2 GB)
19/10/16 17:03:15 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1316
19/10/16 17:03:15 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[16] at csv at FileToDimension.scala:30) (first 15 tasks are for partitions Vector(0))
19/10/16 17:03:15 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks
19/10/16 17:03:15 INFO TaskSetManager: Jars for session None: Map(spark://10.139.64.54:42381/jars/addedFile2283635635480536819io_spray_spray_json_2_11_1_3_5-d5f78.jar -> 1571245130561, spark://10.139.64.54:42381/jars/addedFile1442731520669452270dbfs__FileStore_saak9001_pqm_core_models_1_0-b0ccd.jar -> 1571245130466, spark://10.139.64.54:42381/jars/addedFile8210562299627401364org_rogach_scallop_2_11_3_3_1-ecc25.jar -> 1571245130071, spark://10.139.64.54:42381/jars/addedFile3353797914854493852xml_apis_xml_apis_1_0_b2-167e6.jar -> 1571245130253, spark://10.139.64.54:42381/jars/addedFile6473220421637755275com_typesafe_config_1_3_1-e3e42.jar -> 1571245130174, spark://10.139.64.54:42381/jars/addedFile495216498108986397dom4j_dom4j_1_6_1-3b35e.jar -> 1571245130329, spark://10.139.64.54:42381/jars/addedFile7597737351700963968dbfs__FileStore_saak9001_transengine_1_0_SNAPSHOT-ab9b9.jar -> 1571245130726, spark://10.139.64.54:42381/jars/addedFile8222637637957951783org_scalaj_scalaj_http_2_11_2_3_0-d69e4.jar -> 1571245130806)
19/10/16 17:03:15 INFO TaskSetManager: Files for session None: Map(spark://10.139.64.54:42381/files/addedFile495216498108986397dom4j_dom4j_1_6_1-3b35e.jar -> 1571245130322, spark://10.139.64.54:42381/files/addedFile6473220421637755275com_typesafe_config_1_3_1-e3e42.jar -> 1571245130165, spark://10.139.64.54:42381/files/addedFile1442731520669452270dbfs__FileStore_saak9001_pqm_core_models_1_0-b0ccd.jar -> 1571245130458, spark://10.139.64.54:42381/files/addedFile2283635635480536819io_spray_spray_json_2_11_1_3_5-d5f78.jar -> 1571245130552, spark://10.139.64.54:42381/files/addedFile8210562299627401364org_rogach_scallop_2_11_3_3_1-ecc25.jar -> 1571245130055, spark://10.139.64.54:42381/files/addedFile7597737351700963968dbfs__FileStore_saak9001_transengine_1_0_SNAPSHOT-ab9b9.jar -> 1571245130717, spark://10.139.64.54:42381/files/addedFile3353797914854493852xml_apis_xml_apis_1_0_b2-167e6.jar -> 1571245130246, spark://10.139.64.54:42381/files/addedFile8222637637957951783org_scalaj_scalaj_http_2_11_2_3_0-d69e4.jar -> 1571245130798)
19/10/16 17:03:15 INFO FairSchedulableBuilder: Added task set TaskSet_4.0 tasks to pool 6186194774562528663
19/10/16 17:03:15 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4, 10.139.64.53, executor 0, partition 0, PROCESS_LOCAL, 6055 bytes)
19/10/16 17:03:15 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 10.139.64.53:43111 (size: 9.0 KB, free: 50.8 GB)
19/10/16 17:03:15 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 10.139.64.53:43111 (size: 47.0 KB, free: 50.8 GB)
19/10/16 17:03:15 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 236 ms on 10.139.64.53 (executor 0) (1/1)
19/10/16 17:03:15 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 6186194774562528663
19/10/16 17:03:15 INFO DAGScheduler: ResultStage 4 (csv at FileToDimension.scala:30) finished in 0.245 s
19/10/16 17:03:15 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
19/10/16 17:03:15 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
19/10/16 17:03:15 INFO DAGScheduler: Job 4 finished: csv at FileToDimension.scala:30, took 0.253154 s
19/10/16 17:03:15 INFO FileToDimension: >>>>>>>>>> Writing table RMS_1000255_PQM.PRD_DIM Started.
19/10/16 17:03:15 INFO HiveMetaStore: 1: get_table : db=rms_1000255_pqm tbl=prd_dim
19/10/16 17:03:15 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=rms_1000255_pqm tbl=prd_dim	
19/10/16 17:03:15 INFO FileSourceStrategy: Pruning directories with: 
19/10/16 17:03:15 INFO FileSourceStrategy: Post-Scan Filters: 
19/10/16 17:03:15 INFO FileSourceStrategy: Output Data Schema: struct<QUARTERS: string, QUARTER_IN_YEAR_DSC: string, HYTD: string, PRD_SP: int, WEEKS12: string ... 69 more fields>
19/10/16 17:03:15 INFO FileSourceScanExec: Pushed Filters: 
19/10/16 17:03:15 INFO HiveMetaStore: 1: get_table : db=rms_1000255_pqm tbl=prd_dim
19/10/16 17:03:15 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=rms_1000255_pqm tbl=prd_dim	
19/10/16 17:03:15 INFO HiveMetaStore: 1: get_database: rms_1000255_pqm
19/10/16 17:03:15 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: rms_1000255_pqm	
19/10/16 17:03:15 INFO HiveMetaStore: 1: get_database: rms_1000255_pqm
19/10/16 17:03:15 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: rms_1000255_pqm	
19/10/16 17:03:15 DEBUG HttpTransport: HTTPRequest,Failed,cReqId:8a43c056-826b-4fd5-b5e1-a3d1e43b2a3b.0,lat:9,err:HTTP404(FileNotFoundException),Reqlen:0,Resplen:267,token_ns:5400,sReqId:2abb84f8-6f5d-482e-85c9-9c056255ccdc,path:/user/hive/warehouse/RMS_1000255_PQM.db/prd_dim,qp:op=GETFILESTATUS&tooid=true&api-version=2017-08-01
19/10/16 17:03:15 INFO HiveMetaStore: 1: get_database: rms_1000255_pqm
19/10/16 17:03:15 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: rms_1000255_pqm	
19/10/16 17:03:15 INFO HiveMetaStore: 1: get_database: rms_1000255_pqm
19/10/16 17:03:15 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: rms_1000255_pqm	
19/10/16 17:03:15 DEBUG HttpTransport: HTTPRequest,Failed,cReqId:08a712ad-5381-442b-848c-03651e6748d4.0,lat:7,err:HTTP404(FileNotFoundException),Reqlen:0,Resplen:267,token_ns:4900,sReqId:3237a6dd-b66b-4c95-ac26-a01184e40d64,path:/user/hive/warehouse/RMS_1000255_PQM.db/prd_dim,qp:op=GETFILESTATUS&tooid=true&api-version=2017-08-01
19/10/16 17:03:15 INFO ParquetFileFormat: Using user defined output committer for Parquet: org.apache.spark.sql.parquet.DirectParquetOutputCommitter
19/10/16 17:03:15 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 649.4 KB, free 54.2 GB)
19/10/16 17:03:15 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 46.9 KB, free 54.2 GB)
19/10/16 17:03:15 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 10.139.64.54:39703 (size: 46.9 KB, free: 54.2 GB)
19/10/16 17:03:15 INFO SparkContext: Created broadcast 10 from saveAsTable at DataFrameDecorator.scala:17
19/10/16 17:03:15 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
19/10/16 17:03:15 INFO SparkContext: Starting job: saveAsTable at DataFrameDecorator.scala:17
19/10/16 17:03:15 INFO DAGScheduler: Got job 5 (saveAsTable at DataFrameDecorator.scala:17) with 1 output partitions
19/10/16 17:03:15 INFO DAGScheduler: Final stage: ResultStage 5 (saveAsTable at DataFrameDecorator.scala:17)
19/10/16 17:03:15 INFO DAGScheduler: Parents of final stage: List()
19/10/16 17:03:15 INFO DAGScheduler: Missing parents: List()
19/10/16 17:03:15 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[18] at saveAsTable at DataFrameDecorator.scala:17), which has no missing parents
19/10/16 17:03:15 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 326.2 KB, free 54.2 GB)
19/10/16 17:03:15 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 106.7 KB, free 54.2 GB)
19/10/16 17:03:15 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 10.139.64.54:39703 (size: 106.7 KB, free: 54.2 GB)
19/10/16 17:03:15 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1316
19/10/16 17:03:15 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[18] at saveAsTable at DataFrameDecorator.scala:17) (first 15 tasks are for partitions Vector(0))
19/10/16 17:03:15 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks
19/10/16 17:03:15 INFO TaskSetManager: Jars for session None: Map(spark://10.139.64.54:42381/jars/addedFile2283635635480536819io_spray_spray_json_2_11_1_3_5-d5f78.jar -> 1571245130561, spark://10.139.64.54:42381/jars/addedFile1442731520669452270dbfs__FileStore_saak9001_pqm_core_models_1_0-b0ccd.jar -> 1571245130466, spark://10.139.64.54:42381/jars/addedFile8210562299627401364org_rogach_scallop_2_11_3_3_1-ecc25.jar -> 1571245130071, spark://10.139.64.54:42381/jars/addedFile3353797914854493852xml_apis_xml_apis_1_0_b2-167e6.jar -> 1571245130253, spark://10.139.64.54:42381/jars/addedFile6473220421637755275com_typesafe_config_1_3_1-e3e42.jar -> 1571245130174, spark://10.139.64.54:42381/jars/addedFile495216498108986397dom4j_dom4j_1_6_1-3b35e.jar -> 1571245130329, spark://10.139.64.54:42381/jars/addedFile7597737351700963968dbfs__FileStore_saak9001_transengine_1_0_SNAPSHOT-ab9b9.jar -> 1571245130726, spark://10.139.64.54:42381/jars/addedFile8222637637957951783org_scalaj_scalaj_http_2_11_2_3_0-d69e4.jar -> 1571245130806)
19/10/16 17:03:15 INFO TaskSetManager: Files for session None: Map(spark://10.139.64.54:42381/files/addedFile495216498108986397dom4j_dom4j_1_6_1-3b35e.jar -> 1571245130322, spark://10.139.64.54:42381/files/addedFile6473220421637755275com_typesafe_config_1_3_1-e3e42.jar -> 1571245130165, spark://10.139.64.54:42381/files/addedFile1442731520669452270dbfs__FileStore_saak9001_pqm_core_models_1_0-b0ccd.jar -> 1571245130458, spark://10.139.64.54:42381/files/addedFile2283635635480536819io_spray_spray_json_2_11_1_3_5-d5f78.jar -> 1571245130552, spark://10.139.64.54:42381/files/addedFile8210562299627401364org_rogach_scallop_2_11_3_3_1-ecc25.jar -> 1571245130055, spark://10.139.64.54:42381/files/addedFile7597737351700963968dbfs__FileStore_saak9001_transengine_1_0_SNAPSHOT-ab9b9.jar -> 1571245130717, spark://10.139.64.54:42381/files/addedFile3353797914854493852xml_apis_xml_apis_1_0_b2-167e6.jar -> 1571245130246, spark://10.139.64.54:42381/files/addedFile8222637637957951783org_scalaj_scalaj_http_2_11_2_3_0-d69e4.jar -> 1571245130798)
19/10/16 17:03:15 INFO FairSchedulableBuilder: Added task set TaskSet_5.0 tasks to pool 6186194774562528663
19/10/16 17:03:15 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5, 10.139.64.53, executor 0, partition 0, PROCESS_LOCAL, 6056 bytes)
19/10/16 17:03:15 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 10.139.64.53:43111 (size: 106.7 KB, free: 50.8 GB)
19/10/16 17:03:16 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 10.139.64.53:43111 (size: 46.9 KB, free: 50.8 GB)
19/10/16 17:03:16 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 693 ms on 10.139.64.53 (executor 0) (1/1)
19/10/16 17:03:16 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 6186194774562528663
19/10/16 17:03:16 INFO DAGScheduler: ResultStage 5 (saveAsTable at DataFrameDecorator.scala:17) finished in 0.724 s
19/10/16 17:03:16 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
19/10/16 17:03:16 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
19/10/16 17:03:16 INFO DAGScheduler: Job 5 finished: saveAsTable at DataFrameDecorator.scala:17, took 0.732143 s
19/10/16 17:03:16 INFO DirectoryAtomicCommitProtocol: Committing job f30fd8d1-546b-42c3-b9eb-7a8ce0301a34
19/10/16 17:03:16 INFO FileSystem: FS_OP_CREATE FILE[adl://connectedsystemsuatdl.azuredatalakestore.net/user/hive/warehouse/RMS_1000255_PQM.db/prd_dim/_committed_8965702874608004034] Creating output stream; permission: rw-r--r--, overwrite: true, bufferSize: 65536
19/10/16 17:03:16 INFO AdlFsOutputStream: FS_OP_CREATE FILE[/user/hive/warehouse/RMS_1000255_PQM.db/prd_dim/_committed_8965702874608004034] Closing stream; size: 120
19/10/16 17:03:16 INFO HiveMetaStore: 2: get_database: default
19/10/16 17:03:16 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
19/10/16 17:03:16 WARN HiveConf: HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist
19/10/16 17:03:16 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
19/10/16 17:03:16 INFO AdlFsOutputStream: FS_OP_CREATE FILE[/user/hive/warehouse/RMS_1000255_PQM.db/prd_dim/_committed_8965702874608004034] Upload complete; size: 120
19/10/16 17:03:16 INFO AdlFsOutputStream: FS_OP_CREATE FILE[/user/hive/warehouse/RMS_1000255_PQM.db/prd_dim/_committed_8965702874608004034] Closing stream; size: 120
19/10/16 17:03:16 INFO AdlFsOutputStream: FS_OP_CREATE FILE[/user/hive/warehouse/RMS_1000255_PQM.db/prd_dim/_committed_8965702874608004034] Upload complete; size: 120
19/10/16 17:03:16 INFO FileSystem: FS_OP_CREATE FILE[adl://connectedsystemsuatdl.azuredatalakestore.net/user/hive/warehouse/RMS_1000255_PQM.db/prd_dim/_SUCCESS] Creating output stream; permission: rw-r--r--, overwrite: true, bufferSize: 65536
19/10/16 17:03:16 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
19/10/16 17:03:16 INFO HiveMetaStore: 2: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
19/10/16 17:03:16 INFO ObjectStore: ObjectStore, initialize called
19/10/16 17:03:16 INFO DriverCorral: DBFS health check ok
19/10/16 17:03:16 INFO AdlFsOutputStream: FS_OP_CREATE FILE[/user/hive/warehouse/RMS_1000255_PQM.db/prd_dim/_SUCCESS] Closing stream; size: 0
19/10/16 17:03:16 INFO AdlFsOutputStream: FS_OP_CREATE FILE[/user/hive/warehouse/RMS_1000255_PQM.db/prd_dim/_SUCCESS] Upload complete; size: 0
19/10/16 17:03:16 INFO FileSystem: FS_OP_CREATE FILE[adl://connectedsystemsuatdl.azuredatalakestore.net/user/hive/warehouse/RMS_1000255_PQM.db/prd_dim/_SUCCESS] Creating output stream; permission: rw-r--r--, overwrite: true, bufferSize: 65536
19/10/16 17:03:16 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is POSTGRES
19/10/16 17:03:16 INFO ObjectStore: Initialized ObjectStore
19/10/16 17:03:16 INFO DriverCorral: Metastore health check ok
19/10/16 17:03:16 INFO AdlFsOutputStream: FS_OP_CREATE FILE[/user/hive/warehouse/RMS_1000255_PQM.db/prd_dim/_SUCCESS] Closing stream; size: 0
19/10/16 17:03:16 INFO AdlFsOutputStream: FS_OP_CREATE FILE[/user/hive/warehouse/RMS_1000255_PQM.db/prd_dim/_SUCCESS] Upload complete; size: 0
19/10/16 17:03:16 INFO DirectoryAtomicCommitProtocol: Job commit completed for f30fd8d1-546b-42c3-b9eb-7a8ce0301a34
19/10/16 17:03:16 INFO DirectoryAtomicCommitProtocol: Auto-vacuuming directories updated by f30fd8d1-546b-42c3-b9eb-7a8ce0301a34
19/10/16 17:03:16 INFO DirectoryAtomicCommitProtocol: Started VACUUM on List(adl://connectedsystemsuatdl.azuredatalakestore.net/user/hive/warehouse/RMS_1000255_PQM.db/prd_dim) with data horizon 1571072596753 (now - 48.0 hours), metadata horizon 1571243596753 (now - 0.5 hours)
19/10/16 17:03:16 WARN FileOperations: method deleteMultiple doesn't exist. Falling back to individual deletion.
19/10/16 17:03:16 INFO FileFormatWriter: Write Job 41e3029b-c8e9-4811-8db4-8c098cf63bfd committed.
19/10/16 17:03:16 INFO FileFormatWriter: Finished processing stats for write job 41e3029b-c8e9-4811-8db4-8c098cf63bfd.
19/10/16 17:03:16 DEBUG HttpTransport: HTTPRequest,Failed,cReqId:8f306bf2-3da1-4656-b96b-7f93e2805f92.0,lat:9,err:HTTP404(FileNotFoundException),Reqlen:0,Resplen:283,token_ns:3000,sReqId:e3324734-71df-413d-be94-4e1433390b62,path:/user/hive/warehouse/RMS_1000255_PQM.db/prd_dim/_spark_metadata,qp:op=GETFILESTATUS&tooid=true&api-version=2017-08-01
19/10/16 17:03:16 INFO InMemoryFileIndex: Start listing leaf files and directories. Size of Paths: 1; threshold: 32
19/10/16 17:03:17 INFO InMemoryFileIndex: Start listing leaf files and directories. Size of Paths: 0; threshold: 32
19/10/16 17:03:17 INFO HiveMetaStore: 1: get_database: rms_1000255_pqm
19/10/16 17:03:17 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: rms_1000255_pqm	
19/10/16 17:03:17 INFO HiveMetaStore: 1: get_table : db=rms_1000255_pqm tbl=prd_dim
19/10/16 17:03:17 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=rms_1000255_pqm tbl=prd_dim	
19/10/16 17:03:17 INFO HiveMetaStore: 1: get_database: rms_1000255_pqm
19/10/16 17:03:17 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: rms_1000255_pqm	
19/10/16 17:03:17 INFO HiveMetaStore: 1: get_table : db=rms_1000255_pqm tbl=prd_dim
19/10/16 17:03:17 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=rms_1000255_pqm tbl=prd_dim	
19/10/16 17:03:17 INFO SecuredHiveExternalCatalog: Persisting file based data source table `rms_1000255_pqm`.`prd_dim` into Hive metastore in Hive compatible format.
19/10/16 17:03:17 INFO HiveMetaStore: 1: create_table: Table(tableName:prd_dim, dbName:rms_1000255_pqm, owner:root, createTime:1571245395, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:QUARTERS, type:string, comment:null), FieldSchema(name:QUARTER_IN_YEAR_DSC, type:string, comment:null), FieldSchema(name:HYTD, type:string, comment:null), FieldSchema(name:PRD_SP, type:int, comment:null), FieldSchema(name:WEEKS12, type:string, comment:null), FieldSchema(name:QTD, type:string, comment:null), FieldSchema(name:PRD_DSP_ORDER, type:int, comment:null), FieldSchema(name:MONTHS6, type:string, comment:null), FieldSchema(name:PRD_RLS_IND, type:string, comment:null), FieldSchema(name:HOLIDAY_PERIODS, type:string, comment:null), FieldSchema(name:WEEKS4, type:string, comment:null), FieldSchema(name:YEAR_ORD, type:int, comment:null), FieldSchema(name:YTD, type:string, comment:null), FieldSchema(name:WEEKS24, type:string, comment:null), FieldSchema(name:PMONTH_DSC, type:string, comment:null), FieldSchema(name:FQUARTER_IN_YEAR_DSC, type:string, comment:null), FieldSchema(name:PYEAR_ORD, type:int, comment:null), FieldSchema(name:PWEEK_ORD, type:int, comment:null), FieldSchema(name:PRD_KEY, type:int, comment:null), FieldSchema(name:WEEK_ORD, type:int, comment:null), FieldSchema(name:MONTHS3, type:string, comment:null), FieldSchema(name:CUME_RUL_TYP, type:string, comment:null), FieldSchema(name:PRD_ID, type:int, comment:null), FieldSchema(name:MONTHLY, type:string, comment:null), FieldSchema(name:MONTHS12, type:string, comment:null), FieldSchema(name:RPT_MONTHS, type:string, comment:null), FieldSchema(name:PYEAR_DSC, type:string, comment:null), FieldSchema(name:HALF_YEARS_RTA, type:string, comment:null), FieldSchema(name:WK_ENDING_DSC, type:string, comment:null), FieldSchema(name:MONTHS, type:string, comment:null), FieldSchema(name:PRD_DSC_SHORT, type:string, comment:null), FieldSchema(name:SQUARTER_IN_YEAR_DSC, type:string, comment:null), FieldSchema(name:PRD_DSC_LONG, type:string, comment:null), FieldSchema(name:PPERIOD_DSC, type:string, comment:null), FieldSchema(name:PCKG_ID, type:int, comment:null), FieldSchema(name:PWEEK_DSC, type:string, comment:null), FieldSchema(name:PPERIOD_ORD, type:int, comment:null), FieldSchema(name:SWEEK_IN_YEAR_DSC, type:string, comment:null), FieldSchema(name:CUME_END_DT, type:timestamp, comment:null), FieldSchema(name:MONTH_IN_QUARTER_DSC, type:string, comment:null), FieldSchema(name:PRD_DET_LVL, type:string, comment:null), FieldSchema(name:SMONTH_IN_QUARTER_DSC, type:string, comment:null), FieldSchema(name:PRD_ID_YAGO, type:int, comment:null), FieldSchema(name:WEEKS52, type:string, comment:null), FieldSchema(name:CURRENT_PERIODS, type:string, comment:null), FieldSchema(name:WEEK_IN_MONTH_DSC, type:string, comment:null), FieldSchema(name:WEEKS26, type:string, comment:null), FieldSchema(name:MONTH_ORD, type:int, comment:null), FieldSchema(name:PQUARTER_DSC, type:string, comment:null), FieldSchema(name:FILTER_NAME, type:string, comment:null), FieldSchema(name:PRD_DSP_FOLDER, type:string, comment:null), FieldSchema(name:PRD_SRC_ID, type:int, comment:null), FieldSchema(name:WEEKS4_ORD, type:int, comment:null), FieldSchema(name:DIM_TYP, type:string, comment:null), FieldSchema(name:HALF_YEARS, type:string, comment:null), FieldSchema(name:PQUARTER_ORD, type:int, comment:null), FieldSchema(name:MONTHS1, type:string, comment:null), FieldSchema(name:HOLIDAYS, type:string, comment:null), FieldSchema(name:FYEAR_ORD, type:int, comment:null), FieldSchema(name:PRD_TAG, type:string, comment:null), FieldSchema(name:WEEKS13, type:string, comment:null), FieldSchema(name:SYEAR_DSC, type:string, comment:null), FieldSchema(name:SPERIOD_DSC, type:string, comment:null), FieldSchema(name:NUM_OF_WKS, type:int, comment:null), FieldSchema(name:WK_ENDING_DT, type:timestamp, comment:null), FieldSchema(name:YEAR_DSC, type:string, comment:null), FieldSchema(name:QUARTER_ORD, type:int, comment:null), FieldSchema(name:PERIOD_TO_DATE, type:string, comment:null), FieldSchema(name:DSP_FOLDER_AD, type:string, comment:null), FieldSchema(name:FYEAR_DSC, type:string, comment:null), FieldSchema(name:QUARTERS_RTA, type:string, comment:null)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, parameters:{path=adl://connectedsystemsuatdl.azuredatalakestore.net/user/hive/warehouse/RMS_1000255_PQM.db/prd_dim, serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{spark.sql.sources.schema.part.1=adata":{}},{"name":"FYEAR_ORD","type":"integer","nullable":true,"metadata":{}},{"name":"PRD_TAG","type":"string","nullable":true,"metadata":{}},{"name":"WEEKS13","type":"string","nullable":true,"metadata":{}},{"name":"SYEAR_DSC","type":"string","nullable":true,"metadata":{}},{"name":"SPERIOD_DSC","type":"string","nullable":true,"metadata":{}},{"name":"NUM_OF_WKS","type":"integer","nullable":true,"metadata":{}},{"name":"WK_ENDING_DT","type":"timestamp","nullable":true,"metadata":{}},{"name":"YEAR_DSC","type":"string","nullable":true,"metadata":{}},{"name":"QUARTER_ORD","type":"integer","nullable":true,"metadata":{}},{"name":"PERIOD_TO_DATE","type":"string","nullable":true,"metadata":{}},{"name":"DSP_FOLDER_AD","type":"string","nullable":true,"metadata":{}},{"name":"FYEAR_DSC","type":"string","nullable":true,"metadata":{}},{"name":"QUARTERS_RTA","type":"string","nullable":true,"metadata":{}}]}, spark.sql.sources.schema.part.0={"type":"struct","fields":[{"name":"QUARTERS","type":"string","nullable":true,"metadata":{}},{"name":"QUARTER_IN_YEAR_DSC","type":"string","nullable":true,"metadata":{}},{"name":"HYTD","type":"string","nullable":true,"metadata":{}},{"name":"PRD_SP","type":"integer","nullable":true,"metadata":{}},{"name":"WEEKS12","type":"string","nullable":true,"metadata":{}},{"name":"QTD","type":"string","nullable":true,"metadata":{}},{"name":"PRD_DSP_ORDER","type":"integer","nullable":true,"metadata":{}},{"name":"MONTHS6","type":"string","nullable":true,"metadata":{}},{"name":"PRD_RLS_IND","type":"string","nullable":true,"metadata":{}},{"name":"HOLIDAY_PERIODS","type":"string","nullable":true,"metadata":{}},{"name":"WEEKS4","type":"string","nullable":true,"metadata":{}},{"name":"YEAR_ORD","type":"integer","nullable":true,"metadata":{}},{"name":"YTD","type":"string","nullable":true,"metadata":{}},{"name":"WEEKS24","type":"string","nullable":true,"metadata":{}},{"name":"PMONTH_DSC","type":"string","nullable":true,"metadata":{}},{"name":"FQUARTER_IN_YEAR_DSC","type":"string","nullable":true,"metadata":{}},{"name":"PYEAR_ORD","type":"integer","nullable":true,"metadata":{}},{"name":"PWEEK_ORD","type":"integer","nullable":true,"metadata":{}},{"name":"PRD_KEY","type":"integer","nullable":true,"metadata":{}},{"name":"WEEK_ORD","type":"integer","nullable":true,"metadata":{}},{"name":"MONTHS3","type":"string","nullable":true,"metadata":{}},{"name":"CUME_RUL_TYP","type":"string","nullable":true,"metadata":{}},{"name":"PRD_ID","type":"integer","nullable":true,"metadata":{}},{"name":"MONTHLY","type":"string","nullable":true,"metadata":{}},{"name":"MONTHS12","type":"string","nullable":true,"metadata":{}},{"name":"RPT_MONTHS","type":"string","nullable":true,"metadata":{}},{"name":"PYEAR_DSC","type":"string","nullable":true,"metadata":{}},{"name":"HALF_YEARS_RTA","type":"string","nullable":true,"metadata":{}},{"name":"WK_ENDING_DSC","type":"string","nullable":true,"metadata":{}},{"name":"MONTHS","type":"string","nullable":true,"metadata":{}},{"name":"PRD_DSC_SHORT","type":"string","nullable":true,"metadata":{}},{"name":"SQUARTER_IN_YEAR_DSC","type":"string","nullable":true,"metadata":{}},{"name":"PRD_DSC_LONG","type":"string","nullable":true,"metadata":{}},{"name":"PPERIOD_DSC","type":"string","nullable":true,"metadata":{}},{"name":"PCKG_ID","type":"integer","nullable":true,"metadata":{}},{"name":"PWEEK_DSC","type":"string","nullable":true,"metadata":{}},{"name":"PPERIOD_ORD","type":"integer","nullable":true,"metadata":{}},{"name":"SWEEK_IN_YEAR_DSC","type":"string","nullable":true,"metadata":{}},{"name":"CUME_END_DT","type":"timestamp","nullable":true,"metadata":{}},{"name":"MONTH_IN_QUARTER_DSC","type":"string","nullable":true,"metadata":{}},{"name":"PRD_DET_LVL","type":"string","nullable":true,"metadata":{}},{"name":"SMONTH_IN_QUARTER_DSC","type":"string","nullable":true,"metadata":{}},{"name":"PRD_ID_YAGO","type":"integer","nullable":true,"metadata":{}},{"name":"WEEKS52","type":"string","nullable":true,"metadata":{}},{"name":"CURRENT_PERIODS","type":"string","nullable":true,"metadata":{}},{"name":"WEEK_IN_MONTH_DSC","type":"string","nullable":true,"metadata":{}},{"name":"WEEKS26","type":"string","nullable":true,"metadata":{}},{"name":"MONTH_ORD","type":"integer","nullable":true,"metadata":{}},{"name":"PQUARTER_DSC","type":"string","nullable":true,"metadata":{}},{"name":"FILTER_NAME","type":"string","nullable":true,"metadata":{}},{"name":"PRD_DSP_FOLDER","type":"string","nullable":true,"metadata":{}},{"name":"PRD_SRC_ID","type":"integer","nullable":true,"metadata":{}},{"name":"WEEKS4_ORD","type":"integer","nullable":true,"metadata":{}},{"name":"DIM_TYP","type":"string","nullable":true,"metadata":{}},{"name":"HALF_YEARS","type":"string","nullable":true,"metadata":{}},{"name":"PQUARTER_ORD","type":"integer","nullable":true,"metadata":{}},{"name":"MONTHS1","type":"string","nullable":true,"metadata":{}},{"name":"HOLIDAYS","type":"string","nullable":true,"met, spark.sql.sources.schema.numParts=2, spark.sql.sources.provider=parquet, spark.sql.create.version=2.4.0}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{root=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:root, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:root, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:root, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:root, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null))
19/10/16 17:03:17 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=create_table: Table(tableName:prd_dim, dbName:rms_1000255_pqm, owner:root, createTime:1571245395, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:QUARTERS, type:string, comment:null), FieldSchema(name:QUARTER_IN_YEAR_DSC, type:string, comment:null), FieldSchema(name:HYTD, type:string, comment:null), FieldSchema(name:PRD_SP, type:int, comment:null), FieldSchema(name:WEEKS12, type:string, comment:null), FieldSchema(name:QTD, type:string, comment:null), FieldSchema(name:PRD_DSP_ORDER, type:int, comment:null), FieldSchema(name:MONTHS6, type:string, comment:null), FieldSchema(name:PRD_RLS_IND, type:string, comment:null), FieldSchema(name:HOLIDAY_PERIODS, type:string, comment:null), FieldSchema(name:WEEKS4, type:string, comment:null), FieldSchema(name:YEAR_ORD, type:int, comment:null), FieldSchema(name:YTD, type:string, comment:null), FieldSchema(name:WEEKS24, type:string, comment:null), FieldSchema(name:PMONTH_DSC, type:string, comment:null), FieldSchema(name:FQUARTER_IN_YEAR_DSC, type:string, comment:null), FieldSchema(name:PYEAR_ORD, type:int, comment:null), FieldSchema(name:PWEEK_ORD, type:int, comment:null), FieldSchema(name:PRD_KEY, type:int, comment:null), FieldSchema(name:WEEK_ORD, type:int, comment:null), FieldSchema(name:MONTHS3, type:string, comment:null), FieldSchema(name:CUME_RUL_TYP, type:string, comment:null), FieldSchema(name:PRD_ID, type:int, comment:null), FieldSchema(name:MONTHLY, type:string, comment:null), FieldSchema(name:MONTHS12, type:string, comment:null), FieldSchema(name:RPT_MONTHS, type:string, comment:null), FieldSchema(name:PYEAR_DSC, type:string, comment:null), FieldSchema(name:HALF_YEARS_RTA, type:string, comment:null), FieldSchema(name:WK_ENDING_DSC, type:string, comment:null), FieldSchema(name:MONTHS, type:string, comment:null), FieldSchema(name:PRD_DSC_SHORT, type:string, comment:null), FieldSchema(name:SQUARTER_IN_YEAR_DSC, type:string, comment:null), FieldSchema(name:PRD_DSC_LONG, type:string, comment:null), FieldSchema(name:PPERIOD_DSC, type:string, comment:null), FieldSchema(name:PCKG_ID, type:int, comment:null), FieldSchema(name:PWEEK_DSC, type:string, comment:null), FieldSchema(name:PPERIOD_ORD, type:int, comment:null), FieldSchema(name:SWEEK_IN_YEAR_DSC, type:string, comment:null), FieldSchema(name:CUME_END_DT, type:timestamp, comment:null), FieldSchema(name:MONTH_IN_QUARTER_DSC, type:string, comment:null), FieldSchema(name:PRD_DET_LVL, type:string, comment:null), FieldSchema(name:SMONTH_IN_QUARTER_DSC, type:string, comment:null), FieldSchema(name:PRD_ID_YAGO, type:int, comment:null), FieldSchema(name:WEEKS52, type:string, comment:null), FieldSchema(name:CURRENT_PERIODS, type:string, comment:null), FieldSchema(name:WEEK_IN_MONTH_DSC, type:string, comment:null), FieldSchema(name:WEEKS26, type:string, comment:null), FieldSchema(name:MONTH_ORD, type:int, comment:null), FieldSchema(name:PQUARTER_DSC, type:string, comment:null), FieldSchema(name:FILTER_NAME, type:string, comment:null), FieldSchema(name:PRD_DSP_FOLDER, type:string, comment:null), FieldSchema(name:PRD_SRC_ID, type:int, comment:null), FieldSchema(name:WEEKS4_ORD, type:int, comment:null), FieldSchema(name:DIM_TYP, type:string, comment:null), FieldSchema(name:HALF_YEARS, type:string, comment:null), FieldSchema(name:PQUARTER_ORD, type:int, comment:null), FieldSchema(name:MONTHS1, type:string, comment:null), FieldSchema(name:HOLIDAYS, type:string, comment:null), FieldSchema(name:FYEAR_ORD, type:int, comment:null), FieldSchema(name:PRD_TAG, type:string, comment:null), FieldSchema(name:WEEKS13, type:string, comment:null), FieldSchema(name:SYEAR_DSC, type:string, comment:null), FieldSchema(name:SPERIOD_DSC, type:string, comment:null), FieldSchema(name:NUM_OF_WKS, type:int, comment:null), FieldSchema(name:WK_ENDING_DT, type:timestamp, comment:null), FieldSchema(name:YEAR_DSC, type:string, comment:null), FieldSchema(name:QUARTER_ORD, type:int, comment:null), FieldSchema(name:PERIOD_TO_DATE, type:string, comment:null), FieldSchema(name:DSP_FOLDER_AD, type:string, comment:null), FieldSchema(name:FYEAR_DSC, type:string, comment:null), FieldSchema(name:QUARTERS_RTA, type:string, comment:null)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, parameters:{path=adl://connectedsystemsuatdl.azuredatalakestore.net/user/hive/warehouse/RMS_1000255_PQM.db/prd_dim, serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{spark.sql.sources.schema.part.1=adata":{}},{"name":"FYEAR_ORD","type":"integer","nullable":true,"metadata":{}},{"name":"PRD_TAG","type":"string","nullable":true,"metadata":{}},{"name":"WEEKS13","type":"string","nullable":true,"metadata":{}},{"name":"SYEAR_DSC","type":"string","nullable":true,"metadata":{}},{"name":"SPERIOD_DSC","type":"string","nullable":true,"metadata":{}},{"name":"NUM_OF_WKS","type":"integer","nullable":true,"metadata":{}},{"name":"WK_ENDING_DT","type":"timestamp","nullable":true,"metadata":{}},{"name":"YEAR_DSC","type":"string","nullable":true,"metadata":{}},{"name":"QUARTER_ORD","type":"integer","nullable":true,"metadata":{}},{"name":"PERIOD_TO_DATE","type":"string","nullable":true,"metadata":{}},{"name":"DSP_FOLDER_AD","type":"string","nullable":true,"metadata":{}},{"name":"FYEAR_DSC","type":"string","nullable":true,"metadata":{}},{"name":"QUARTERS_RTA","type":"string","nullable":true,"metadata":{}}]}, spark.sql.sources.schema.part.0={"type":"struct","fields":[{"name":"QUARTERS","type":"string","nullable":true,"metadata":{}},{"name":"QUARTER_IN_YEAR_DSC","type":"string","nullable":true,"metadata":{}},{"name":"HYTD","type":"string","nullable":true,"metadata":{}},{"name":"PRD_SP","type":"integer","nullable":true,"metadata":{}},{"name":"WEEKS12","type":"string","nullable":true,"metadata":{}},{"name":"QTD","type":"string","nullable":true,"metadata":{}},{"name":"PRD_DSP_ORDER","type":"integer","nullable":true,"metadata":{}},{"name":"MONTHS6","type":"string","nullable":true,"metadata":{}},{"name":"PRD_RLS_IND","type":"string","nullable":true,"metadata":{}},{"name":"HOLIDAY_PERIODS","type":"string","nullable":true,"metadata":{}},{"name":"WEEKS4","type":"string","nullable":true,"metadata":{}},{"name":"YEAR_ORD","type":"integer","nullable":true,"metadata":{}},{"name":"YTD","type":"string","nullable":true,"metadata":{}},{"name":"WEEKS24","type":"string","nullable":true,"metadata":{}},{"name":"PMONTH_DSC","type":"string","nullable":true,"metadata":{}},{"name":"FQUARTER_IN_YEAR_DSC","type":"string","nullable":true,"metadata":{}},{"name":"PYEAR_ORD","type":"integer","nullable":true,"metadata":{}},{"name":"PWEEK_ORD","type":"integer","nullable":true,"metadata":{}},{"name":"PRD_KEY","type":"integer","nullable":true,"metadata":{}},{"name":"WEEK_ORD","type":"integer","nullable":true,"metadata":{}},{"name":"MONTHS3","type":"string","nullable":true,"metadata":{}},{"name":"CUME_RUL_TYP","type":"string","nullable":true,"metadata":{}},{"name":"PRD_ID","type":"integer","nullable":true,"metadata":{}},{"name":"MONTHLY","type":"string","nullable":true,"metadata":{}},{"name":"MONTHS12","type":"string","nullable":true,"metadata":{}},{"name":"RPT_MONTHS","type":"string","nullable":true,"metadata":{}},{"name":"PYEAR_DSC","type":"string","nullable":true,"metadata":{}},{"name":"HALF_YEARS_RTA","type":"string","nullable":true,"metadata":{}},{"name":"WK_ENDING_DSC","type":"string","nullable":true,"metadata":{}},{"name":"MONTHS","type":"string","nullable":true,"metadata":{}},{"name":"PRD_DSC_SHORT","type":"string","nullable":true,"metadata":{}},{"name":"SQUARTER_IN_YEAR_DSC","type":"string","nullable":true,"metadata":{}},{"name":"PRD_DSC_LONG","type":"string","nullable":true,"metadata":{}},{"name":"PPERIOD_DSC","type":"string","nullable":true,"metadata":{}},{"name":"PCKG_ID","type":"integer","nullable":true,"metadata":{}},{"name":"PWEEK_DSC","type":"string","nullable":true,"metadata":{}},{"name":"PPERIOD_ORD","type":"integer","nullable":true,"metadata":{}},{"name":"SWEEK_IN_YEAR_DSC","type":"string","nullable":true,"metadata":{}},{"name":"CUME_END_DT","type":"timestamp","nullable":true,"metadata":{}},{"name":"MONTH_IN_QUARTER_DSC","type":"string","nullable":true,"metadata":{}},{"name":"PRD_DET_LVL","type":"string","nullable":true,"metadata":{}},{"name":"SMONTH_IN_QUARTER_DSC","type":"string","nullable":true,"metadata":{}},{"name":"PRD_ID_YAGO","type":"integer","nullable":true,"metadata":{}},{"name":"WEEKS52","type":"string","nullable":true,"metadata":{}},{"name":"CURRENT_PERIODS","type":"string","nullable":true,"metadata":{}},{"name":"WEEK_IN_MONTH_DSC","type":"string","nullable":true,"metadata":{}},{"name":"WEEKS26","type":"string","nullable":true,"metadata":{}},{"name":"MONTH_ORD","type":"integer","nullable":true,"metadata":{}},{"name":"PQUARTER_DSC","type":"string","nullable":true,"metadata":{}},{"name":"FILTER_NAME","type":"string","nullable":true,"metadata":{}},{"name":"PRD_DSP_FOLDER","type":"string","nullable":true,"metadata":{}},{"name":"PRD_SRC_ID","type":"integer","nullable":true,"metadata":{}},{"name":"WEEKS4_ORD","type":"integer","nullable":true,"metadata":{}},{"name":"DIM_TYP","type":"string","nullable":true,"metadata":{}},{"name":"HALF_YEARS","type":"string","nullable":true,"metadata":{}},{"name":"PQUARTER_ORD","type":"integer","nullable":true,"metadata":{}},{"name":"MONTHS1","type":"string","nullable":true,"metadata":{}},{"name":"HOLIDAYS","type":"string","nullable":true,"met, spark.sql.sources.schema.numParts=2, spark.sql.sources.provider=parquet, spark.sql.create.version=2.4.0}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{root=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:root, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:root, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:root, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:root, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null))	
19/10/16 17:03:17 INFO FileToDimension: >>>>>>>>>> Writing table RMS_1000255_PQM.PRD_DIM Completed.
19/10/16 17:03:17 INFO FileToDimension: FileToDimension Some(PeriodCSV) for PRD_DIM input file wasbs://teuatcontainer01@teuatstorage.blob.core.windows.net//RMS_1000255_PQM/PRD_DIM.csv>>>>> Completed
19/10/16 17:03:17 INFO ProductExecutor: Executing step..Steps(Some(1),Some(ProductCSV),Some(Product CSV Build),Some(List()),Some(Parquet),Some(reads csv and builds Product file),Some(aaa),Some(library),Some(String),Some(String),Some(Table),Some(PRDC_DIM),Some(StepLibraryDetails(Some(abc.jar),Some(com.nielsen.te.v3.core.executors.ProductExecutor))),Some(String),None)
19/10/16 17:03:17 INFO SQLAppStatusListener: Execution ID: 21 Total Executor Run Time: 658
19/10/16 17:03:17 INFO HiveMetaStore: 1: get_database: abd_1000255
19/10/16 17:03:17 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: abd_1000255	
19/10/16 17:03:17 INFO HiveMetaStore: 1: get_table : db=abd_1000255 tbl=trag_node_hie_prd
19/10/16 17:03:17 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=abd_1000255 tbl=trag_node_hie_prd	
19/10/16 17:03:17 ERROR ScalaDriverLocal: User Code Stack Trace: 
org.apache.spark.sql.AnalysisException: Table or view not found: `abd_1000255`.`trag_node_hie_prd`;;
'UnresolvedRelation `abd_1000255`.`trag_node_hie_prd`

	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:89)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:84)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:148)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:84)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:102)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:116)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:113)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:113)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$analyzed$1.apply(QueryExecution.scala:82)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$analyzed$1.apply(QueryExecution.scala:80)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:80)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:80)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:72)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:80)
	at org.apache.spark.sql.SparkSession.table(SparkSession.scala:677)
	at org.apache.spark.sql.SparkSession.table(SparkSession.scala:673)
	at com.nielsen.te.v3.core.executors.explodeProcess$$anonfun$mergeDbsTblData$1.apply(ProductExecutor.scala:50)
	at com.nielsen.te.v3.core.executors.explodeProcess$$anonfun$mergeDbsTblData$1.apply(ProductExecutor.scala:49)
	at scala.collection.immutable.List.foreach(List.scala:381)
	at com.nielsen.te.v3.core.executors.explodeProcess$class.mergeDbsTblData(ProductExecutor.scala:49)
	at com.nielsen.te.v3.core.executors.ProductExecutor.mergeDbsTblData(ProductExecutor.scala:63)
	at com.nielsen.te.v3.core.executors.ProductExecutor.executeStep(ProductExecutor.scala:77)
	at com.nielsen.te.v3.executor.LibraryStepExecutor.executeStep(StepExecutor.scala:56)
	at com.nielsen.te.v3.executor.StepProcessor$$anonfun$stepProcessor$1.apply(StepProcessor.scala:129)
	at com.nielsen.te.v3.executor.StepProcessor$$anonfun$stepProcessor$1.apply(StepProcessor.scala:127)
	at scala.collection.immutable.List.foreach(List.scala:381)
	at com.nielsen.te.v3.executor.StepProcessor$.stepProcessor(StepProcessor.scala:127)
	at com.nielsen.te.v3.TEv3Main$.appMain(TEv3Main.scala:47)
	at com.nielsen.te.v3.TEv3Main$.main(TEv3Main.scala:51)
	at line85237363e5484a9a98c17c2f3e11d1cc25.$read$$iw$$iw$$iw$$iw$$iw$$iw.<init>(command--1:1)
	at line85237363e5484a9a98c17c2f3e11d1cc25.$read$$iw$$iw$$iw$$iw$$iw.<init>(command--1:44)
	at line85237363e5484a9a98c17c2f3e11d1cc25.$read$$iw$$iw$$iw$$iw.<init>(command--1:46)
	at line85237363e5484a9a98c17c2f3e11d1cc25.$read$$iw$$iw$$iw.<init>(command--1:48)
	at line85237363e5484a9a98c17c2f3e11d1cc25.$read$$iw$$iw.<init>(command--1:50)
	at line85237363e5484a9a98c17c2f3e11d1cc25.$read$$iw.<init>(command--1:52)
	at line85237363e5484a9a98c17c2f3e11d1cc25.$read.<init>(command--1:54)
	at line85237363e5484a9a98c17c2f3e11d1cc25.$read$.<init>(command--1:58)
	at line85237363e5484a9a98c17c2f3e11d1cc25.$read$.<clinit>(command--1)
	at line85237363e5484a9a98c17c2f3e11d1cc25.$eval$.$print$lzycompute(<notebook>:7)
	at line85237363e5484a9a98c17c2f3e11d1cc25.$eval$.$print(<notebook>:6)
	at line85237363e5484a9a98c17c2f3e11d1cc25.$eval.$print(<notebook>)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:786)
	at scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:1047)
	at scala.tools.nsc.interpreter.IMain$WrappedRequest$$anonfun$loadAndRunReq$1.apply(IMain.scala:638)
	at scala.tools.nsc.interpreter.IMain$WrappedRequest$$anonfun$loadAndRunReq$1.apply(IMain.scala:637)
	at scala.reflect.internal.util.ScalaClassLoader$class.asContext(ScalaClassLoader.scala:31)
	at scala.reflect.internal.util.AbstractFileClassLoader.asContext(AbstractFileClassLoader.scala:19)
	at scala.tools.nsc.interpreter.IMain$WrappedRequest.loadAndRunReq(IMain.scala:637)
	at scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:569)
	at scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:565)
	at com.databricks.backend.daemon.driver.DriverILoop.execute(DriverILoop.scala:199)
	at com.databricks.backend.daemon.driver.ScalaDriverLocal$$anonfun$repl$1.apply$mcV$sp(ScalaDriverLocal.scala:189)
	at com.databricks.backend.daemon.driver.ScalaDriverLocal$$anonfun$repl$1.apply(ScalaDriverLocal.scala:189)
	at com.databricks.backend.daemon.driver.ScalaDriverLocal$$anonfun$repl$1.apply(ScalaDriverLocal.scala:189)
	at com.databricks.backend.daemon.driver.DriverLocal$TrapExitInternal$.trapExit(DriverLocal.scala:587)
	at com.databricks.backend.daemon.driver.DriverLocal$TrapExit$.apply(DriverLocal.scala:542)
	at com.databricks.backend.daemon.driver.ScalaDriverLocal.repl(ScalaDriverLocal.scala:189)
	at com.databricks.backend.daemon.driver.DriverLocal$$anonfun$execute$7.apply(DriverLocal.scala:324)
	at com.databricks.backend.daemon.driver.DriverLocal$$anonfun$execute$7.apply(DriverLocal.scala:304)
	at com.databricks.logging.UsageLogging$$anonfun$withAttributionContext$1.apply(UsageLogging.scala:235)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at com.databricks.logging.UsageLogging$class.withAttributionContext(UsageLogging.scala:230)
	at com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:45)
	at com.databricks.logging.UsageLogging$class.withAttributionTags(UsageLogging.scala:268)
	at com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:45)
	at com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:304)
	at com.databricks.backend.daemon.driver.DriverWrapper$$anonfun$tryExecutingCommand$2.apply(DriverWrapper.scala:589)
	at com.databricks.backend.daemon.driver.DriverWrapper$$anonfun$tryExecutingCommand$2.apply(DriverWrapper.scala:589)
	at scala.util.Try$.apply(Try.scala:192)
	at com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:584)
	at com.databricks.backend.daemon.driver.DriverWrapper.getCommandOutputAndError(DriverWrapper.scala:475)
	at com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:542)
	at com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:381)
	at com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:328)
	at com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:215)
	at java.lang.Thread.run(Thread.java:748)
19/10/16 17:03:17 INFO ProgressReporter$: Removed result fetcher for 6186194774562528663_8903127354785209889_job-67849-run-1-action-72025
19/10/16 17:03:19 INFO DriverCorral$: Cleaning the wrapper ReplId-55d9c-75f21-2d5d9-7 (currently in status Idle(ReplId-55d9c-75f21-2d5d9-7))
19/10/16 17:03:19 INFO DriverCorral$: sending shutdown signal for REPL ReplId-55d9c-75f21-2d5d9-7
19/10/16 17:03:19 INFO DriverCorral$: sending the interrupt signal for REPL ReplId-55d9c-75f21-2d5d9-7
19/10/16 17:03:19 INFO DriverCorral$: waiting for localThread to stop for REPL ReplId-55d9c-75f21-2d5d9-7
19/10/16 17:03:19 INFO DriverCorral$: ReplId-55d9c-75f21-2d5d9-7 successfully discarded
19/10/16 17:03:21 ERROR TaskSchedulerImpl: Lost executor 0 on 10.139.64.53: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
19/10/16 17:03:21 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20191016165818-0000/0 is now LOST (worker lost)
19/10/16 17:03:21 INFO StandaloneSchedulerBackend: Executor app-20191016165818-0000/0 removed: worker lost
19/10/16 17:03:21 INFO DAGScheduler: Executor lost: 0 (epoch 0)
19/10/16 17:03:21 INFO BlockManagerMasterEndpoint: Trying to remove executor 0 from BlockManagerMaster.
19/10/16 17:03:21 INFO BlockManagerMasterEndpoint: Removing block manager BlockManagerId(0, 10.139.64.53, 43111, None)
19/10/16 17:03:21 INFO BlockManagerMasterEndpoint: Trying to remove executor 0 from BlockManagerMaster.
19/10/16 17:03:21 INFO StandaloneAppClient$ClientEndpoint: Master removed worker worker-20191016165804-10.139.64.53-42791: 10.139.64.53:42791 got disassociated
19/10/16 17:03:21 INFO BlockManagerMaster: Removed 0 successfully in removeExecutor
19/10/16 17:03:21 INFO BlockManagerMaster: Removal of executor 0 requested
19/10/16 17:03:21 INFO StandaloneSchedulerBackend: Worker worker-20191016165804-10.139.64.53-42791 removed: 10.139.64.53:42791 got disassociated
19/10/16 17:03:21 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asked to remove non-existent executor 0
19/10/16 17:03:21 INFO TaskSchedulerImpl: Handle removed worker worker-20191016165804-10.139.64.53-42791: 10.139.64.53:42791 got disassociated
19/10/16 17:03:21 INFO DAGScheduler: Shuffle files lost for worker worker-20191016165804-10.139.64.53-42791 on host 10.139.64.53
