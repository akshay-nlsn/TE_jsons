19/10/17 21:07:45 INFO StaticConf$: DB_HOME: /databricks
19/10/17 21:07:45 INFO DriverDaemon$: ========== driver starting up ==========
19/10/17 21:07:45 INFO DriverDaemon$: Java: Oracle Corporation 1.8.0_191
19/10/17 21:07:45 INFO DriverDaemon$: OS: Linux/amd64 4.15.0-1050-azure
19/10/17 21:07:45 INFO DriverDaemon$: CWD: /databricks/driver
19/10/17 21:07:45 INFO DriverDaemon$: Mem: Max: 97.3G loaded GCs: PS Scavenge, PS MarkSweep
19/10/17 21:07:45 INFO DriverDaemon$: Logging multibyte characters: âœ“
19/10/17 21:07:45 INFO DriverDaemon$: 'publicFile' appender in root logger: class com.databricks.logging.RedactionRollingFileAppender
19/10/17 21:07:45 INFO DriverDaemon$: 'org.apache.log4j.Appender' appender in root logger: class com.codahale.metrics.log4j.InstrumentedAppender
19/10/17 21:07:45 INFO DriverDaemon$: 'null' appender in root logger: class com.databricks.logging.RequestTracker
19/10/17 21:07:45 INFO DriverDaemon$: == Modules:
19/10/17 21:07:46 INFO DriverDaemon$: Starting prometheus metrics log export timer
19/10/17 21:07:46 INFO DriverDaemon$: Universe Git Hash: 7201322412393895429516835306b6ef047d0688
19/10/17 21:07:46 INFO DriverDaemon$: Spark Git Hash: a937effdb77c94e1293f1fa405f29834568a066a
19/10/17 21:07:46 WARN RunHelpers$: Missing tag isolation client: java.util.NoSuchElementException: key not found: TagDefinition(clientType,The client type for a request, used for isolating resources for the request.)
19/10/17 21:07:46 INFO DatabricksILoop$: Creating throwaway interpreter
19/10/17 21:07:46 INFO SparkConfUtils$: Customize spark config according to file /tmp/custom-spark.conf
19/10/17 21:07:46 INFO SparkConfUtils$: new spark config: spark.sql.warehouse.dir -> adl://connectedsystemsuatdl.azuredatalakestore.net/user/hive/warehouse
19/10/17 21:07:46 INFO SparkConfUtils$: new spark config: spark.hadoop.javax.jdo.option.ConnectionPassword -> 560I890@fd80709
19/10/17 21:07:46 INFO SparkConfUtils$: new spark config: spark.hadoop.dfs.adls.oauth2.credential -> viUHaEfNg8cHQRJSRNMzpIv1T6goiUg+c9R/XSbYBFA=
19/10/17 21:07:46 INFO SparkConfUtils$: new spark config: spark.hadoop.javax.jdo.option.ConnectionDriverName -> org.postgresql.Driver
19/10/17 21:07:46 INFO SparkConfUtils$: new spark config: spark.databricks.acl.dfAclsEnabled -> false
19/10/17 21:07:46 INFO SparkConfUtils$: new spark config: spark.driver.tempDirectory -> /local_disk0/tmp
19/10/17 21:07:46 INFO SparkConfUtils$: new spark config: spark.hadoop.datanucleus.connectionPool.maxIdle -> 1
19/10/17 21:07:46 INFO SparkConfUtils$: new spark config: spark.hadoop.dfs.adls.oauth2.client.id -> 003106c3-ff8c-4c62-a060-24b8b32ea512
19/10/17 21:07:46 INFO SparkConfUtils$: new spark config: spark.hadoop.dfs.adls.oauth2.refresh.url -> https://login.microsoftonline.com/6ac7a1f4-5fb1-4153-bb4f-12d2020a1f7d/oauth2/token
19/10/17 21:07:46 INFO SparkConfUtils$: new spark config: spark.executor.tempDirectory -> /local_disk0/tmp
19/10/17 21:07:46 INFO SparkConfUtils$: new spark config: spark.databricks.delta.preview.enabled -> true
19/10/17 21:07:46 INFO SparkConfUtils$: new spark config: spark.task.maxFailures -> 10
19/10/17 21:07:46 INFO SparkConfUtils$: new spark config: spark.sql.hive.metastore.jars -> maven
19/10/17 21:07:46 INFO SparkConfUtils$: new spark config: spark.databricks.aggressiveWindowDownS -> 300
19/10/17 21:07:46 INFO SparkConfUtils$: new spark config: spark.hadoop.javax.jdo.option.ConnectionUserName -> tedbuat
19/10/17 21:07:46 INFO SparkConfUtils$: new spark config: spark.databricks.sql.hive.msck.batchSize -> 200
19/10/17 21:07:46 INFO SparkConfUtils$: new spark config: spark.hadoop.javax.jdo.option.ConnectionURL -> jdbc:postgresql://uathmsdb.trans.enterprisecsp.net:5432/hive_adls
19/10/17 21:07:46 INFO SparkConfUtils$: new spark config: datanucleus.fixedDatastore -> false
19/10/17 21:07:46 INFO SparkConfUtils$: new spark config: spark.databricks.sql.hive.msck.parallelism -> 20
19/10/17 21:07:46 INFO SparkConfUtils$: new spark config: spark.hadoop.datanucleus.connectionPool.maxPoolSize -> 30
19/10/17 21:07:46 INFO SparkConfUtils$: new spark config: spark.hadoop.datanucleus.connectionPool.minPoolSize -> 15
19/10/17 21:07:46 INFO SparkConfUtils$: new spark config: spark.sql.parquet.filterPushdown -> false
19/10/17 21:07:46 INFO SparkConfUtils$: new spark config: spark.hadoop.dfs.adls.oauth2.access.token.provider.type -> ClientCredential
19/10/17 21:07:46 INFO SparkConfUtils$: new spark config: spark.sql.shuffle.partitions -> 2000
19/10/17 21:07:46 INFO SparkConfUtils$: new spark config: spark.driver.maxResultSize -> 0
19/10/17 21:07:46 INFO SparkConfUtils$: new spark config: fs.azure.account.key.teuatsa.blob.core.windows.net -> IsrxMnnzLeO9N44beU4eMw0LaiKJdi1hHmKZFUiPa8nFNKZAFyhH0Hv/UCoC4qhgcdq63zwTRGFhUKV9d4ZsBw==
19/10/17 21:07:46 INFO SparkConfUtils$: new spark config: datanucleus.autoCreateSchema -> true
19/10/17 21:07:46 INFO SparkConfUtils$: new spark config: spark.hadoop.fs.adl.impl.disable.cache -> false
19/10/17 21:07:46 INFO SparkConfUtils$: new spark config: spark.sql.hive.metastore.version -> 2.3.0
19/10/17 21:07:46 WARN MetastoreMonitor$: Uri scheme postgresql is not supported.
19/10/17 21:07:46 WARN MetastoreMonitor$: Unexpected partial metastore configuration (userOpt=Some(tedbuat), pw.isDefined=true, uriOpt=None)
19/10/17 21:07:46 INFO MetastoreMonitor$: Generic external metastore configurd (config=jdbc:postgresql://uathmsdb.trans.enterprisecsp.net:5432/hive_adls)
19/10/17 21:07:46 INFO MetastoreMonitor: Not monitoring ExternalGenericMetastore
19/10/17 21:07:46 INFO JettyClient$: Creating new HttpClient with SSLContextFactory=None,maxRequestHeaderSize=65536, namePrefix=Some(DriverDaemon), idleTimeout=2 hours
19/10/17 21:07:46 INFO DriverCorral: Creating the driver context
19/10/17 21:07:46 INFO DatabricksILoop$: Class Server Dir: /local_disk0/tmp/repl/spark-3380005823756244851-0b66baa2-a785-42de-a2de-5143320e55d8
19/10/17 21:07:46 INFO SparkConfUtils$: Customize spark config according to file /tmp/custom-spark.conf
19/10/17 21:07:46 INFO SparkConfUtils$: new spark config: spark.sql.warehouse.dir -> adl://connectedsystemsuatdl.azuredatalakestore.net/user/hive/warehouse
19/10/17 21:07:46 INFO SparkConfUtils$: new spark config: spark.hadoop.javax.jdo.option.ConnectionPassword -> 560I890@fd80709
19/10/17 21:07:46 INFO SparkConfUtils$: new spark config: spark.hadoop.dfs.adls.oauth2.credential -> viUHaEfNg8cHQRJSRNMzpIv1T6goiUg+c9R/XSbYBFA=
19/10/17 21:07:46 INFO SparkConfUtils$: new spark config: spark.hadoop.javax.jdo.option.ConnectionDriverName -> org.postgresql.Driver
19/10/17 21:07:46 INFO SparkConfUtils$: new spark config: spark.databricks.acl.dfAclsEnabled -> false
19/10/17 21:07:46 INFO SparkConfUtils$: new spark config: spark.driver.tempDirectory -> /local_disk0/tmp
19/10/17 21:07:46 INFO SparkConfUtils$: new spark config: spark.hadoop.datanucleus.connectionPool.maxIdle -> 1
19/10/17 21:07:46 INFO SparkConfUtils$: new spark config: spark.hadoop.dfs.adls.oauth2.client.id -> 003106c3-ff8c-4c62-a060-24b8b32ea512
19/10/17 21:07:46 INFO SparkConfUtils$: new spark config: spark.hadoop.dfs.adls.oauth2.refresh.url -> https://login.microsoftonline.com/6ac7a1f4-5fb1-4153-bb4f-12d2020a1f7d/oauth2/token
19/10/17 21:07:46 INFO SparkConfUtils$: new spark config: spark.executor.tempDirectory -> /local_disk0/tmp
19/10/17 21:07:46 INFO SparkConfUtils$: new spark config: spark.databricks.delta.preview.enabled -> true
19/10/17 21:07:46 INFO SparkConfUtils$: new spark config: spark.task.maxFailures -> 10
19/10/17 21:07:46 INFO SparkConfUtils$: new spark config: spark.sql.hive.metastore.jars -> maven
19/10/17 21:07:46 INFO SparkConfUtils$: new spark config: spark.databricks.aggressiveWindowDownS -> 300
19/10/17 21:07:46 INFO SparkConfUtils$: new spark config: spark.hadoop.javax.jdo.option.ConnectionUserName -> tedbuat
19/10/17 21:07:46 INFO SparkConfUtils$: new spark config: spark.databricks.sql.hive.msck.batchSize -> 200
19/10/17 21:07:46 INFO SparkConfUtils$: new spark config: spark.hadoop.javax.jdo.option.ConnectionURL -> jdbc:postgresql://uathmsdb.trans.enterprisecsp.net:5432/hive_adls
19/10/17 21:07:46 INFO SparkConfUtils$: new spark config: datanucleus.fixedDatastore -> false
19/10/17 21:07:46 INFO SparkConfUtils$: new spark config: spark.databricks.sql.hive.msck.parallelism -> 20
19/10/17 21:07:46 INFO SparkConfUtils$: new spark config: spark.hadoop.datanucleus.connectionPool.maxPoolSize -> 30
19/10/17 21:07:46 INFO SparkConfUtils$: new spark config: spark.hadoop.datanucleus.connectionPool.minPoolSize -> 15
19/10/17 21:07:46 INFO SparkConfUtils$: new spark config: spark.sql.parquet.filterPushdown -> false
19/10/17 21:07:46 INFO SparkConfUtils$: new spark config: spark.hadoop.dfs.adls.oauth2.access.token.provider.type -> ClientCredential
19/10/17 21:07:46 INFO SparkConfUtils$: new spark config: spark.sql.shuffle.partitions -> 2000
19/10/17 21:07:46 INFO SparkConfUtils$: new spark config: spark.driver.maxResultSize -> 0
19/10/17 21:07:46 INFO SparkConfUtils$: new spark config: fs.azure.account.key.teuatsa.blob.core.windows.net -> IsrxMnnzLeO9N44beU4eMw0LaiKJdi1hHmKZFUiPa8nFNKZAFyhH0Hv/UCoC4qhgcdq63zwTRGFhUKV9d4ZsBw==
19/10/17 21:07:46 INFO SparkConfUtils$: new spark config: datanucleus.autoCreateSchema -> true
19/10/17 21:07:46 INFO SparkConfUtils$: new spark config: spark.hadoop.fs.adl.impl.disable.cache -> false
19/10/17 21:07:46 INFO SparkConfUtils$: new spark config: spark.sql.hive.metastore.version -> 2.3.0
19/10/17 21:07:46 WARN SparkConf: The configuration key 'spark.scheduler.listenerbus.eventqueue.size' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.scheduler.listenerbus.eventqueue.capacity' instead.
19/10/17 21:07:46 WARN SparkConf: The configuration key 'spark.akka.frameSize' has been deprecated as of Spark 1.6 and may be removed in the future. Please use the new key 'spark.rpc.message.maxSize' instead.
19/10/17 21:07:46 INFO SparkContext: Running Spark version 2.4.0
19/10/17 21:07:47 WARN SparkConf: Detected deprecated memory fraction settings: [spark.storage.memoryFraction, spark.shuffle.memoryFraction]. As of Spark 1.6, execution and storage memory management are unified. All memory fractions used in the old model are now deprecated and no longer read. If you wish to use the old memory management, you may explicitly enable `spark.memory.useLegacyMode` (not recommended).
19/10/17 21:07:47 INFO SparkContext: Submitted application: Databricks Shell
19/10/17 21:07:47 INFO SparkContext: Spark configuration:
datanucleus.autoCreateSchema=true
datanucleus.fixedDatastore=false
eventLog.rolloverIntervalSeconds=3600
fs.azure.account.key.teuatsa.blob.core.windows.net=IsrxMnnzLeO9N44beU4eMw0LaiKJdi1hHmKZFUiPa8nFNKZAFyhH0Hv/UCoC4qhgcdq63zwTRGFhUKV9d4ZsBw==
spark.akka.frameSize=256
spark.app.name=Databricks Shell
spark.cleaner.referenceTracking.blocking=false
spark.databricks.acl.client=com.databricks.spark.sql.acl.client.SparkSqlAclClient
spark.databricks.acl.dfAclsEnabled=false
spark.databricks.acl.provider=com.databricks.sql.acl.ReflectionBackedAclProvider
spark.databricks.aggressiveWindowDownS=300
spark.databricks.cloudProvider=Azure
spark.databricks.clusterSource=JOB
spark.databricks.clusterUsageTags.autoTerminationMinutes=0
spark.databricks.clusterUsageTags.clusterAggressiveAutoscalingWindowDownSeconds=300
spark.databricks.clusterUsageTags.clusterAllTags=[{"key":"Vendor","value":"Databricks"},{"key":"Creator","value":"basheer.ahamed@nielsen.com"},{"key":"ClusterName","value":"job-68230-run-1"},{"key":"ClusterId","value":"1017-210348-beset741"},{"key":"DatabricksEnvironment","value":"workerenv-5614106561397268"}]
spark.databricks.clusterUsageTags.clusterCreator=JobLauncher
spark.databricks.clusterUsageTags.clusterGeneration=0
spark.databricks.clusterUsageTags.clusterId=1017-210348-beset741
spark.databricks.clusterUsageTags.clusterLogDeliveryEnabled=true
spark.databricks.clusterUsageTags.clusterLogDestination=dbfs:/mnt/te_mount/uat_logs/
spark.databricks.clusterUsageTags.clusterMaxWorkers=5
spark.databricks.clusterUsageTags.clusterMetastoreAccessType=RDS_DIRECT
spark.databricks.clusterUsageTags.clusterMinWorkers=1
spark.databricks.clusterUsageTags.clusterName=job-68230-run-1
spark.databricks.clusterUsageTags.clusterNoDriverDaemon=false
spark.databricks.clusterUsageTags.clusterNodeType=Standard_E16s_v3
spark.databricks.clusterUsageTags.clusterNumSshKeys=0
spark.databricks.clusterUsageTags.clusterOwnerOrgId=5614106561397268
spark.databricks.clusterUsageTags.clusterOwnerUserId=*********(redacted)
spark.databricks.clusterUsageTags.clusterPinned=false
spark.databricks.clusterUsageTags.clusterPythonVersion=2
spark.databricks.clusterUsageTags.clusterResourceClass=default
spark.databricks.clusterUsageTags.clusterScalingType=autoscaling
spark.databricks.clusterUsageTags.clusterSku=STANDARD_SKU
spark.databricks.clusterUsageTags.clusterState=Pending
spark.databricks.clusterUsageTags.clusterStateMessage=Starting Spark
spark.databricks.clusterUsageTags.clusterTargetWorkers=1
spark.databricks.clusterUsageTags.clusterWorkers=1
spark.databricks.clusterUsageTags.containerType=LXC
spark.databricks.clusterUsageTags.driverContainerId=fa35d7cea8ca4117a15435926a70b039
spark.databricks.clusterUsageTags.driverContainerPrivateIp=10.139.64.37
spark.databricks.clusterUsageTags.driverInstanceId=143cf8c5f1744fcaa46bb4fe65f2c3ab
spark.databricks.clusterUsageTags.driverInstancePrivateIp=10.139.0.37
spark.databricks.clusterUsageTags.driverNodeType=Standard_E16s_v3
spark.databricks.clusterUsageTags.driverPublicDns=52.167.200.246
spark.databricks.clusterUsageTags.enableCredentialPassthrough=*********(redacted)
spark.databricks.clusterUsageTags.enableDfAcls=false
spark.databricks.clusterUsageTags.enableElasticDisk=true
spark.databricks.clusterUsageTags.enableJdbcAutoStart=true
spark.databricks.clusterUsageTags.enableJobsAutostart=true
spark.databricks.clusterUsageTags.enableLocalDiskEncryption=false
spark.databricks.clusterUsageTags.enableSqlAclsOnly=false
spark.databricks.clusterUsageTags.hailEnabled=false
spark.databricks.clusterUsageTags.isSingleUserCluster=*********(redacted)
spark.databricks.clusterUsageTags.numPerClusterInitScriptsV2=0
spark.databricks.clusterUsageTags.sparkVersion=5.2.x-scala2.11
spark.databricks.clusterUsageTags.userProvidedRemoteVolumeCount=*********(redacted)
spark.databricks.clusterUsageTags.userProvidedRemoteVolumeSizeGb=*********(redacted)
spark.databricks.clusterUsageTags.userProvidedRemoteVolumeType=*********(redacted)
spark.databricks.clusterUsageTags.workerEnvironmentId=workerenv-5614106561397268
spark.databricks.credential.redactor=*********(redacted)
spark.databricks.delta.logStore.crossCloud.fatal=true
spark.databricks.delta.multiClusterWrites.enabled=true
spark.databricks.delta.preview.enabled=true
spark.databricks.driverNodeTypeId=Standard_E16s_v3
spark.databricks.eventLog.dir=eventlogs
spark.databricks.io.directoryCommit.enableLogicalDelete=false
spark.databricks.overrideDefaultCommitProtocol=org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol
spark.databricks.passthrough.adls.tokenProviderClassName=*********(redacted)
spark.databricks.passthrough.s3a.tokenProviderClassName=*********(redacted)
spark.databricks.preemption.enabled=true
spark.databricks.r.cleanWorkspace=true
spark.databricks.redactor=com.databricks.spark.util.DatabricksSparkLogRedactorProxy
spark.databricks.session.share=false
spark.databricks.sparkContextId=3380005823756244851
spark.databricks.sql.hive.msck.batchSize=200
spark.databricks.sql.hive.msck.parallelism=20
spark.databricks.tahoe.logStore.aws.class=com.databricks.tahoe.store.MultiClusterLogStore
spark.databricks.tahoe.logStore.azure.class=com.databricks.tahoe.store.AzureLogStore
spark.databricks.tahoe.logStore.class=com.databricks.tahoe.store.DelegatingLogStore
spark.databricks.workerNodeTypeId=Standard_E16s_v3
spark.driver.allowMultipleContexts=false
spark.driver.maxResultSize=0
spark.driver.tempDirectory=/local_disk0/tmp
spark.eventLog.enabled=false
spark.executor.extraClassPath=*********(redacted)
spark.executor.extraJavaOptions=-Djava.io.tmpdir=/local_disk0/tmp -XX:ReservedCodeCacheSize=256m -XX:+UseCodeCacheFlushing -Djava.security.properties=/databricks/spark/dbconf/java/extra.security -XX:+PrintFlagsFinal -XX:+PrintGCDateStamps -verbose:gc -XX:+PrintGCDetails -Xss4m -Djavax.xml.datatype.DatatypeFactory=com.sun.org.apache.xerces.internal.jaxp.datatype.DatatypeFactoryImpl -Djavax.xml.parsers.DocumentBuilderFactory=com.sun.org.apache.xerces.internal.jaxp.DocumentBuilderFactoryImpl -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -Djavax.xml.validation.SchemaFactory:http://www.w3.org/2001/XMLSchema=com.sun.org.apache.xerces.internal.jaxp.validation.XMLSchemaFactory -Dorg.xml.sax.driver=com.sun.org.apache.xerces.internal.parsers.SAXParser -Dorg.w3c.dom.DOMImplementationSourceList=com.sun.org.apache.xerces.internal.dom.DOMXSImplementationSourceImpl -Ddatabricks.serviceName=spark-executor-1
spark.executor.memory=97871m
spark.executor.tempDirectory=/local_disk0/tmp
spark.extraListeners=com.databricks.backend.daemon.driver.DBCEventLoggingListener
spark.files.fetchFailure.unRegisterOutputOnHost=true
spark.files.overwrite=true
spark.files.useFetchCache=false
spark.hadoop.databricks.dbfs.client.version=v2
spark.hadoop.databricks.s3commit.client.sslTrustAll=false
spark.hadoop.datanucleus.connectionPool.maxIdle=1
spark.hadoop.datanucleus.connectionPool.maxPoolSize=30
spark.hadoop.datanucleus.connectionPool.minPoolSize=15
spark.hadoop.dfs.adls.oauth2.access.token.provider.type=*********(redacted)
spark.hadoop.dfs.adls.oauth2.client.id=*********(redacted)
spark.hadoop.dfs.adls.oauth2.credential=*********(redacted)
spark.hadoop.dfs.adls.oauth2.refresh.url=*********(redacted)
spark.hadoop.fs.abfs.impl=shaded.databricks.v20180920_b33d810.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem
spark.hadoop.fs.abfs.impl.disable.cache=true
spark.hadoop.fs.abfss.impl=shaded.databricks.v20180920_b33d810.org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem
spark.hadoop.fs.abfss.impl.disable.cache=true
spark.hadoop.fs.adl.impl=com.databricks.adl.AdlFileSystem
spark.hadoop.fs.adl.impl.disable.cache=false
spark.hadoop.fs.azure.skip.metrics=true
spark.hadoop.fs.s3.impl=com.databricks.s3a.S3AFileSystem
spark.hadoop.fs.s3a.connection.maximum=200
spark.hadoop.fs.s3a.fast.upload=true
spark.hadoop.fs.s3a.fast.upload.default=true
spark.hadoop.fs.s3a.impl=com.databricks.s3a.S3AFileSystem
spark.hadoop.fs.s3a.multipart.size=10485760
spark.hadoop.fs.s3a.multipart.threshold=104857600
spark.hadoop.fs.s3a.threads.max=136
spark.hadoop.fs.s3n.impl=com.databricks.s3a.S3AFileSystem
spark.hadoop.fs.wasb.impl=shaded.databricks.org.apache.hadoop.fs.azure.NativeAzureFileSystem
spark.hadoop.fs.wasb.impl.disable.cache=true
spark.hadoop.fs.wasbs.impl=shaded.databricks.org.apache.hadoop.fs.azure.NativeAzureFileSystem
spark.hadoop.fs.wasbs.impl.disable.cache=true
spark.hadoop.hive.server2.idle.operation.timeout=7200000
spark.hadoop.hive.server2.idle.session.timeout=900000
spark.hadoop.hive.server2.keystore.password=*********(redacted)
spark.hadoop.hive.server2.keystore.path=/databricks/keys/jetty-ssl-driver-keystore.jks
spark.hadoop.hive.server2.session.check.interval=60000
spark.hadoop.hive.server2.thrift.http.port=10000
spark.hadoop.hive.server2.transport.mode=http
spark.hadoop.hive.server2.use.SSL=true
spark.hadoop.hive.warehouse.subdir.inherit.perms=false
spark.hadoop.javax.jdo.option.ConnectionDriverName=org.postgresql.Driver
spark.hadoop.javax.jdo.option.ConnectionPassword=*********(redacted)
spark.hadoop.javax.jdo.option.ConnectionURL=*********(redacted)
spark.hadoop.javax.jdo.option.ConnectionUserName=*********(redacted)
spark.hadoop.mapred.output.committer.class=com.databricks.backend.daemon.data.client.DirectOutputCommitter
spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version=2
spark.hadoop.parquet.memory.pool.ratio=0.5
spark.hadoop.spark.sql.parquet.output.committer.class=org.apache.spark.sql.parquet.DirectParquetOutputCommitter
spark.hadoop.spark.sql.sources.outputCommitterClass=com.databricks.backend.daemon.data.client.MapReduceDirectOutputCommitter
spark.hadoop.spark.thriftserver.customHeadersToProperties=*********(redacted)
spark.home=/databricks/spark
spark.logConf=true
spark.master=spark://10.139.64.37:7077
spark.metrics.conf=/databricks/spark/conf/metrics.properties
spark.r.backendConnectionTimeout=604800
spark.r.numRBackendThreads=1
spark.rdd.compress=true
spark.repl.class.outputDir=/local_disk0/tmp/repl/spark-3380005823756244851-0b66baa2-a785-42de-a2de-5143320e55d8
spark.scheduler.listenerbus.eventqueue.size=20000
spark.scheduler.mode=FAIR
spark.serializer.objectStreamReset=100
spark.shuffle.manager=SORT
spark.shuffle.memoryFraction=0.2
spark.shuffle.reduceLocality.enabled=false
spark.shuffle.service.enabled=true
spark.shuffle.service.port=4048
spark.sparkr.use.daemon=false
spark.speculation=false
spark.speculation.multiplier=3
spark.speculation.quantile=0.9
spark.sql.allowMultipleContexts=false
spark.sql.hive.convertCTAS=true
spark.sql.hive.convertMetastoreParquet=true
spark.sql.hive.metastore.jars=maven
spark.sql.hive.metastore.sharedPrefixes=org.mariadb.jdbc,com.mysql.jdbc,org.postgresql,com.microsoft.sqlserver,microsoft.sql.DateTimeOffset,microsoft.sql.Types,com.databricks,com.codahale,com.fasterxml.jackson,shaded.databricks
spark.sql.hive.metastore.version=2.3.0
spark.sql.parquet.cacheMetadata=true
spark.sql.parquet.compression.codec=snappy
spark.sql.parquet.filterPushdown=false
spark.sql.shuffle.partitions=2000
spark.sql.sources.commitProtocolClass=com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol
spark.sql.streaming.checkpointFileManagerClass=com.databricks.spark.sql.streaming.DatabricksCheckpointFileManager
spark.sql.ui.retainedExecutions=100
spark.sql.warehouse.dir=*********(redacted)
spark.storage.blockManagerTimeoutIntervalMs=300000
spark.storage.memoryFraction=0.5
spark.streaming.driver.writeAheadLog.allowBatching=true
spark.streaming.driver.writeAheadLog.closeFileAfterWrite=true
spark.task.maxFailures=10
spark.task.reaper.enabled=true
spark.task.reaper.killTimeout=60s
spark.ui.port=45433
spark.worker.cleanup.enabled=false
19/10/17 21:07:47 INFO SecurityManager: Changing view acls to: root
19/10/17 21:07:47 INFO SecurityManager: Changing modify acls to: root
19/10/17 21:07:47 INFO SecurityManager: Changing view acls groups to: 
19/10/17 21:07:47 INFO SecurityManager: Changing modify acls groups to: 
19/10/17 21:07:47 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
19/10/17 21:07:47 INFO Utils: Successfully started service 'sparkDriver' on port 32805.
19/10/17 21:07:47 INFO SparkEnv: Registering MapOutputTracker
19/10/17 21:07:47 INFO SparkEnv: Registering BlockManagerMaster
19/10/17 21:07:47 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
19/10/17 21:07:47 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
19/10/17 21:07:47 INFO DiskBlockManager: Created local directory at /local_disk0/blockmgr-9de3ee78-d621-4315-994d-dd0820b2ca04
19/10/17 21:07:47 INFO MemoryStore: MemoryStore started with capacity 54.2 GB
19/10/17 21:07:47 INFO SparkEnv: Registering OutputCommitCoordinator
19/10/17 21:07:47 WARN MetricsSystem: Using default name SparkStatusTracker for source because neither spark.metrics.namespace nor spark.app.id is set.
19/10/17 21:07:47 INFO log: Logging initialized @4483ms
19/10/17 21:07:48 INFO Server: jetty-9.3.20.v20170531
19/10/17 21:07:48 INFO Server: Started @4596ms
19/10/17 21:07:48 INFO AbstractConnector: Started ServerConnector@536d97f8{HTTP/1.1,[http/1.1]}{10.139.64.37:45433}
19/10/17 21:07:48 INFO Utils: Successfully started service 'SparkUI' on port 45433.
19/10/17 21:07:48 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@7bac686b{/jobs,null,AVAILABLE,@Spark}
19/10/17 21:07:48 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@5b275811{/jobs/json,null,AVAILABLE,@Spark}
19/10/17 21:07:48 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@2f0ed952{/jobs/job,null,AVAILABLE,@Spark}
19/10/17 21:07:48 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@6d074b14{/jobs/job/json,null,AVAILABLE,@Spark}
19/10/17 21:07:48 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@c017175{/stages,null,AVAILABLE,@Spark}
19/10/17 21:07:48 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@559cedee{/stages/json,null,AVAILABLE,@Spark}
19/10/17 21:07:48 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@18371d89{/stages/stage,null,AVAILABLE,@Spark}
19/10/17 21:07:48 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@7af3874e{/stages/stage/json,null,AVAILABLE,@Spark}
19/10/17 21:07:48 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@5a8816cc{/stages/pool,null,AVAILABLE,@Spark}
19/10/17 21:07:48 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@68565bc7{/stages/pool/json,null,AVAILABLE,@Spark}
19/10/17 21:07:48 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@37baddde{/storage,null,AVAILABLE,@Spark}
19/10/17 21:07:48 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@5115f590{/storage/json,null,AVAILABLE,@Spark}
19/10/17 21:07:48 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@4b31a708{/storage/rdd,null,AVAILABLE,@Spark}
19/10/17 21:07:48 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@11e355ca{/storage/rdd/json,null,AVAILABLE,@Spark}
19/10/17 21:07:48 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@70d63e05{/environment,null,AVAILABLE,@Spark}
19/10/17 21:07:48 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@e11ecfa{/environment/json,null,AVAILABLE,@Spark}
19/10/17 21:07:48 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@485e13d7{/executors,null,AVAILABLE,@Spark}
19/10/17 21:07:48 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@43984213{/executors/json,null,AVAILABLE,@Spark}
19/10/17 21:07:48 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@2ff7a73{/executors/threadDump,null,AVAILABLE,@Spark}
19/10/17 21:07:48 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@38830ea{/executors/threadDump/json,null,AVAILABLE,@Spark}
19/10/17 21:07:48 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@3b705be7{/executors/heapHistogram,null,AVAILABLE,@Spark}
19/10/17 21:07:48 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@3a790e40{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
19/10/17 21:07:48 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@43acd79e{/static,null,AVAILABLE,@Spark}
19/10/17 21:07:48 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@77ab22be{/,null,AVAILABLE,@Spark}
19/10/17 21:07:48 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@59fbb34{/api,null,AVAILABLE,@Spark}
19/10/17 21:07:48 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@6ca372ef{/jobs/job/kill,null,AVAILABLE,@Spark}
19/10/17 21:07:48 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@3ebe4ccc{/stages/stage/kill,null,AVAILABLE,@Spark}
19/10/17 21:07:48 INFO SparkUI: Bound SparkUI to 10.139.64.37, and started at http://10.139.64.37:45433
19/10/17 21:07:48 WARN FairSchedulableBuilder: Fair Scheduler configuration file not found so jobs will be scheduled in FIFO order. To use fair scheduling, configure pools in fairscheduler.xml or set spark.scheduler.allocation.file to a file that contains the configuration.
19/10/17 21:07:48 INFO FairSchedulableBuilder: Created default pool: default, schedulingMode: FIFO, minShare: 0, weight: 1
19/10/17 21:07:48 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://10.139.64.37:7077...
19/10/17 21:07:48 INFO TransportClientFactory: Successfully created connection to /10.139.64.37:7077 after 43 ms (0 ms spent in bootstraps)
19/10/17 21:07:48 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20191017210748-0000
19/10/17 21:07:48 INFO TaskSchedulerImpl: Task preemption enabled.
19/10/17 21:07:48 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33311.
19/10/17 21:07:48 INFO NettyBlockTransferService: Server created on 10.139.64.37:33311
19/10/17 21:07:48 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
19/10/17 21:07:48 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.139.64.37, 33311, None)
19/10/17 21:07:48 INFO BlockManagerMasterEndpoint: Registering block manager 10.139.64.37:33311 with 54.2 GB RAM, BlockManagerId(driver, 10.139.64.37, 33311, None)
19/10/17 21:07:48 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.139.64.37, 33311, None)
19/10/17 21:07:48 INFO BlockManager: external shuffle service port = 4048
19/10/17 21:07:48 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.139.64.37, 33311, None)
19/10/17 21:07:49 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@2582b0ef{/metrics/json,null,AVAILABLE,@Spark}
19/10/17 21:07:49 INFO DBCEventLoggingListener: Initializing DBCEventLoggingListener
19/10/17 21:07:49 INFO DBCEventLoggingListener: Logging events to eventlogs/3380005823756244851/eventlog
19/10/17 21:07:49 INFO SparkContext: Registered listener com.databricks.backend.daemon.driver.DBCEventLoggingListener
19/10/17 21:07:49 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
19/10/17 21:07:49 INFO SparkContext: Loading Spark Service RPC Server
19/10/17 21:07:49 INFO SparkServiceRPCServer: Spark Service RPC Server is disabled.
19/10/17 21:07:49 INFO DatabricksILoop$: Successfully registered spark metrics in Prometheus registry
19/10/17 21:07:49 INFO DatabricksILoop$: Successfully initialized SparkContext
19/10/17 21:07:49 INFO SharedState: Scheduler stats enabled.
19/10/17 21:07:49 INFO SharedState: loading hive config file: file:/databricks/hive/conf/hive-site.xml
19/10/17 21:07:49 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('adl://connectedsystemsuatdl.azuredatalakestore.net/user/hive/warehouse').
19/10/17 21:07:49 INFO SharedState: Warehouse path is 'adl://connectedsystemsuatdl.azuredatalakestore.net/user/hive/warehouse'.
19/10/17 21:07:49 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@15fb4566{/SQL,null,AVAILABLE,@Spark}
19/10/17 21:07:49 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@25ffd826{/SQL/json,null,AVAILABLE,@Spark}
19/10/17 21:07:49 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@1a538ed8{/SQL/execution,null,AVAILABLE,@Spark}
19/10/17 21:07:49 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@78910096{/SQL/execution/json,null,AVAILABLE,@Spark}
19/10/17 21:07:49 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@7364eed1{/static/sql,null,AVAILABLE,@Spark}
19/10/17 21:07:49 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@6fa2448b{/storage/iocache,null,AVAILABLE,@Spark}
19/10/17 21:07:49 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@61bb1e4d{/storage/iocache/json,null,AVAILABLE,@Spark}
19/10/17 21:07:49 INFO LogStore: LogStore class: class com.databricks.tahoe.store.DelegatingLogStore
19/10/17 21:07:49 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20191017210748-0000/0 on worker-20191017210749-10.139.64.32-35989 (10.139.64.32:35989) with 16 core(s)
19/10/17 21:07:49 INFO StandaloneSchedulerBackend: Granted executor ID app-20191017210748-0000/0 on hostPort 10.139.64.32:35989 with 16 core(s), 95.6 GB RAM
19/10/17 21:07:49 INFO DatabricksILoop$: Finished creating throwaway interpreter
19/10/17 21:07:49 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20191017210748-0000/0 is now RUNNING
19/10/17 21:07:50 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
19/10/17 21:07:52 INFO HiveUtils: Initializing execution hive, version 1.2.1
19/10/17 21:07:52 INFO HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
19/10/17 21:07:52 INFO ObjectStore: ObjectStore, initialize called
19/10/17 21:07:52 INFO Persistence: Property spark.hadoop.datanucleus.connectionPool.maxIdle unknown - will be ignored
19/10/17 21:07:52 INFO Persistence: Property spark.hadoop.datanucleus.connectionPool.maxPoolSize unknown - will be ignored
19/10/17 21:07:52 INFO Persistence: Property spark.hadoop.datanucleus.connectionPool.minPoolSize unknown - will be ignored
19/10/17 21:07:52 INFO Persistence: Property spark.hadoop.javax.jdo.option.ConnectionDriverName unknown - will be ignored
19/10/17 21:07:52 INFO Persistence: Property spark.hadoop.javax.jdo.option.ConnectionURL unknown - will be ignored
19/10/17 21:07:52 INFO Persistence: Property spark.hadoop.javax.jdo.option.ConnectionUserName unknown - will be ignored
19/10/17 21:07:52 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
19/10/17 21:07:52 INFO Persistence: Property spark.hadoop.javax.jdo.option.ConnectionPassword unknown - will be ignored
19/10/17 21:07:52 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
19/10/17 21:07:52 INFO Persistence: Property datanucleus.schema.autoCreateAll unknown - will be ignored
19/10/17 21:07:52 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.139.64.32:60062) with ID 0
19/10/17 21:07:52 INFO BlockManagerMasterEndpoint: Registering block manager 10.139.64.32:33807 with 50.8 GB RAM, BlockManagerId(0, 10.139.64.32, 33807, None)
19/10/17 21:07:54 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
19/10/17 21:07:55 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
19/10/17 21:07:55 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
19/10/17 21:07:56 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
19/10/17 21:07:56 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
19/10/17 21:07:56 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
19/10/17 21:07:56 INFO ObjectStore: Initialized ObjectStore
19/10/17 21:07:56 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0
19/10/17 21:07:56 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
19/10/17 21:07:57 INFO HiveMetaStore: Added admin role in metastore
19/10/17 21:07:57 INFO HiveMetaStore: Added public role in metastore
19/10/17 21:07:57 INFO HiveMetaStore: No user is added in admin role, since config is empty
19/10/17 21:07:57 INFO HiveMetaStore: 0: get_all_databases
19/10/17 21:07:57 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_all_databases	
19/10/17 21:07:57 INFO HiveMetaStore: 0: get_functions: db=default pat=*
19/10/17 21:07:57 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
19/10/17 21:07:57 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MResourceUri" is tagged as "embedded-only" so does not have its own datastore table.
19/10/17 21:07:57 INFO JettyClient$: Creating new HttpClient with SSLContextFactory=None,maxRequestHeaderSize=65536, namePrefix=Some(DBFSV2), idleTimeout=2 hours
19/10/17 21:07:58 WARN EC2MetadataUtils: Unable to retrieve the requested metadata.
19/10/17 21:07:58 INFO deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum
19/10/17 21:07:58 INFO DatabricksFileSystemV2Factory: Creating wasbs file system for wasbs://root@dbstorage5cunvudtvyhjm.blob.core.windows.net
19/10/17 21:07:58 INFO DBFS: Initialized DBFS with DBFSV2 as the delegate.
19/10/17 21:07:58 INFO SessionState: Created local directory: /local_disk0/tmp/root
19/10/17 21:07:58 INFO SessionState: Created local directory: /local_disk0/tmp/0cb6bd6b-8caa-4e3c-9834-bb79be938173_resources
19/10/17 21:07:58 INFO SessionState: Created HDFS directory: /tmp/hive/root/0cb6bd6b-8caa-4e3c-9834-bb79be938173
19/10/17 21:07:58 INFO SessionState: Created local directory: /local_disk0/tmp/root/0cb6bd6b-8caa-4e3c-9834-bb79be938173
19/10/17 21:07:58 INFO SessionState: Created HDFS directory: /tmp/hive/root/0cb6bd6b-8caa-4e3c-9834-bb79be938173/_tmp_space.db
19/10/17 21:07:58 INFO HiveClientImpl: Warehouse location for Hive client (version 1.2.2) is adl://connectedsystemsuatdl.azuredatalakestore.net/user/hive/warehouse
19/10/17 21:07:58 INFO SessionManager: Operation log root directory is created: /local_disk0/tmp/root/operation_logs
19/10/17 21:07:58 INFO SessionManager: HiveServer2: Background operation thread pool size: 100
19/10/17 21:07:58 INFO SessionManager: HiveServer2: Background operation thread wait queue size: 100
19/10/17 21:07:58 INFO SessionManager: HiveServer2: Background operation thread keepalive time: 10 seconds
19/10/17 21:07:58 INFO AbstractService: Service:OperationManager is inited.
19/10/17 21:07:58 INFO AbstractService: Service:SessionManager is inited.
19/10/17 21:07:58 INFO AbstractService: Service: CLIService is inited.
19/10/17 21:07:58 INFO AbstractService: Service:ThriftHttpCLIService is inited.
19/10/17 21:07:58 INFO AbstractService: Service: HiveServer2 is inited.
19/10/17 21:07:58 INFO AbstractService: Service:OperationManager is started.
19/10/17 21:07:58 INFO AbstractService: Service:SessionManager is started.
19/10/17 21:07:58 INFO AbstractService: Service:CLIService is started.
19/10/17 21:07:58 INFO ObjectStore: ObjectStore, initialize called
19/10/17 21:07:58 INFO Query: Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
19/10/17 21:07:58 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
19/10/17 21:07:58 INFO ObjectStore: Initialized ObjectStore
19/10/17 21:07:58 INFO HiveMetaStore: 0: get_databases: default
19/10/17 21:07:58 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_databases: default	
19/10/17 21:07:58 INFO HiveMetaStore: 0: Shutting down the object store...
19/10/17 21:07:58 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=Shutting down the object store...	
19/10/17 21:07:58 INFO HiveMetaStore: 0: Metastore shutdown complete.
19/10/17 21:07:58 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=Metastore shutdown complete.	
19/10/17 21:07:58 INFO AbstractService: Service:ThriftHttpCLIService is started.
19/10/17 21:07:58 INFO AbstractService: Service:HiveServer2 is started.
19/10/17 21:07:58 INFO ThriftCLIService: HTTP Server SSL: adding excluded protocols: [SSLv2, SSLv3]
19/10/17 21:07:58 INFO ThriftCLIService: HTTP Server SSL: SslContextFactory.getExcludeProtocols = [SSL, SSLv2, SSLv2Hello, SSLv3]
19/10/17 21:07:58 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@2db4a84a{/sqlserver,null,AVAILABLE,@Spark}
19/10/17 21:07:58 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@5cef5fc9{/sqlserver/json,null,AVAILABLE,@Spark}
19/10/17 21:07:58 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@55a29589{/sqlserver/session,null,AVAILABLE,@Spark}
19/10/17 21:07:58 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@6d5fea64{/sqlserver/session/json,null,AVAILABLE,@Spark}
19/10/17 21:07:58 INFO DriverDaemon: Starting driver daemon...
19/10/17 21:07:58 INFO SparkConfUtils$: Customize spark config according to file /tmp/custom-spark.conf
19/10/17 21:07:58 INFO SparkConfUtils$: new spark config: spark.sql.warehouse.dir -> adl://connectedsystemsuatdl.azuredatalakestore.net/user/hive/warehouse
19/10/17 21:07:58 INFO SparkConfUtils$: new spark config: spark.hadoop.javax.jdo.option.ConnectionPassword -> 560I890@fd80709
19/10/17 21:07:58 INFO SparkConfUtils$: new spark config: spark.hadoop.dfs.adls.oauth2.credential -> viUHaEfNg8cHQRJSRNMzpIv1T6goiUg+c9R/XSbYBFA=
19/10/17 21:07:58 INFO SparkConfUtils$: new spark config: spark.hadoop.javax.jdo.option.ConnectionDriverName -> org.postgresql.Driver
19/10/17 21:07:58 INFO SparkConfUtils$: new spark config: spark.databricks.acl.dfAclsEnabled -> false
19/10/17 21:07:58 INFO SparkConfUtils$: new spark config: spark.driver.tempDirectory -> /local_disk0/tmp
19/10/17 21:07:58 INFO SparkConfUtils$: new spark config: spark.hadoop.datanucleus.connectionPool.maxIdle -> 1
19/10/17 21:07:58 INFO SparkConfUtils$: new spark config: spark.hadoop.dfs.adls.oauth2.client.id -> 003106c3-ff8c-4c62-a060-24b8b32ea512
19/10/17 21:07:58 INFO SparkConfUtils$: new spark config: spark.hadoop.dfs.adls.oauth2.refresh.url -> https://login.microsoftonline.com/6ac7a1f4-5fb1-4153-bb4f-12d2020a1f7d/oauth2/token
19/10/17 21:07:58 INFO SparkConfUtils$: new spark config: spark.executor.tempDirectory -> /local_disk0/tmp
19/10/17 21:07:58 INFO SparkConfUtils$: new spark config: spark.databricks.delta.preview.enabled -> true
19/10/17 21:07:58 INFO SparkConfUtils$: new spark config: spark.task.maxFailures -> 10
19/10/17 21:07:58 INFO SparkConfUtils$: new spark config: spark.sql.hive.metastore.jars -> maven
19/10/17 21:07:58 INFO SparkConfUtils$: new spark config: spark.databricks.aggressiveWindowDownS -> 300
19/10/17 21:07:58 INFO SparkConfUtils$: new spark config: spark.hadoop.javax.jdo.option.ConnectionUserName -> tedbuat
19/10/17 21:07:58 INFO SparkConfUtils$: new spark config: spark.databricks.sql.hive.msck.batchSize -> 200
19/10/17 21:07:58 INFO SparkConfUtils$: new spark config: spark.hadoop.javax.jdo.option.ConnectionURL -> jdbc:postgresql://uathmsdb.trans.enterprisecsp.net:5432/hive_adls
19/10/17 21:07:58 INFO SparkConfUtils$: new spark config: datanucleus.fixedDatastore -> false
19/10/17 21:07:58 INFO SparkConfUtils$: new spark config: spark.databricks.sql.hive.msck.parallelism -> 20
19/10/17 21:07:58 INFO SparkConfUtils$: new spark config: spark.hadoop.datanucleus.connectionPool.maxPoolSize -> 30
19/10/17 21:07:58 INFO SparkConfUtils$: new spark config: spark.hadoop.datanucleus.connectionPool.minPoolSize -> 15
19/10/17 21:07:58 INFO SparkConfUtils$: new spark config: spark.sql.parquet.filterPushdown -> false
19/10/17 21:07:58 INFO SparkConfUtils$: new spark config: spark.hadoop.dfs.adls.oauth2.access.token.provider.type -> ClientCredential
19/10/17 21:07:58 INFO SparkConfUtils$: new spark config: spark.sql.shuffle.partitions -> 2000
19/10/17 21:07:58 INFO SparkConfUtils$: new spark config: spark.driver.maxResultSize -> 0
19/10/17 21:07:58 INFO SparkConfUtils$: new spark config: fs.azure.account.key.teuatsa.blob.core.windows.net -> IsrxMnnzLeO9N44beU4eMw0LaiKJdi1hHmKZFUiPa8nFNKZAFyhH0Hv/UCoC4qhgcdq63zwTRGFhUKV9d4ZsBw==
19/10/17 21:07:58 INFO SparkConfUtils$: new spark config: datanucleus.autoCreateSchema -> true
19/10/17 21:07:58 INFO SparkConfUtils$: new spark config: spark.hadoop.fs.adl.impl.disable.cache -> false
19/10/17 21:07:58 INFO SparkConfUtils$: new spark config: spark.sql.hive.metastore.version -> 2.3.0
19/10/17 21:07:58 WARN SparkConf: The configuration key 'spark.scheduler.listenerbus.eventqueue.size' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.scheduler.listenerbus.eventqueue.capacity' instead.
19/10/17 21:07:58 WARN SparkConf: The configuration key 'spark.akka.frameSize' has been deprecated as of Spark 1.6 and may be removed in the future. Please use the new key 'spark.rpc.message.maxSize' instead.
19/10/17 21:07:58 INFO DriverDaemon$$anon$1: Message out thread ready
19/10/17 21:07:58 INFO Server: jetty-9.3.20.v20170531
19/10/17 21:07:58 INFO AbstractConnector: Started ServerConnector@91c11b6{HTTP/1.1,[http/1.1]}{0.0.0.0:6061}
19/10/17 21:07:58 INFO Server: Started @15509ms
19/10/17 21:07:59 INFO DriverDaemon: Driver daemon started.
19/10/17 21:07:59 INFO Server: jetty-9.3.20.v20170531
19/10/17 21:07:59 WARN SecurityHandler: ServletContext@o.e.j.s.ServletContextHandler@2096a5ff{/,null,STARTING} has uncovered http methods for path: /*
19/10/17 21:07:59 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@2096a5ff{/,null,AVAILABLE}
19/10/17 21:07:59 INFO SslContextFactory: x509=X509@6191bea1(1,h=[databrickscloud.com],w=[]) for SslContextFactory@41931a2e(file:///databricks/keys/jetty-ssl-driver-keystore.jks,null)
19/10/17 21:07:59 INFO AbstractConnector: Started ServerConnector@7ef329df{SSL,[ssl, http/1.1]}{0.0.0.0:10000}
19/10/17 21:07:59 INFO Server: Started @15618ms
19/10/17 21:07:59 INFO ThriftCLIService: Started ThriftHttpCLIService in https mode on port 10000 path=/cliservice/* with 5...500 worker threads
19/10/17 21:07:59 INFO DriverCorral: Loading the root classloader
19/10/17 21:07:59 INFO DriverCorral: Starting repl ReplId-2fa31-8b167-9ae9d
19/10/17 21:07:59 INFO DriverCorral: Starting repl ReplId-27511-6e59b-8f9ac-8
19/10/17 21:07:59 INFO SQLDriverWrapper: setupRepl:ReplId-27511-6e59b-8f9ac-8: finished to load
19/10/17 21:07:59 INFO SQLDriverWrapper: setupRepl:ReplId-2fa31-8b167-9ae9d: finished to load
19/10/17 21:07:59 INFO DriverCorral: Starting repl ReplId-158e9-ac213-009a4-a
19/10/17 21:07:59 INFO SQLDriverWrapper: setupRepl:ReplId-158e9-ac213-009a4-a: finished to load
19/10/17 21:07:59 INFO DriverCorral: Starting repl ReplId-50876-c4ecc-638f
19/10/17 21:07:59 INFO SQLDriverWrapper: setupRepl:ReplId-50876-c4ecc-638f: finished to load
19/10/17 21:07:59 INFO DriverCorral: Starting repl ReplId-4e54e-451fa-68b75-4
19/10/17 21:07:59 INFO SQLDriverWrapper: setupRepl:ReplId-4e54e-451fa-68b75-4: finished to load
19/10/17 21:07:59 INFO DriverCorral: Starting repl ReplId-24689-2bb65-56f28-6
19/10/17 21:07:59 INFO RDriverLocal: 1. RDriverLocal.ffa5ed0e-af50-4c6f-b80b-e6241918911c: object created with for ReplId-24689-2bb65-56f28-6.
19/10/17 21:07:59 INFO RDriverLocal: 2. RDriverLocal.ffa5ed0e-af50-4c6f-b80b-e6241918911c: initializing ...
19/10/17 21:07:59 INFO RDriverLocal: 3. RDriverLocal.ffa5ed0e-af50-4c6f-b80b-e6241918911c: started RBackend thread on port 34531
19/10/17 21:07:59 INFO RDriverLocal: 4. RDriverLocal.ffa5ed0e-af50-4c6f-b80b-e6241918911c: waiting for SparkR to be installed ...
19/10/17 21:08:02 INFO LibraryDownloadManager: Downloading a library that was not in the cache: UploadedLibrary(org/rogach/scallop_2.11-3.3.1.jar,dbfs:/FileStore/jars/maven/org/rogach/scallop_2.11-3.3.1.jar,scala)
19/10/17 21:08:03 INFO LibraryDownloadManager: Downloaded library org/rogach/scallop_2.11-3.3.1.jar as local file /local_disk0/tmp/addedFile3437214070662729572org_rogach_scallop_2_11_3_3_1-ecc25.jar
19/10/17 21:08:03 INFO SharedDriverContext: Adding libraries that haven't been added before: Set(/local_disk0/tmp/addedFile3437214070662729572org_rogach_scallop_2_11_3_3_1-ecc25.jar)
19/10/17 21:08:03 INFO SparkContext: Added file /local_disk0/tmp/addedFile3437214070662729572org_rogach_scallop_2_11_3_3_1-ecc25.jar at spark://10.139.64.37:32805/files/addedFile3437214070662729572org_rogach_scallop_2_11_3_3_1-ecc25.jar with timestamp 1571346483114
19/10/17 21:08:03 INFO Utils: Copying /local_disk0/tmp/addedFile3437214070662729572org_rogach_scallop_2_11_3_3_1-ecc25.jar to /local_disk0/spark-299327bc-7e6a-4596-be59-7d341f09ac4a/userFiles-d34d4113-bf39-4d51-a9dd-ceb72c2e45ea/addedFile3437214070662729572org_rogach_scallop_2_11_3_3_1-ecc25.jar
19/10/17 21:08:03 INFO SparkContext: Added JAR /local_disk0/tmp/addedFile3437214070662729572org_rogach_scallop_2_11_3_3_1-ecc25.jar at spark://10.139.64.37:32805/jars/addedFile3437214070662729572org_rogach_scallop_2_11_3_3_1-ecc25.jar with timestamp 1571346483129
19/10/17 21:08:03 INFO LibraryDownloadManager: Downloading a library that was not in the cache: UploadedLibrary(com/typesafe/config-1.3.1.jar,dbfs:/FileStore/jars/maven/com/typesafe/config-1.3.1.jar,scala)
19/10/17 21:08:03 INFO LibraryDownloadManager: Downloaded library com/typesafe/config-1.3.1.jar as local file /local_disk0/tmp/addedFile921809598074381246com_typesafe_config_1_3_1-e3e42.jar
19/10/17 21:08:03 INFO SharedDriverContext: Adding libraries that haven't been added before: Set(/local_disk0/tmp/addedFile921809598074381246com_typesafe_config_1_3_1-e3e42.jar)
19/10/17 21:08:03 INFO SparkContext: Added file /local_disk0/tmp/addedFile921809598074381246com_typesafe_config_1_3_1-e3e42.jar at spark://10.139.64.37:32805/files/addedFile921809598074381246com_typesafe_config_1_3_1-e3e42.jar with timestamp 1571346483288
19/10/17 21:08:03 INFO Utils: Copying /local_disk0/tmp/addedFile921809598074381246com_typesafe_config_1_3_1-e3e42.jar to /local_disk0/spark-299327bc-7e6a-4596-be59-7d341f09ac4a/userFiles-d34d4113-bf39-4d51-a9dd-ceb72c2e45ea/addedFile921809598074381246com_typesafe_config_1_3_1-e3e42.jar
19/10/17 21:08:03 INFO SparkContext: Added JAR /local_disk0/tmp/addedFile921809598074381246com_typesafe_config_1_3_1-e3e42.jar at spark://10.139.64.37:32805/jars/addedFile921809598074381246com_typesafe_config_1_3_1-e3e42.jar with timestamp 1571346483296
19/10/17 21:08:03 INFO LibraryDownloadManager: Downloading a library that was not in the cache: UploadedLibrary(xml-apis/xml-apis-1.0.b2.jar,dbfs:/FileStore/jars/maven/xml-apis/xml-apis-1.0.b2.jar,scala)
19/10/17 21:08:03 INFO LibraryDownloadManager: Downloaded library xml-apis/xml-apis-1.0.b2.jar as local file /local_disk0/tmp/addedFile9193930907825338753xml_apis_xml_apis_1_0_b2-167e6.jar
19/10/17 21:08:03 INFO SharedDriverContext: Adding libraries that haven't been added before: Set(/local_disk0/tmp/addedFile9193930907825338753xml_apis_xml_apis_1_0_b2-167e6.jar)
19/10/17 21:08:03 INFO SparkContext: Added file /local_disk0/tmp/addedFile9193930907825338753xml_apis_xml_apis_1_0_b2-167e6.jar at spark://10.139.64.37:32805/files/addedFile9193930907825338753xml_apis_xml_apis_1_0_b2-167e6.jar with timestamp 1571346483455
19/10/17 21:08:03 INFO Utils: Copying /local_disk0/tmp/addedFile9193930907825338753xml_apis_xml_apis_1_0_b2-167e6.jar to /local_disk0/spark-299327bc-7e6a-4596-be59-7d341f09ac4a/userFiles-d34d4113-bf39-4d51-a9dd-ceb72c2e45ea/addedFile9193930907825338753xml_apis_xml_apis_1_0_b2-167e6.jar
19/10/17 21:08:03 INFO SparkContext: Added JAR /local_disk0/tmp/addedFile9193930907825338753xml_apis_xml_apis_1_0_b2-167e6.jar at spark://10.139.64.37:32805/jars/addedFile9193930907825338753xml_apis_xml_apis_1_0_b2-167e6.jar with timestamp 1571346483463
19/10/17 21:08:03 INFO LibraryDownloadManager: Downloading a library that was not in the cache: UploadedLibrary(dom4j/dom4j-1.6.1.jar,dbfs:/FileStore/jars/maven/dom4j/dom4j-1.6.1.jar,scala)
19/10/17 21:08:03 INFO LibraryDownloadManager: Downloaded library dom4j/dom4j-1.6.1.jar as local file /local_disk0/tmp/addedFile5514790601964570712dom4j_dom4j_1_6_1-3b35e.jar
19/10/17 21:08:03 INFO SharedDriverContext: Adding libraries that haven't been added before: Set(/local_disk0/tmp/addedFile5514790601964570712dom4j_dom4j_1_6_1-3b35e.jar)
19/10/17 21:08:03 INFO SparkContext: Added file /local_disk0/tmp/addedFile5514790601964570712dom4j_dom4j_1_6_1-3b35e.jar at spark://10.139.64.37:32805/files/addedFile5514790601964570712dom4j_dom4j_1_6_1-3b35e.jar with timestamp 1571346483594
19/10/17 21:08:03 INFO Utils: Copying /local_disk0/tmp/addedFile5514790601964570712dom4j_dom4j_1_6_1-3b35e.jar to /local_disk0/spark-299327bc-7e6a-4596-be59-7d341f09ac4a/userFiles-d34d4113-bf39-4d51-a9dd-ceb72c2e45ea/addedFile5514790601964570712dom4j_dom4j_1_6_1-3b35e.jar
19/10/17 21:08:03 INFO SparkContext: Added JAR /local_disk0/tmp/addedFile5514790601964570712dom4j_dom4j_1_6_1-3b35e.jar at spark://10.139.64.37:32805/jars/addedFile5514790601964570712dom4j_dom4j_1_6_1-3b35e.jar with timestamp 1571346483602
19/10/17 21:08:03 INFO LibraryDownloadManager: Downloading a library that was not in the cache: UploadedLibrary(dbfs:/FileStore/saak9001/pqm_core_models-1.0.jar,dbfs:/FileStore/saak9001/pqm_core_models-1.0.jar,scala)
19/10/17 21:08:03 INFO LibraryDownloadManager: Downloaded library dbfs:/FileStore/saak9001/pqm_core_models-1.0.jar as local file /local_disk0/tmp/addedFile4063054051174557443dbfs__FileStore_saak9001_pqm_core_models_1_0-b0ccd.jar
19/10/17 21:08:03 INFO SharedDriverContext: Adding libraries that haven't been added before: Set(/local_disk0/tmp/addedFile4063054051174557443dbfs__FileStore_saak9001_pqm_core_models_1_0-b0ccd.jar)
19/10/17 21:08:03 INFO SparkContext: Added file /local_disk0/tmp/addedFile4063054051174557443dbfs__FileStore_saak9001_pqm_core_models_1_0-b0ccd.jar at spark://10.139.64.37:32805/files/addedFile4063054051174557443dbfs__FileStore_saak9001_pqm_core_models_1_0-b0ccd.jar with timestamp 1571346483968
19/10/17 21:08:03 INFO Utils: Copying /local_disk0/tmp/addedFile4063054051174557443dbfs__FileStore_saak9001_pqm_core_models_1_0-b0ccd.jar to /local_disk0/spark-299327bc-7e6a-4596-be59-7d341f09ac4a/userFiles-d34d4113-bf39-4d51-a9dd-ceb72c2e45ea/addedFile4063054051174557443dbfs__FileStore_saak9001_pqm_core_models_1_0-b0ccd.jar
19/10/17 21:08:03 INFO SparkContext: Added JAR /local_disk0/tmp/addedFile4063054051174557443dbfs__FileStore_saak9001_pqm_core_models_1_0-b0ccd.jar at spark://10.139.64.37:32805/jars/addedFile4063054051174557443dbfs__FileStore_saak9001_pqm_core_models_1_0-b0ccd.jar with timestamp 1571346483976
19/10/17 21:08:03 INFO LibraryDownloadManager: Downloading a library that was not in the cache: UploadedLibrary(io/spray/spray-json_2.11-1.3.5.jar,dbfs:/FileStore/jars/maven/io/spray/spray-json_2.11-1.3.5.jar,scala)
19/10/17 21:08:04 INFO LibraryDownloadManager: Downloaded library io/spray/spray-json_2.11-1.3.5.jar as local file /local_disk0/tmp/addedFile7430104289198212865io_spray_spray_json_2_11_1_3_5-d5f78.jar
19/10/17 21:08:04 INFO SharedDriverContext: Adding libraries that haven't been added before: Set(/local_disk0/tmp/addedFile7430104289198212865io_spray_spray_json_2_11_1_3_5-d5f78.jar)
19/10/17 21:08:04 INFO SparkContext: Added file /local_disk0/tmp/addedFile7430104289198212865io_spray_spray_json_2_11_1_3_5-d5f78.jar at spark://10.139.64.37:32805/files/addedFile7430104289198212865io_spray_spray_json_2_11_1_3_5-d5f78.jar with timestamp 1571346484077
19/10/17 21:08:04 INFO Utils: Copying /local_disk0/tmp/addedFile7430104289198212865io_spray_spray_json_2_11_1_3_5-d5f78.jar to /local_disk0/spark-299327bc-7e6a-4596-be59-7d341f09ac4a/userFiles-d34d4113-bf39-4d51-a9dd-ceb72c2e45ea/addedFile7430104289198212865io_spray_spray_json_2_11_1_3_5-d5f78.jar
19/10/17 21:08:04 INFO SparkContext: Added JAR /local_disk0/tmp/addedFile7430104289198212865io_spray_spray_json_2_11_1_3_5-d5f78.jar at spark://10.139.64.37:32805/jars/addedFile7430104289198212865io_spray_spray_json_2_11_1_3_5-d5f78.jar with timestamp 1571346484084
19/10/17 21:08:04 INFO LibraryDownloadManager: Downloading a library that was not in the cache: UploadedLibrary(dbfs:/FileStore/saak9001/transengine-1.0-SNAPSHOT.jar,dbfs:/FileStore/saak9001/transengine-1.0-SNAPSHOT.jar,scala)
19/10/17 21:08:04 INFO LibraryDownloadManager: Downloaded library dbfs:/FileStore/saak9001/transengine-1.0-SNAPSHOT.jar as local file /local_disk0/tmp/addedFile2730237718112202115dbfs__FileStore_saak9001_transengine_1_0_SNAPSHOT-ab9b9.jar
19/10/17 21:08:04 INFO SharedDriverContext: Adding libraries that haven't been added before: Set(/local_disk0/tmp/addedFile2730237718112202115dbfs__FileStore_saak9001_transengine_1_0_SNAPSHOT-ab9b9.jar)
19/10/17 21:08:04 INFO SparkContext: Added file /local_disk0/tmp/addedFile2730237718112202115dbfs__FileStore_saak9001_transengine_1_0_SNAPSHOT-ab9b9.jar at spark://10.139.64.37:32805/files/addedFile2730237718112202115dbfs__FileStore_saak9001_transengine_1_0_SNAPSHOT-ab9b9.jar with timestamp 1571346484186
19/10/17 21:08:04 INFO Utils: Copying /local_disk0/tmp/addedFile2730237718112202115dbfs__FileStore_saak9001_transengine_1_0_SNAPSHOT-ab9b9.jar to /local_disk0/spark-299327bc-7e6a-4596-be59-7d341f09ac4a/userFiles-d34d4113-bf39-4d51-a9dd-ceb72c2e45ea/addedFile2730237718112202115dbfs__FileStore_saak9001_transengine_1_0_SNAPSHOT-ab9b9.jar
19/10/17 21:08:04 INFO SparkContext: Added JAR /local_disk0/tmp/addedFile2730237718112202115dbfs__FileStore_saak9001_transengine_1_0_SNAPSHOT-ab9b9.jar at spark://10.139.64.37:32805/jars/addedFile2730237718112202115dbfs__FileStore_saak9001_transengine_1_0_SNAPSHOT-ab9b9.jar with timestamp 1571346484193
19/10/17 21:08:04 INFO LibraryDownloadManager: Downloading a library that was not in the cache: UploadedLibrary(org/scalaj/scalaj-http_2.11-2.3.0.jar,dbfs:/FileStore/jars/maven/org/scalaj/scalaj-http_2.11-2.3.0.jar,scala)
19/10/17 21:08:04 INFO LibraryDownloadManager: Downloaded library org/scalaj/scalaj-http_2.11-2.3.0.jar as local file /local_disk0/tmp/addedFile5190065922075015895org_scalaj_scalaj_http_2_11_2_3_0-d69e4.jar
19/10/17 21:08:04 INFO SharedDriverContext: Adding libraries that haven't been added before: Set(/local_disk0/tmp/addedFile5190065922075015895org_scalaj_scalaj_http_2_11_2_3_0-d69e4.jar)
19/10/17 21:08:04 INFO SparkContext: Added file /local_disk0/tmp/addedFile5190065922075015895org_scalaj_scalaj_http_2_11_2_3_0-d69e4.jar at spark://10.139.64.37:32805/files/addedFile5190065922075015895org_scalaj_scalaj_http_2_11_2_3_0-d69e4.jar with timestamp 1571346484265
19/10/17 21:08:04 INFO Utils: Copying /local_disk0/tmp/addedFile5190065922075015895org_scalaj_scalaj_http_2_11_2_3_0-d69e4.jar to /local_disk0/spark-299327bc-7e6a-4596-be59-7d341f09ac4a/userFiles-d34d4113-bf39-4d51-a9dd-ceb72c2e45ea/addedFile5190065922075015895org_scalaj_scalaj_http_2_11_2_3_0-d69e4.jar
19/10/17 21:08:04 INFO SparkContext: Added JAR /local_disk0/tmp/addedFile5190065922075015895org_scalaj_scalaj_http_2_11_2_3_0-d69e4.jar at spark://10.139.64.37:32805/jars/addedFile5190065922075015895org_scalaj_scalaj_http_2_11_2_3_0-d69e4.jar with timestamp 1571346484274
19/10/17 21:08:05 INFO RDriverLocal$: SparkR installation completed.
19/10/17 21:08:05 INFO RDriverLocal: 5. RDriverLocal.ffa5ed0e-af50-4c6f-b80b-e6241918911c: launching R process ...
19/10/17 21:08:05 INFO RDriverLocal: 6. RDriverLocal.ffa5ed0e-af50-4c6f-b80b-e6241918911c: cgroup isolation disabled, not placing R process in REPL cgroup.
19/10/17 21:08:05 INFO RDriverLocal: 7. RDriverLocal.ffa5ed0e-af50-4c6f-b80b-e6241918911c: starting R process on port 52568 (attempt 1) ...
19/10/17 21:08:07 INFO RDriverLocal: 8. RDriverLocal.ffa5ed0e-af50-4c6f-b80b-e6241918911c: R process started with RServe listening on port 52568.
19/10/17 21:08:07 INFO RDriverLocal: 9. RDriverLocal.ffa5ed0e-af50-4c6f-b80b-e6241918911c: setting up BufferedStreamThread with bufferSize: 100.
19/10/17 21:08:08 INFO RDriverLocal: 10. RDriverLocal.ffa5ed0e-af50-4c6f-b80b-e6241918911c: starting interpreter to talk to R process ...
19/10/17 21:08:08 WARN SparkContext: Using an existing SparkContext; some configuration may not take effect.
19/10/17 21:08:08 INFO RDriverLocal: 11. RDriverLocal.ffa5ed0e-af50-4c6f-b80b-e6241918911c: R interpretter is connected.
19/10/17 21:08:08 INFO RDriverWrapper: setupRepl:ReplId-24689-2bb65-56f28-6: finished to load
19/10/17 21:08:13 INFO DriverCorral: Starting repl ReplId-7dfa9-f3c0f-e1909-9
19/10/17 21:08:13 INFO LogicalPlanStats: Setting LogicalPlanStats visitor to com.databricks.sql.optimizer.statsEstimation.DatabricksLogicalPlanStatsVisitor$
19/10/17 21:08:13 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.0 using maven.
19/10/17 21:08:13 INFO IsolatedClientLoader: Initiating download of metastore jars from maven. This may take a while and is not recommended for production use. Please follow the instructions here: https://docs.databricks.com/user-guide/advanced/external-hive-metastore.html#spark-options on how to download the jars just once and use them in your cluster configuration. A log message beginning with 'Downloaded metastore jars' will print once the download is complete.
19/10/17 21:09:10 INFO IsolatedClientLoader: Downloaded metastore jars to /local_disk0/tmp/hive-v2_3-84baf06f-c2de-4447-9d67-3ed1b5044534
19/10/17 21:09:10 INFO HiveConf: Found configuration file null
19/10/17 21:09:11 INFO SessionState: Created HDFS directory: /tmp/hive/root/0e3a0ccf-4639-4bf6-9b07-f9c0bfe21145
19/10/17 21:09:11 INFO SessionState: Created local directory: /local_disk0/tmp/root/0e3a0ccf-4639-4bf6-9b07-f9c0bfe21145
19/10/17 21:09:11 INFO SessionState: Created HDFS directory: /tmp/hive/root/0e3a0ccf-4639-4bf6-9b07-f9c0bfe21145/_tmp_space.db
19/10/17 21:09:11 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.3) is adl://connectedsystemsuatdl.azuredatalakestore.net/user/hive/warehouse
19/10/17 21:09:11 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
19/10/17 21:09:11 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
19/10/17 21:09:11 INFO HiveMetaStore: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
19/10/17 21:09:11 INFO ObjectStore: ObjectStore, initialize called
19/10/17 21:09:11 INFO Persistence: Property spark.hadoop.datanucleus.connectionPool.maxIdle unknown - will be ignored
19/10/17 21:09:11 INFO Persistence: Property spark.hadoop.datanucleus.connectionPool.maxPoolSize unknown - will be ignored
19/10/17 21:09:11 INFO Persistence: Property datanucleus.fixedDatastore unknown - will be ignored
19/10/17 21:09:11 INFO Persistence: Property spark.hadoop.datanucleus.connectionPool.minPoolSize unknown - will be ignored
19/10/17 21:09:11 INFO Persistence: Property spark.hadoop.javax.jdo.option.ConnectionDriverName unknown - will be ignored
19/10/17 21:09:11 INFO Persistence: Property spark.hadoop.javax.jdo.option.ConnectionURL unknown - will be ignored
19/10/17 21:09:11 INFO Persistence: Property spark.hadoop.javax.jdo.option.ConnectionUserName unknown - will be ignored
19/10/17 21:09:11 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
19/10/17 21:09:11 INFO Persistence: Property spark.hadoop.javax.jdo.option.ConnectionPassword unknown - will be ignored
19/10/17 21:09:11 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
19/10/17 21:09:12 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
19/10/17 21:09:13 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is POSTGRES
19/10/17 21:09:13 INFO ObjectStore: Initialized ObjectStore
19/10/17 21:09:13 INFO HiveMetaStore: Added admin role in metastore
19/10/17 21:09:13 INFO HiveMetaStore: Added public role in metastore
19/10/17 21:09:13 INFO HiveMetaStore: No user is added in admin role, since config is empty
19/10/17 21:09:13 INFO HiveMetaStore: 0: get_all_functions
19/10/17 21:09:13 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_all_functions	
19/10/17 21:09:13 INFO Hive: Registering function collect brickhouse.udf.collect.CollectUDAF
19/10/17 21:09:13 INFO Hive: Registering function collect brickhouse.udf.collect.CollectUDAF
19/10/17 21:09:13 INFO HiveMetaStore: 0: get_database: default
19/10/17 21:09:13 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
19/10/17 21:09:13 INFO SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=0e3a0ccf-4639-4bf6-9b07-f9c0bfe21145, clientType=HIVECLI]
19/10/17 21:09:13 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.
19/10/17 21:09:13 INFO metastore: Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
19/10/17 21:09:13 INFO HiveMetaStore: 0: Cleaning up thread local RawStore...
19/10/17 21:09:13 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
19/10/17 21:09:13 INFO HiveMetaStore: 0: Done cleaning up thread local RawStore
19/10/17 21:09:13 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
19/10/17 21:09:13 INFO SessionState: Added [/local_disk0/tmp/addedFile9193930907825338753xml_apis_xml_apis_1_0_b2-167e6.jar] to class path
19/10/17 21:09:13 INFO SessionState: Added resources: [/local_disk0/tmp/addedFile9193930907825338753xml_apis_xml_apis_1_0_b2-167e6.jar]
19/10/17 21:09:13 WARN SparkContext: The jar /local_disk0/tmp/addedFile9193930907825338753xml_apis_xml_apis_1_0_b2-167e6.jar has been added already. Overwriting of added jars is not supported in the current version.
19/10/17 21:09:13 INFO SessionState: Added [/local_disk0/tmp/addedFile5190065922075015895org_scalaj_scalaj_http_2_11_2_3_0-d69e4.jar] to class path
19/10/17 21:09:13 INFO SessionState: Added resources: [/local_disk0/tmp/addedFile5190065922075015895org_scalaj_scalaj_http_2_11_2_3_0-d69e4.jar]
19/10/17 21:09:13 WARN SparkContext: The jar /local_disk0/tmp/addedFile5190065922075015895org_scalaj_scalaj_http_2_11_2_3_0-d69e4.jar has been added already. Overwriting of added jars is not supported in the current version.
19/10/17 21:09:13 INFO SessionState: Added [/local_disk0/tmp/addedFile2730237718112202115dbfs__FileStore_saak9001_transengine_1_0_SNAPSHOT-ab9b9.jar] to class path
19/10/17 21:09:13 INFO SessionState: Added resources: [/local_disk0/tmp/addedFile2730237718112202115dbfs__FileStore_saak9001_transengine_1_0_SNAPSHOT-ab9b9.jar]
19/10/17 21:09:13 WARN SparkContext: The jar /local_disk0/tmp/addedFile2730237718112202115dbfs__FileStore_saak9001_transengine_1_0_SNAPSHOT-ab9b9.jar has been added already. Overwriting of added jars is not supported in the current version.
19/10/17 21:09:13 INFO SessionState: Added [/local_disk0/tmp/addedFile3437214070662729572org_rogach_scallop_2_11_3_3_1-ecc25.jar] to class path
19/10/17 21:09:13 INFO SessionState: Added resources: [/local_disk0/tmp/addedFile3437214070662729572org_rogach_scallop_2_11_3_3_1-ecc25.jar]
19/10/17 21:09:13 WARN SparkContext: The jar /local_disk0/tmp/addedFile3437214070662729572org_rogach_scallop_2_11_3_3_1-ecc25.jar has been added already. Overwriting of added jars is not supported in the current version.
19/10/17 21:09:13 INFO SessionState: Added [/local_disk0/tmp/addedFile4063054051174557443dbfs__FileStore_saak9001_pqm_core_models_1_0-b0ccd.jar] to class path
19/10/17 21:09:13 INFO SessionState: Added resources: [/local_disk0/tmp/addedFile4063054051174557443dbfs__FileStore_saak9001_pqm_core_models_1_0-b0ccd.jar]
19/10/17 21:09:13 WARN SparkContext: The jar /local_disk0/tmp/addedFile4063054051174557443dbfs__FileStore_saak9001_pqm_core_models_1_0-b0ccd.jar has been added already. Overwriting of added jars is not supported in the current version.
19/10/17 21:09:13 INFO SessionState: Added [/local_disk0/tmp/addedFile921809598074381246com_typesafe_config_1_3_1-e3e42.jar] to class path
19/10/17 21:09:13 INFO SessionState: Added resources: [/local_disk0/tmp/addedFile921809598074381246com_typesafe_config_1_3_1-e3e42.jar]
19/10/17 21:09:13 WARN SparkContext: The jar /local_disk0/tmp/addedFile921809598074381246com_typesafe_config_1_3_1-e3e42.jar has been added already. Overwriting of added jars is not supported in the current version.
19/10/17 21:09:13 INFO SessionState: Added [/local_disk0/tmp/addedFile7430104289198212865io_spray_spray_json_2_11_1_3_5-d5f78.jar] to class path
19/10/17 21:09:13 INFO SessionState: Added resources: [/local_disk0/tmp/addedFile7430104289198212865io_spray_spray_json_2_11_1_3_5-d5f78.jar]
19/10/17 21:09:13 WARN SparkContext: The jar /local_disk0/tmp/addedFile7430104289198212865io_spray_spray_json_2_11_1_3_5-d5f78.jar has been added already. Overwriting of added jars is not supported in the current version.
19/10/17 21:09:13 INFO SessionState: Added [/local_disk0/tmp/addedFile5514790601964570712dom4j_dom4j_1_6_1-3b35e.jar] to class path
19/10/17 21:09:13 INFO SessionState: Added resources: [/local_disk0/tmp/addedFile5514790601964570712dom4j_dom4j_1_6_1-3b35e.jar]
19/10/17 21:09:13 WARN SparkContext: The jar /local_disk0/tmp/addedFile5514790601964570712dom4j_dom4j_1_6_1-3b35e.jar has been added already. Overwriting of added jars is not supported in the current version.
19/10/17 21:09:13 INFO ScalaDriverWrapper: setupRepl:ReplId-7dfa9-f3c0f-e1909-9: finished to load
19/10/17 21:09:14 WARN ScalaDriverLocal: User entity information is incomplete, missing userId,orgId,user.
19/10/17 21:09:14 INFO ProgressReporter$: Added result fetcher for 9077743079231951001_7575602824216384756_859eb492-5549-475d-ba18-5ae73a4179d1
19/10/17 21:09:14 INFO SignalUtils: Registered signal handler for INT
19/10/17 21:09:14 INFO SQLAppStatusListener: Execution ID: 0 Total Executor Run Time: 0
19/10/17 21:09:14 INFO SQLAppStatusListener: Execution ID: 1 Total Executor Run Time: 0
19/10/17 21:09:14 INFO SQLAppStatusListener: Execution ID: 2 Total Executor Run Time: 0
19/10/17 21:09:14 INFO SQLAppStatusListener: Execution ID: 3 Total Executor Run Time: 0
19/10/17 21:09:14 INFO SQLAppStatusListener: Execution ID: 4 Total Executor Run Time: 0
19/10/17 21:09:14 INFO SQLAppStatusListener: Execution ID: 5 Total Executor Run Time: 0
19/10/17 21:09:14 INFO SQLAppStatusListener: Execution ID: 6 Total Executor Run Time: 0
19/10/17 21:09:14 INFO SQLAppStatusListener: Execution ID: 7 Total Executor Run Time: 0
19/10/17 21:09:15 INFO DriverILoop: Set class prefix to: lineb604a04a6cd34eac9d4eaedbf41769f4
19/10/17 21:09:15 INFO DriverILoop: set ContextClassLoader
19/10/17 21:09:15 INFO DriverILoop: initialized intp
19/10/17 21:09:18 INFO ProgressReporter$: Removed result fetcher for 9077743079231951001_7575602824216384756_859eb492-5549-475d-ba18-5ae73a4179d1
19/10/17 21:09:22 INFO DriverCorral: Starting repl ReplId-3ebe7-44b6c-93d0b-4
19/10/17 21:09:22 INFO LogicalPlanStats: Setting LogicalPlanStats visitor to com.databricks.sql.optimizer.statsEstimation.DatabricksLogicalPlanStatsVisitor$
19/10/17 21:09:22 INFO SessionState: Added [/local_disk0/tmp/addedFile9193930907825338753xml_apis_xml_apis_1_0_b2-167e6.jar] to class path
19/10/17 21:09:22 INFO SessionState: Added resources: [/local_disk0/tmp/addedFile9193930907825338753xml_apis_xml_apis_1_0_b2-167e6.jar]
19/10/17 21:09:22 WARN SparkContext: The jar /local_disk0/tmp/addedFile9193930907825338753xml_apis_xml_apis_1_0_b2-167e6.jar has been added already. Overwriting of added jars is not supported in the current version.
19/10/17 21:09:22 INFO SQLAppStatusListener: Execution ID: 8 Total Executor Run Time: 0
19/10/17 21:09:22 INFO SessionState: Added [/local_disk0/tmp/addedFile5190065922075015895org_scalaj_scalaj_http_2_11_2_3_0-d69e4.jar] to class path
19/10/17 21:09:22 INFO SessionState: Added resources: [/local_disk0/tmp/addedFile5190065922075015895org_scalaj_scalaj_http_2_11_2_3_0-d69e4.jar]
19/10/17 21:09:22 WARN SparkContext: The jar /local_disk0/tmp/addedFile5190065922075015895org_scalaj_scalaj_http_2_11_2_3_0-d69e4.jar has been added already. Overwriting of added jars is not supported in the current version.
19/10/17 21:09:22 INFO SQLAppStatusListener: Execution ID: 9 Total Executor Run Time: 0
19/10/17 21:09:22 INFO SessionState: Added [/local_disk0/tmp/addedFile2730237718112202115dbfs__FileStore_saak9001_transengine_1_0_SNAPSHOT-ab9b9.jar] to class path
19/10/17 21:09:22 INFO SessionState: Added resources: [/local_disk0/tmp/addedFile2730237718112202115dbfs__FileStore_saak9001_transengine_1_0_SNAPSHOT-ab9b9.jar]
19/10/17 21:09:22 WARN SparkContext: The jar /local_disk0/tmp/addedFile2730237718112202115dbfs__FileStore_saak9001_transengine_1_0_SNAPSHOT-ab9b9.jar has been added already. Overwriting of added jars is not supported in the current version.
19/10/17 21:09:22 INFO SQLAppStatusListener: Execution ID: 10 Total Executor Run Time: 0
19/10/17 21:09:22 INFO SessionState: Added [/local_disk0/tmp/addedFile3437214070662729572org_rogach_scallop_2_11_3_3_1-ecc25.jar] to class path
19/10/17 21:09:22 INFO SessionState: Added resources: [/local_disk0/tmp/addedFile3437214070662729572org_rogach_scallop_2_11_3_3_1-ecc25.jar]
19/10/17 21:09:22 WARN SparkContext: The jar /local_disk0/tmp/addedFile3437214070662729572org_rogach_scallop_2_11_3_3_1-ecc25.jar has been added already. Overwriting of added jars is not supported in the current version.
19/10/17 21:09:22 INFO SQLAppStatusListener: Execution ID: 11 Total Executor Run Time: 0
19/10/17 21:09:22 INFO SessionState: Added [/local_disk0/tmp/addedFile4063054051174557443dbfs__FileStore_saak9001_pqm_core_models_1_0-b0ccd.jar] to class path
19/10/17 21:09:22 INFO SessionState: Added resources: [/local_disk0/tmp/addedFile4063054051174557443dbfs__FileStore_saak9001_pqm_core_models_1_0-b0ccd.jar]
19/10/17 21:09:22 WARN SparkContext: The jar /local_disk0/tmp/addedFile4063054051174557443dbfs__FileStore_saak9001_pqm_core_models_1_0-b0ccd.jar has been added already. Overwriting of added jars is not supported in the current version.
19/10/17 21:09:22 INFO SQLAppStatusListener: Execution ID: 12 Total Executor Run Time: 0
19/10/17 21:09:22 INFO SessionState: Added [/local_disk0/tmp/addedFile921809598074381246com_typesafe_config_1_3_1-e3e42.jar] to class path
19/10/17 21:09:22 INFO SessionState: Added resources: [/local_disk0/tmp/addedFile921809598074381246com_typesafe_config_1_3_1-e3e42.jar]
19/10/17 21:09:22 WARN SparkContext: The jar /local_disk0/tmp/addedFile921809598074381246com_typesafe_config_1_3_1-e3e42.jar has been added already. Overwriting of added jars is not supported in the current version.
19/10/17 21:09:22 INFO SQLAppStatusListener: Execution ID: 13 Total Executor Run Time: 0
19/10/17 21:09:22 INFO SessionState: Added [/local_disk0/tmp/addedFile7430104289198212865io_spray_spray_json_2_11_1_3_5-d5f78.jar] to class path
19/10/17 21:09:22 INFO SessionState: Added resources: [/local_disk0/tmp/addedFile7430104289198212865io_spray_spray_json_2_11_1_3_5-d5f78.jar]
19/10/17 21:09:22 WARN SparkContext: The jar /local_disk0/tmp/addedFile7430104289198212865io_spray_spray_json_2_11_1_3_5-d5f78.jar has been added already. Overwriting of added jars is not supported in the current version.
19/10/17 21:09:22 INFO SQLAppStatusListener: Execution ID: 14 Total Executor Run Time: 0
19/10/17 21:09:22 INFO SessionState: Added [/local_disk0/tmp/addedFile5514790601964570712dom4j_dom4j_1_6_1-3b35e.jar] to class path
19/10/17 21:09:22 INFO SessionState: Added resources: [/local_disk0/tmp/addedFile5514790601964570712dom4j_dom4j_1_6_1-3b35e.jar]
19/10/17 21:09:22 WARN SparkContext: The jar /local_disk0/tmp/addedFile5514790601964570712dom4j_dom4j_1_6_1-3b35e.jar has been added already. Overwriting of added jars is not supported in the current version.
19/10/17 21:09:22 INFO ScalaDriverWrapper: setupRepl:ReplId-3ebe7-44b6c-93d0b-4: finished to load
19/10/17 21:09:22 INFO SQLAppStatusListener: Execution ID: 15 Total Executor Run Time: 0
19/10/17 21:09:22 WARN ScalaDriverLocal: User entity information is incomplete, missing userId,user.
19/10/17 21:09:22 INFO ProgressReporter$: Added result fetcher for 4521178943219552436_6721938277465988443_job-68230-run-1-action-72418
19/10/17 21:09:23 INFO DriverILoop: Set class prefix to: line1befe054149d4284b87dacf6847fefc2
19/10/17 21:09:23 INFO DriverILoop: set ContextClassLoader
19/10/17 21:09:23 INFO DriverILoop: initialized intp
19/10/17 21:09:26 WARN SparkContext: Using an existing SparkContext; some configuration may not take effect.
19/10/17 21:09:26 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
19/10/17 21:09:26 INFO Instructions: Instruction File has been Read Successfully.>>>>>>>>>>>
19/10/17 21:09:26 INFO HiveMetaStore: 1: get_database: bd_output_1000255
19/10/17 21:09:26 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: bd_output_1000255	
19/10/17 21:09:26 WARN HiveConf: HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist
19/10/17 21:09:26 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
19/10/17 21:09:26 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
19/10/17 21:09:26 INFO HiveMetaStore: 1: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
19/10/17 21:09:26 INFO ObjectStore: ObjectStore, initialize called
19/10/17 21:09:26 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is POSTGRES
19/10/17 21:09:26 INFO ObjectStore: Initialized ObjectStore
19/10/17 21:09:26 WARN ObjectStore: Failed to get database bd_output_1000255, returning NoSuchObjectException
19/10/17 21:09:26 INFO HiveMetaStore: 1: get_database: bd_output_1000255
19/10/17 21:09:26 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: bd_output_1000255	
19/10/17 21:09:26 WARN ObjectStore: Failed to get database bd_output_1000255, returning NoSuchObjectException
19/10/17 21:09:26 INFO SQLAppStatusListener: Execution ID: 16 Total Executor Run Time: 0
19/10/17 21:09:26 INFO HiveMetaStore: 1: get_database: global_temp
19/10/17 21:09:26 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: global_temp	
19/10/17 21:09:26 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException
19/10/17 21:09:26 INFO HiveMetaStore: 1: create_database: Database(name:bd_output_1000255, description:, locationUri:adl://connectedsystemsuatdl.azuredatalakestore.net/user/hive/warehouse/bd_output_1000255.db, parameters:{})
19/10/17 21:09:26 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=create_database: Database(name:bd_output_1000255, description:, locationUri:adl://connectedsystemsuatdl.azuredatalakestore.net/user/hive/warehouse/bd_output_1000255.db, parameters:{})	
19/10/17 21:09:26 WARN ObjectStore: Failed to get database bd_output_1000255, returning NoSuchObjectException
19/10/17 21:09:26 DEBUG AccessTokenProvider: AADToken: no token. Returning expiring=true
19/10/17 21:09:26 DEBUG AccessTokenProvider: AAD Token is missing or expired: Calling refresh-token from abstract base class
19/10/17 21:09:26 DEBUG ClientCredsTokenProvider: AADToken: refreshing client-credential based token
19/10/17 21:09:26 DEBUG AzureADAuthenticator: AADToken: starting to fetch token using client creds for client ID 003106c3-ff8c-4c62-a060-24b8b32ea512
19/10/17 21:09:27 DEBUG AzureADAuthenticator: AADToken: fetched token with expiry Fri Oct 18 05:09:27 UTC 2019
19/10/17 21:09:27 DEBUG HttpTransport: HTTPRequest,Failed,cReqId:4eb75355-0384-4a47-8675-51972d57afc2.0,lat:265,err:HTTP404(FileNotFoundException),Reqlen:0,Resplen:261,token_ns:142464600,sReqId:3f4b9174-febd-476a-8058-4bd08a2d9838,path:/user/hive/warehouse/bd_output_1000255.db,qp:op=GETFILESTATUS&tooid=true&api-version=2017-08-01
19/10/17 21:09:27 INFO FileUtils: Creating directory if it doesn't exist: adl://connectedsystemsuatdl.azuredatalakestore.net/user/hive/warehouse/bd_output_1000255.db
19/10/17 21:09:27 INFO SQLAppStatusListener: Execution ID: 17 Total Executor Run Time: 0
19/10/17 21:09:27 INFO FileToDimension: FileToDimension Some(MarketCSV) for MRKT_DIM input file wasbs://teuatcontainer01@teuatstorage.blob.core.windows.net//bd_output_1000255/MRKT_DIM.csv>>>>> Started
19/10/17 21:09:27 INFO InMemoryFileIndex: Start listing leaf files and directories. Size of Paths: 1; threshold: 32
19/10/17 21:09:27 INFO InMemoryFileIndex: Start listing leaf files and directories. Size of Paths: 0; threshold: 32
19/10/17 21:09:27 INFO InMemoryFileIndex: Start listing leaf files and directories. Size of Paths: 1; threshold: 32
19/10/17 21:09:27 INFO InMemoryFileIndex: Start listing leaf files and directories. Size of Paths: 0; threshold: 32
19/10/17 21:09:28 INFO FileSourceStrategy: Pruning directories with: 
19/10/17 21:09:28 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#34, None)) > 0)
19/10/17 21:09:28 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
19/10/17 21:09:28 INFO FileSourceScanExec: Pushed Filters: 
19/10/17 21:09:29 INFO CodeGenerator: Code generated in 335.0935 ms
19/10/17 21:09:29 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 649.6 KB, free 54.2 GB)
19/10/17 21:09:29 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 46.9 KB, free 54.2 GB)
19/10/17 21:09:29 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.139.64.37:33311 (size: 46.9 KB, free: 54.2 GB)
19/10/17 21:09:29 INFO SparkContext: Created broadcast 0 from csv at FileToDimension.scala:30
19/10/17 21:09:29 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
19/10/17 21:09:29 INFO SparkContext: Starting job: csv at FileToDimension.scala:30
19/10/17 21:09:29 INFO DAGScheduler: Got job 0 (csv at FileToDimension.scala:30) with 1 output partitions
19/10/17 21:09:29 INFO DAGScheduler: Final stage: ResultStage 0 (csv at FileToDimension.scala:30)
19/10/17 21:09:29 INFO DAGScheduler: Parents of final stage: List()
19/10/17 21:09:29 INFO DAGScheduler: Missing parents: List()
19/10/17 21:09:29 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at csv at FileToDimension.scala:30), which has no missing parents
19/10/17 21:09:29 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 14.1 KB, free 54.2 GB)
19/10/17 21:09:29 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.0 KB, free 54.2 GB)
19/10/17 21:09:29 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.139.64.37:33311 (size: 7.0 KB, free: 54.2 GB)
19/10/17 21:09:29 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1316
19/10/17 21:09:29 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at csv at FileToDimension.scala:30) (first 15 tasks are for partitions Vector(0))
19/10/17 21:09:29 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
19/10/17 21:09:29 INFO TaskSetManager: Jars for session None: Map(spark://10.139.64.37:32805/jars/addedFile5514790601964570712dom4j_dom4j_1_6_1-3b35e.jar -> 1571346483602, spark://10.139.64.37:32805/jars/addedFile9193930907825338753xml_apis_xml_apis_1_0_b2-167e6.jar -> 1571346483463, spark://10.139.64.37:32805/jars/addedFile4063054051174557443dbfs__FileStore_saak9001_pqm_core_models_1_0-b0ccd.jar -> 1571346483976, spark://10.139.64.37:32805/jars/addedFile7430104289198212865io_spray_spray_json_2_11_1_3_5-d5f78.jar -> 1571346484084, spark://10.139.64.37:32805/jars/addedFile921809598074381246com_typesafe_config_1_3_1-e3e42.jar -> 1571346483296, spark://10.139.64.37:32805/jars/addedFile5190065922075015895org_scalaj_scalaj_http_2_11_2_3_0-d69e4.jar -> 1571346484274, spark://10.139.64.37:32805/jars/addedFile2730237718112202115dbfs__FileStore_saak9001_transengine_1_0_SNAPSHOT-ab9b9.jar -> 1571346484193, spark://10.139.64.37:32805/jars/addedFile3437214070662729572org_rogach_scallop_2_11_3_3_1-ecc25.jar -> 1571346483129)
19/10/17 21:09:29 INFO TaskSetManager: Files for session None: Map(spark://10.139.64.37:32805/files/addedFile7430104289198212865io_spray_spray_json_2_11_1_3_5-d5f78.jar -> 1571346484077, spark://10.139.64.37:32805/files/addedFile9193930907825338753xml_apis_xml_apis_1_0_b2-167e6.jar -> 1571346483455, spark://10.139.64.37:32805/files/addedFile3437214070662729572org_rogach_scallop_2_11_3_3_1-ecc25.jar -> 1571346483114, spark://10.139.64.37:32805/files/addedFile5514790601964570712dom4j_dom4j_1_6_1-3b35e.jar -> 1571346483594, spark://10.139.64.37:32805/files/addedFile921809598074381246com_typesafe_config_1_3_1-e3e42.jar -> 1571346483288, spark://10.139.64.37:32805/files/addedFile2730237718112202115dbfs__FileStore_saak9001_transengine_1_0_SNAPSHOT-ab9b9.jar -> 1571346484186, spark://10.139.64.37:32805/files/addedFile4063054051174557443dbfs__FileStore_saak9001_pqm_core_models_1_0-b0ccd.jar -> 1571346483968, spark://10.139.64.37:32805/files/addedFile5190065922075015895org_scalaj_scalaj_http_2_11_2_3_0-d69e4.jar -> 1571346484265)
19/10/17 21:09:29 WARN FairSchedulableBuilder: A job was submitted with scheduler pool 4521178943219552436, which has not been configured. This can happen when the file that pools are read from isn't set, or when that file doesn't contain 4521178943219552436. Created 4521178943219552436 with default configuration (schedulingMode: FIFO, minShare: 0, weight: 1)
19/10/17 21:09:29 INFO FairSchedulableBuilder: Added task set TaskSet_0.0 tasks to pool 4521178943219552436
19/10/17 21:09:29 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, 10.139.64.32, executor 0, partition 0, PROCESS_LOCAL, 6058 bytes)
19/10/17 21:09:30 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.139.64.32:33807 (size: 7.0 KB, free: 50.8 GB)
19/10/17 21:09:31 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.139.64.32:33807 (size: 46.9 KB, free: 50.8 GB)
19/10/17 21:09:32 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2487 ms on 10.139.64.32 (executor 0) (1/1)
19/10/17 21:09:32 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 4521178943219552436
19/10/17 21:09:32 INFO DAGScheduler: ResultStage 0 (csv at FileToDimension.scala:30) finished in 2.616 s
19/10/17 21:09:32 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
19/10/17 21:09:32 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
19/10/17 21:09:32 INFO DAGScheduler: Job 0 finished: csv at FileToDimension.scala:30, took 2.692803 s
19/10/17 21:09:32 INFO CodeGenerator: Code generated in 20.7301 ms
19/10/17 21:09:32 INFO FileSourceStrategy: Pruning directories with: 
19/10/17 21:09:32 INFO FileSourceStrategy: Post-Scan Filters: 
19/10/17 21:09:32 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
19/10/17 21:09:32 INFO FileSourceScanExec: Pushed Filters: 
19/10/17 21:09:32 INFO SQLAppStatusListener: Execution ID: 18 Total Executor Run Time: 1718
19/10/17 21:09:32 INFO CodeGenerator: Code generated in 48.5671 ms
19/10/17 21:09:32 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 649.6 KB, free 54.2 GB)
19/10/17 21:09:32 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 46.9 KB, free 54.2 GB)
19/10/17 21:09:32 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.139.64.37:33311 (size: 46.9 KB, free: 54.2 GB)
19/10/17 21:09:32 INFO SparkContext: Created broadcast 2 from csv at FileToDimension.scala:30
19/10/17 21:09:32 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
19/10/17 21:09:32 INFO SparkContext: Starting job: csv at FileToDimension.scala:30
19/10/17 21:09:32 INFO DAGScheduler: Got job 1 (csv at FileToDimension.scala:30) with 1 output partitions
19/10/17 21:09:32 INFO DAGScheduler: Final stage: ResultStage 1 (csv at FileToDimension.scala:30)
19/10/17 21:09:32 INFO DAGScheduler: Parents of final stage: List()
19/10/17 21:09:32 INFO DAGScheduler: Missing parents: List()
19/10/17 21:09:32 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at csv at FileToDimension.scala:30), which has no missing parents
19/10/17 21:09:32 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 18.0 KB, free 54.2 GB)
19/10/17 21:09:32 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 9.0 KB, free 54.2 GB)
19/10/17 21:09:32 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.139.64.37:33311 (size: 9.0 KB, free: 54.2 GB)
19/10/17 21:09:32 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1316
19/10/17 21:09:32 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at csv at FileToDimension.scala:30) (first 15 tasks are for partitions Vector(0))
19/10/17 21:09:32 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks
19/10/17 21:09:32 INFO TaskSetManager: Jars for session None: Map(spark://10.139.64.37:32805/jars/addedFile5514790601964570712dom4j_dom4j_1_6_1-3b35e.jar -> 1571346483602, spark://10.139.64.37:32805/jars/addedFile9193930907825338753xml_apis_xml_apis_1_0_b2-167e6.jar -> 1571346483463, spark://10.139.64.37:32805/jars/addedFile4063054051174557443dbfs__FileStore_saak9001_pqm_core_models_1_0-b0ccd.jar -> 1571346483976, spark://10.139.64.37:32805/jars/addedFile7430104289198212865io_spray_spray_json_2_11_1_3_5-d5f78.jar -> 1571346484084, spark://10.139.64.37:32805/jars/addedFile921809598074381246com_typesafe_config_1_3_1-e3e42.jar -> 1571346483296, spark://10.139.64.37:32805/jars/addedFile5190065922075015895org_scalaj_scalaj_http_2_11_2_3_0-d69e4.jar -> 1571346484274, spark://10.139.64.37:32805/jars/addedFile2730237718112202115dbfs__FileStore_saak9001_transengine_1_0_SNAPSHOT-ab9b9.jar -> 1571346484193, spark://10.139.64.37:32805/jars/addedFile3437214070662729572org_rogach_scallop_2_11_3_3_1-ecc25.jar -> 1571346483129)
19/10/17 21:09:32 INFO TaskSetManager: Files for session None: Map(spark://10.139.64.37:32805/files/addedFile7430104289198212865io_spray_spray_json_2_11_1_3_5-d5f78.jar -> 1571346484077, spark://10.139.64.37:32805/files/addedFile9193930907825338753xml_apis_xml_apis_1_0_b2-167e6.jar -> 1571346483455, spark://10.139.64.37:32805/files/addedFile3437214070662729572org_rogach_scallop_2_11_3_3_1-ecc25.jar -> 1571346483114, spark://10.139.64.37:32805/files/addedFile5514790601964570712dom4j_dom4j_1_6_1-3b35e.jar -> 1571346483594, spark://10.139.64.37:32805/files/addedFile921809598074381246com_typesafe_config_1_3_1-e3e42.jar -> 1571346483288, spark://10.139.64.37:32805/files/addedFile2730237718112202115dbfs__FileStore_saak9001_transengine_1_0_SNAPSHOT-ab9b9.jar -> 1571346484186, spark://10.139.64.37:32805/files/addedFile4063054051174557443dbfs__FileStore_saak9001_pqm_core_models_1_0-b0ccd.jar -> 1571346483968, spark://10.139.64.37:32805/files/addedFile5190065922075015895org_scalaj_scalaj_http_2_11_2_3_0-d69e4.jar -> 1571346484265)
19/10/17 21:09:32 INFO FairSchedulableBuilder: Added task set TaskSet_1.0 tasks to pool 4521178943219552436
19/10/17 21:09:32 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, 10.139.64.32, executor 0, partition 0, PROCESS_LOCAL, 6058 bytes)
19/10/17 21:09:32 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.139.64.32:33807 (size: 9.0 KB, free: 50.8 GB)
19/10/17 21:09:34 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.139.64.32:33807 (size: 46.9 KB, free: 50.8 GB)
19/10/17 21:09:34 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 2187 ms on 10.139.64.32 (executor 0) (1/1)
19/10/17 21:09:34 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 4521178943219552436
19/10/17 21:09:34 INFO DAGScheduler: ResultStage 1 (csv at FileToDimension.scala:30) finished in 2.198 s
19/10/17 21:09:34 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
19/10/17 21:09:34 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
19/10/17 21:09:34 INFO DAGScheduler: Job 1 finished: csv at FileToDimension.scala:30, took 2.208204 s
19/10/17 21:09:34 INFO FileToDimension: >>>>>>>>>> Writing table bd_output_1000255.MRKT_DIM Started.
19/10/17 21:09:34 INFO HiveMetaStore: 1: get_table : db=bd_output_1000255 tbl=mrkt_dim
19/10/17 21:09:34 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=bd_output_1000255 tbl=mrkt_dim	
19/10/17 21:09:34 WARN Utils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.debug.maxToStringFields' in SparkEnv.conf.
19/10/17 21:09:34 INFO FileSourceStrategy: Pruning directories with: 
19/10/17 21:09:34 INFO FileSourceStrategy: Post-Scan Filters: 
19/10/17 21:09:34 INFO FileSourceStrategy: Output Data Schema: struct<CLIENT_ID: int, MRKT_DSC_SHRT: string, MRKT_DSC_LNG: string, SAMS_RSTR_IND: int, CSL_BLK_DT: timestamp ... 62 more fields>
19/10/17 21:09:34 INFO FileSourceScanExec: Pushed Filters: 
19/10/17 21:09:34 INFO HiveMetaStore: 1: get_table : db=bd_output_1000255 tbl=mrkt_dim
19/10/17 21:09:34 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=bd_output_1000255 tbl=mrkt_dim	
19/10/17 21:09:34 INFO HiveMetaStore: 1: get_database: bd_output_1000255
19/10/17 21:09:34 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: bd_output_1000255	
19/10/17 21:09:34 INFO HiveMetaStore: 1: get_database: bd_output_1000255
19/10/17 21:09:34 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: bd_output_1000255	
19/10/17 21:09:34 DEBUG HttpTransport: HTTPRequest,Failed,cReqId:210f41b8-c7fe-40a3-bcad-e351e9a6772c.0,lat:31,err:HTTP404(FileNotFoundException),Reqlen:0,Resplen:270,token_ns:4600,sReqId:0ddb0ea9-8231-4a6c-a30d-037495751b08,path:/user/hive/warehouse/bd_output_1000255.db/mrkt_dim,qp:op=GETFILESTATUS&tooid=true&api-version=2017-08-01
19/10/17 21:09:34 INFO HiveMetaStore: 1: get_database: bd_output_1000255
19/10/17 21:09:34 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: bd_output_1000255	
19/10/17 21:09:34 INFO HiveMetaStore: 1: get_database: bd_output_1000255
19/10/17 21:09:34 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: bd_output_1000255	
19/10/17 21:09:34 DEBUG HttpTransport: HTTPRequest,Failed,cReqId:cec9d038-3a79-4783-8733-7a8d5ad2ca5f.0,lat:7,err:HTTP404(FileNotFoundException),Reqlen:0,Resplen:270,token_ns:6400,sReqId:72bf4398-fb42-4b2c-ae13-8a5ad087a4b9,path:/user/hive/warehouse/bd_output_1000255.db/mrkt_dim,qp:op=GETFILESTATUS&tooid=true&api-version=2017-08-01
19/10/17 21:09:34 INFO ParquetFileFormat: Using user defined output committer for Parquet: org.apache.spark.sql.parquet.DirectParquetOutputCommitter
19/10/17 21:09:34 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 649.4 KB, free 54.2 GB)
19/10/17 21:09:34 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 46.9 KB, free 54.2 GB)
19/10/17 21:09:34 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.139.64.37:33311 (size: 46.9 KB, free: 54.2 GB)
19/10/17 21:09:34 INFO SparkContext: Created broadcast 4 from saveAsTable at DataFrameDecorator.scala:17
19/10/17 21:09:34 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
19/10/17 21:09:35 INFO SparkContext: Starting job: saveAsTable at DataFrameDecorator.scala:17
19/10/17 21:09:35 INFO DAGScheduler: Got job 2 (saveAsTable at DataFrameDecorator.scala:17) with 1 output partitions
19/10/17 21:09:35 INFO DAGScheduler: Final stage: ResultStage 2 (saveAsTable at DataFrameDecorator.scala:17)
19/10/17 21:09:35 INFO DAGScheduler: Parents of final stage: List()
19/10/17 21:09:35 INFO DAGScheduler: Missing parents: List()
19/10/17 21:09:35 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[8] at saveAsTable at DataFrameDecorator.scala:17), which has no missing parents
19/10/17 21:09:35 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 325.1 KB, free 54.2 GB)
19/10/17 21:09:35 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 106.5 KB, free 54.2 GB)
19/10/17 21:09:35 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 10.139.64.37:33311 (size: 106.5 KB, free: 54.2 GB)
19/10/17 21:09:35 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1316
19/10/17 21:09:35 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[8] at saveAsTable at DataFrameDecorator.scala:17) (first 15 tasks are for partitions Vector(0))
19/10/17 21:09:35 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks
19/10/17 21:09:35 INFO TaskSetManager: Jars for session None: Map(spark://10.139.64.37:32805/jars/addedFile5514790601964570712dom4j_dom4j_1_6_1-3b35e.jar -> 1571346483602, spark://10.139.64.37:32805/jars/addedFile9193930907825338753xml_apis_xml_apis_1_0_b2-167e6.jar -> 1571346483463, spark://10.139.64.37:32805/jars/addedFile4063054051174557443dbfs__FileStore_saak9001_pqm_core_models_1_0-b0ccd.jar -> 1571346483976, spark://10.139.64.37:32805/jars/addedFile7430104289198212865io_spray_spray_json_2_11_1_3_5-d5f78.jar -> 1571346484084, spark://10.139.64.37:32805/jars/addedFile921809598074381246com_typesafe_config_1_3_1-e3e42.jar -> 1571346483296, spark://10.139.64.37:32805/jars/addedFile5190065922075015895org_scalaj_scalaj_http_2_11_2_3_0-d69e4.jar -> 1571346484274, spark://10.139.64.37:32805/jars/addedFile2730237718112202115dbfs__FileStore_saak9001_transengine_1_0_SNAPSHOT-ab9b9.jar -> 1571346484193, spark://10.139.64.37:32805/jars/addedFile3437214070662729572org_rogach_scallop_2_11_3_3_1-ecc25.jar -> 1571346483129)
19/10/17 21:09:35 INFO TaskSetManager: Files for session None: Map(spark://10.139.64.37:32805/files/addedFile7430104289198212865io_spray_spray_json_2_11_1_3_5-d5f78.jar -> 1571346484077, spark://10.139.64.37:32805/files/addedFile9193930907825338753xml_apis_xml_apis_1_0_b2-167e6.jar -> 1571346483455, spark://10.139.64.37:32805/files/addedFile3437214070662729572org_rogach_scallop_2_11_3_3_1-ecc25.jar -> 1571346483114, spark://10.139.64.37:32805/files/addedFile5514790601964570712dom4j_dom4j_1_6_1-3b35e.jar -> 1571346483594, spark://10.139.64.37:32805/files/addedFile921809598074381246com_typesafe_config_1_3_1-e3e42.jar -> 1571346483288, spark://10.139.64.37:32805/files/addedFile2730237718112202115dbfs__FileStore_saak9001_transengine_1_0_SNAPSHOT-ab9b9.jar -> 1571346484186, spark://10.139.64.37:32805/files/addedFile4063054051174557443dbfs__FileStore_saak9001_pqm_core_models_1_0-b0ccd.jar -> 1571346483968, spark://10.139.64.37:32805/files/addedFile5190065922075015895org_scalaj_scalaj_http_2_11_2_3_0-d69e4.jar -> 1571346484265)
19/10/17 21:09:35 INFO FairSchedulableBuilder: Added task set TaskSet_2.0 tasks to pool 4521178943219552436
19/10/17 21:09:35 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, 10.139.64.32, executor 0, partition 0, PROCESS_LOCAL, 6058 bytes)
19/10/17 21:09:35 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 10.139.64.32:33807 (size: 106.5 KB, free: 50.8 GB)
19/10/17 21:09:36 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.139.64.32:33807 (size: 46.9 KB, free: 50.8 GB)
19/10/17 21:09:38 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 3024 ms on 10.139.64.32 (executor 0) (1/1)
19/10/17 21:09:38 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 4521178943219552436
19/10/17 21:09:38 INFO DAGScheduler: ResultStage 2 (saveAsTable at DataFrameDecorator.scala:17) finished in 3.063 s
19/10/17 21:09:38 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
19/10/17 21:09:38 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
19/10/17 21:09:38 INFO DAGScheduler: Job 2 finished: saveAsTable at DataFrameDecorator.scala:17, took 3.076567 s
19/10/17 21:09:38 INFO DirectoryAtomicCommitProtocol: Committing job 2c178a26-0da7-4a48-b964-3cf71d9339d8
19/10/17 21:09:38 INFO FileSystem: FS_OP_CREATE FILE[adl://connectedsystemsuatdl.azuredatalakestore.net/user/hive/warehouse/bd_output_1000255.db/mrkt_dim/_committed_5208035692629839807] Creating output stream; permission: rw-r--r--, overwrite: true, bufferSize: 65536
19/10/17 21:09:38 INFO AdlFsOutputStream: FS_OP_CREATE FILE[/user/hive/warehouse/bd_output_1000255.db/mrkt_dim/_committed_5208035692629839807] Closing stream; size: 120
19/10/17 21:09:38 INFO AdlFsOutputStream: FS_OP_CREATE FILE[/user/hive/warehouse/bd_output_1000255.db/mrkt_dim/_committed_5208035692629839807] Upload complete; size: 120
19/10/17 21:09:38 INFO AdlFsOutputStream: FS_OP_CREATE FILE[/user/hive/warehouse/bd_output_1000255.db/mrkt_dim/_committed_5208035692629839807] Closing stream; size: 120
19/10/17 21:09:38 INFO AdlFsOutputStream: FS_OP_CREATE FILE[/user/hive/warehouse/bd_output_1000255.db/mrkt_dim/_committed_5208035692629839807] Upload complete; size: 120
19/10/17 21:09:38 INFO FileSystem: FS_OP_CREATE FILE[adl://connectedsystemsuatdl.azuredatalakestore.net/user/hive/warehouse/bd_output_1000255.db/mrkt_dim/_SUCCESS] Creating output stream; permission: rw-r--r--, overwrite: true, bufferSize: 65536
19/10/17 21:09:38 INFO AdlFsOutputStream: FS_OP_CREATE FILE[/user/hive/warehouse/bd_output_1000255.db/mrkt_dim/_SUCCESS] Closing stream; size: 0
19/10/17 21:09:38 INFO AdlFsOutputStream: FS_OP_CREATE FILE[/user/hive/warehouse/bd_output_1000255.db/mrkt_dim/_SUCCESS] Upload complete; size: 0
19/10/17 21:09:38 INFO FileSystem: FS_OP_CREATE FILE[adl://connectedsystemsuatdl.azuredatalakestore.net/user/hive/warehouse/bd_output_1000255.db/mrkt_dim/_SUCCESS] Creating output stream; permission: rw-r--r--, overwrite: true, bufferSize: 65536
19/10/17 21:09:38 INFO AdlFsOutputStream: FS_OP_CREATE FILE[/user/hive/warehouse/bd_output_1000255.db/mrkt_dim/_SUCCESS] Closing stream; size: 0
19/10/17 21:09:38 INFO AdlFsOutputStream: FS_OP_CREATE FILE[/user/hive/warehouse/bd_output_1000255.db/mrkt_dim/_SUCCESS] Upload complete; size: 0
19/10/17 21:09:38 INFO DirectoryAtomicCommitProtocol: Job commit completed for 2c178a26-0da7-4a48-b964-3cf71d9339d8
19/10/17 21:09:38 INFO DirectoryAtomicCommitProtocol: Auto-vacuuming directories updated by 2c178a26-0da7-4a48-b964-3cf71d9339d8
19/10/17 21:09:38 INFO DirectoryAtomicCommitProtocol: Started VACUUM on List(adl://connectedsystemsuatdl.azuredatalakestore.net/user/hive/warehouse/bd_output_1000255.db/mrkt_dim) with data horizon 1571173778438 (now - 48.0 hours), metadata horizon 1571344778438 (now - 0.5 hours)
19/10/17 21:09:38 WARN FileOperations: method deleteMultiple doesn't exist. Falling back to individual deletion.
19/10/17 21:09:38 INFO FileFormatWriter: Write Job c22f143e-8f12-4919-9026-1545496b183e committed.
19/10/17 21:09:38 INFO FileFormatWriter: Finished processing stats for write job c22f143e-8f12-4919-9026-1545496b183e.
19/10/17 21:09:38 DEBUG HttpTransport: HTTPRequest,Failed,cReqId:7bbce231-1474-4ca9-bd91-f280efcf52e8.0,lat:7,err:HTTP404(FileNotFoundException),Reqlen:0,Resplen:286,token_ns:3600,sReqId:0cf95a19-2647-470f-b8e0-63cbbd0280e3,path:/user/hive/warehouse/bd_output_1000255.db/mrkt_dim/_spark_metadata,qp:op=GETFILESTATUS&tooid=true&api-version=2017-08-01
19/10/17 21:09:38 INFO InMemoryFileIndex: Start listing leaf files and directories. Size of Paths: 1; threshold: 32
19/10/17 21:09:38 INFO InMemoryFileIndex: Start listing leaf files and directories. Size of Paths: 0; threshold: 32
19/10/17 21:09:38 INFO HiveMetaStore: 1: get_database: bd_output_1000255
19/10/17 21:09:38 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: bd_output_1000255	
19/10/17 21:09:38 INFO HiveMetaStore: 1: get_table : db=bd_output_1000255 tbl=mrkt_dim
19/10/17 21:09:38 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=bd_output_1000255 tbl=mrkt_dim	
19/10/17 21:09:38 INFO HiveMetaStore: 1: get_database: bd_output_1000255
19/10/17 21:09:38 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: bd_output_1000255	
19/10/17 21:09:38 INFO HiveMetaStore: 1: get_table : db=bd_output_1000255 tbl=mrkt_dim
19/10/17 21:09:38 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=bd_output_1000255 tbl=mrkt_dim	
19/10/17 21:09:38 INFO SecuredHiveExternalCatalog: Persisting file based data source table `bd_output_1000255`.`mrkt_dim` into Hive metastore in Hive compatible format.
19/10/17 21:09:38 INFO HiveMetaStore: 1: create_table: Table(tableName:mrkt_dim, dbName:bd_output_1000255, owner:root, createTime:1571346574, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:CLIENT_ID, type:int, comment:null), FieldSchema(name:MRKT_DSC_SHRT, type:string, comment:null), FieldSchema(name:MRKT_DSC_LNG, type:string, comment:null), FieldSchema(name:SAMS_RSTR_IND, type:int, comment:null), FieldSchema(name:CSL_BLK_DT, type:timestamp, comment:null), FieldSchema(name:POD_RLS_IND, type:string, comment:null), FieldSchema(name:PRJCTD_IND, type:int, comment:null), FieldSchema(name:MRKT_TYP, type:string, comment:null), FieldSchema(name:MRKT_RPTBL_STRT_DT, type:timestamp, comment:null), FieldSchema(name:MRKT_RPTBL_STOP_DT, type:timestamp, comment:null), FieldSchema(name:TAGRS, type:string, comment:null), FieldSchema(name:MRKT_CD, type:string, comment:null), FieldSchema(name:ACV_ADJ_IND, type:int, comment:null), FieldSchema(name:MKT_DISP_NAME, type:string, comment:null), FieldSchema(name:MRKT_DRILL_IND, type:string, comment:null), FieldSchema(name:STR_PL_RSTR, type:int, comment:null), FieldSchema(name:WGHT_RULE_NUM, type:int, comment:null), FieldSchema(name:XDIM_MRKT2STR, type:string, comment:null), FieldSchema(name:XAOC_COMP_MKT_KEY, type:int, comment:null), FieldSchema(name:COMP_MKT_KEY, type:int, comment:null), FieldSchema(name:MRKT_KEY, type:int, comment:null), FieldSchema(name:COMP_MKT, type:string, comment:null), FieldSchema(name:XAOC_COMP_MKT, type:string, comment:null), FieldSchema(name:MARKET_TYPE, type:string, comment:null), FieldSchema(name:MRKT_RSTR_ID, type:int, comment:null), FieldSchema(name:MRKT_RSTR_POS, type:int, comment:null), FieldSchema(name:MRKT_IND, type:string, comment:null), FieldSchema(name:CHANNEL, type:string, comment:null), FieldSchema(name:SHR_MRKT_KEY, type:string, comment:null), FieldSchema(name:ETH_MKT_TYP, type:string, comment:null), FieldSchema(name:MRKT_AGG_FLG, type:int, comment:null), FieldSchema(name:DT_SRC_DTL_NM, type:string, comment:null), FieldSchema(name:SUPUP, type:string, comment:null), FieldSchema(name:COUNTRY, type:string, comment:null), FieldSchema(name:CS_MARKET_TYPE, type:string, comment:null), FieldSchema(name:CS_MARKET_SET, type:string, comment:null), FieldSchema(name:SUPPRESSION, type:string, comment:null), FieldSchema(name:DEF_MRKT_FLG, type:string, comment:null), FieldSchema(name:GEOGRAPHY, type:string, comment:null), FieldSchema(name:CS_CHANNEL, type:string, comment:null), FieldSchema(name:CS_MARKET_SET_DETAIL, type:string, comment:null), FieldSchema(name:CS_MARKET_TYPE_SET, type:string, comment:null), FieldSchema(name:CS_TA_CHANNEL, type:string, comment:null), FieldSchema(name:MRKT_TT_DMG_CD, type:int, comment:null), FieldSchema(name:MRKT_TYPE_HS, type:string, comment:null), FieldSchema(name:TA_REM, type:string, comment:null), FieldSchema(name:XAOC_TA_REM, type:string, comment:null), FieldSchema(name:KO_MARKET_PARENT_CHANNEL, type:string, comment:null), FieldSchema(name:KO_MARKET_PARENT_1, type:string, comment:null), FieldSchema(name:KO_MARKET_PARENT_2, type:string, comment:null), FieldSchema(name:KO_MARKET_PARENT_3, type:string, comment:null), FieldSchema(name:KO_MARKET_PARENT_4, type:string, comment:null), FieldSchema(name:KO_MARKET_LEVEL_CHANNEL, type:string, comment:null), FieldSchema(name:KO_MARKET_LEVEL_1, type:string, comment:null), FieldSchema(name:KO_MARKET_LEVEL_2, type:string, comment:null), FieldSchema(name:KO_MARKET_LEVEL_3, type:string, comment:null), FieldSchema(name:KO_MARKET_LEVEL_4, type:string, comment:null), FieldSchema(name:KO_MARKET_ALTERNATE_MKTS, type:string, comment:null), FieldSchema(name:KO_MARKET_ALL_CHILD_MKTS, type:string, comment:null), FieldSchema(name:CM_TA_RM_ALL, type:string, comment:null), FieldSchema(name:XAOC_CM_TA_RM_ALL, type:string, comment:null), FieldSchema(name:ODMP_DISP_NAME, type:string, comment:null), FieldSchema(name:MARKET_DISPLAY_NAME_KO, type:string, comment:null), FieldSchema(name:DYN_MRKT_FLG, type:string, comment:null)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, parameters:{path=adl://connectedsystemsuatdl.azuredatalakestore.net/user/hive/warehouse/bd_output_1000255.db/mrkt_dim, serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{spark.sql.sources.schema.part.1=ta":{}},{"name":"KO_MARKET_LEVEL_4","type":"string","nullable":true,"metadata":{}},{"name":"KO_MARKET_ALTERNATE_MKTS","type":"string","nullable":true,"metadata":{}},{"name":"KO_MARKET_ALL_CHILD_MKTS","type":"string","nullable":true,"metadata":{}},{"name":"CM_TA_RM_ALL","type":"string","nullable":true,"metadata":{}},{"name":"XAOC_CM_TA_RM_ALL","type":"string","nullable":true,"metadata":{}},{"name":"ODMP_DISP_NAME","type":"string","nullable":true,"metadata":{}},{"name":"MARKET_DISPLAY_NAME_KO","type":"string","nullable":true,"metadata":{}},{"name":"DYN_MRKT_FLG","type":"string","nullable":true,"metadata":{}}]}, spark.sql.sources.schema.part.0={"type":"struct","fields":[{"name":"CLIENT_ID","type":"integer","nullable":true,"metadata":{}},{"name":"MRKT_DSC_SHRT","type":"string","nullable":true,"metadata":{}},{"name":"MRKT_DSC_LNG","type":"string","nullable":true,"metadata":{}},{"name":"SAMS_RSTR_IND","type":"integer","nullable":true,"metadata":{}},{"name":"CSL_BLK_DT","type":"timestamp","nullable":true,"metadata":{}},{"name":"POD_RLS_IND","type":"string","nullable":true,"metadata":{}},{"name":"PRJCTD_IND","type":"integer","nullable":true,"metadata":{}},{"name":"MRKT_TYP","type":"string","nullable":true,"metadata":{}},{"name":"MRKT_RPTBL_STRT_DT","type":"timestamp","nullable":true,"metadata":{}},{"name":"MRKT_RPTBL_STOP_DT","type":"timestamp","nullable":true,"metadata":{}},{"name":"TAGRS","type":"string","nullable":true,"metadata":{}},{"name":"MRKT_CD","type":"string","nullable":true,"metadata":{}},{"name":"ACV_ADJ_IND","type":"integer","nullable":true,"metadata":{}},{"name":"MKT_DISP_NAME","type":"string","nullable":true,"metadata":{}},{"name":"MRKT_DRILL_IND","type":"string","nullable":true,"metadata":{}},{"name":"STR_PL_RSTR","type":"integer","nullable":true,"metadata":{}},{"name":"WGHT_RULE_NUM","type":"integer","nullable":true,"metadata":{}},{"name":"XDIM_MRKT2STR","type":"string","nullable":true,"metadata":{}},{"name":"XAOC_COMP_MKT_KEY","type":"integer","nullable":true,"metadata":{}},{"name":"COMP_MKT_KEY","type":"integer","nullable":true,"metadata":{}},{"name":"MRKT_KEY","type":"integer","nullable":true,"metadata":{}},{"name":"COMP_MKT","type":"string","nullable":true,"metadata":{}},{"name":"XAOC_COMP_MKT","type":"string","nullable":true,"metadata":{}},{"name":"MARKET_TYPE","type":"string","nullable":true,"metadata":{}},{"name":"MRKT_RSTR_ID","type":"integer","nullable":true,"metadata":{}},{"name":"MRKT_RSTR_POS","type":"integer","nullable":true,"metadata":{}},{"name":"MRKT_IND","type":"string","nullable":true,"metadata":{}},{"name":"CHANNEL","type":"string","nullable":true,"metadata":{}},{"name":"SHR_MRKT_KEY","type":"string","nullable":true,"metadata":{}},{"name":"ETH_MKT_TYP","type":"string","nullable":true,"metadata":{}},{"name":"MRKT_AGG_FLG","type":"integer","nullable":true,"metadata":{}},{"name":"DT_SRC_DTL_NM","type":"string","nullable":true,"metadata":{}},{"name":"SUPUP","type":"string","nullable":true,"metadata":{}},{"name":"COUNTRY","type":"string","nullable":true,"metadata":{}},{"name":"CS_MARKET_TYPE","type":"string","nullable":true,"metadata":{}},{"name":"CS_MARKET_SET","type":"string","nullable":true,"metadata":{}},{"name":"SUPPRESSION","type":"string","nullable":true,"metadata":{}},{"name":"DEF_MRKT_FLG","type":"string","nullable":true,"metadata":{}},{"name":"GEOGRAPHY","type":"string","nullable":true,"metadata":{}},{"name":"CS_CHANNEL","type":"string","nullable":true,"metadata":{}},{"name":"CS_MARKET_SET_DETAIL","type":"string","nullable":true,"metadata":{}},{"name":"CS_MARKET_TYPE_SET","type":"string","nullable":true,"metadata":{}},{"name":"CS_TA_CHANNEL","type":"string","nullable":true,"metadata":{}},{"name":"MRKT_TT_DMG_CD","type":"integer","nullable":true,"metadata":{}},{"name":"MRKT_TYPE_HS","type":"string","nullable":true,"metadata":{}},{"name":"TA_REM","type":"string","nullable":true,"metadata":{}},{"name":"XAOC_TA_REM","type":"string","nullable":true,"metadata":{}},{"name":"KO_MARKET_PARENT_CHANNEL","type":"string","nullable":true,"metadata":{}},{"name":"KO_MARKET_PARENT_1","type":"string","nullable":true,"metadata":{}},{"name":"KO_MARKET_PARENT_2","type":"string","nullable":true,"metadata":{}},{"name":"KO_MARKET_PARENT_3","type":"string","nullable":true,"metadata":{}},{"name":"KO_MARKET_PARENT_4","type":"string","nullable":true,"metadata":{}},{"name":"KO_MARKET_LEVEL_CHANNEL","type":"string","nullable":true,"metadata":{}},{"name":"KO_MARKET_LEVEL_1","type":"string","nullable":true,"metadata":{}},{"name":"KO_MARKET_LEVEL_2","type":"string","nullable":true,"metadata":{}},{"name":"KO_MARKET_LEVEL_3","type":"string","nullable":true,"metada, spark.sql.sources.schema.numParts=2, spark.sql.sources.provider=parquet, spark.sql.create.version=2.4.0}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{root=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:root, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:root, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:root, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:root, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null))
19/10/17 21:09:38 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=create_table: Table(tableName:mrkt_dim, dbName:bd_output_1000255, owner:root, createTime:1571346574, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:CLIENT_ID, type:int, comment:null), FieldSchema(name:MRKT_DSC_SHRT, type:string, comment:null), FieldSchema(name:MRKT_DSC_LNG, type:string, comment:null), FieldSchema(name:SAMS_RSTR_IND, type:int, comment:null), FieldSchema(name:CSL_BLK_DT, type:timestamp, comment:null), FieldSchema(name:POD_RLS_IND, type:string, comment:null), FieldSchema(name:PRJCTD_IND, type:int, comment:null), FieldSchema(name:MRKT_TYP, type:string, comment:null), FieldSchema(name:MRKT_RPTBL_STRT_DT, type:timestamp, comment:null), FieldSchema(name:MRKT_RPTBL_STOP_DT, type:timestamp, comment:null), FieldSchema(name:TAGRS, type:string, comment:null), FieldSchema(name:MRKT_CD, type:string, comment:null), FieldSchema(name:ACV_ADJ_IND, type:int, comment:null), FieldSchema(name:MKT_DISP_NAME, type:string, comment:null), FieldSchema(name:MRKT_DRILL_IND, type:string, comment:null), FieldSchema(name:STR_PL_RSTR, type:int, comment:null), FieldSchema(name:WGHT_RULE_NUM, type:int, comment:null), FieldSchema(name:XDIM_MRKT2STR, type:string, comment:null), FieldSchema(name:XAOC_COMP_MKT_KEY, type:int, comment:null), FieldSchema(name:COMP_MKT_KEY, type:int, comment:null), FieldSchema(name:MRKT_KEY, type:int, comment:null), FieldSchema(name:COMP_MKT, type:string, comment:null), FieldSchema(name:XAOC_COMP_MKT, type:string, comment:null), FieldSchema(name:MARKET_TYPE, type:string, comment:null), FieldSchema(name:MRKT_RSTR_ID, type:int, comment:null), FieldSchema(name:MRKT_RSTR_POS, type:int, comment:null), FieldSchema(name:MRKT_IND, type:string, comment:null), FieldSchema(name:CHANNEL, type:string, comment:null), FieldSchema(name:SHR_MRKT_KEY, type:string, comment:null), FieldSchema(name:ETH_MKT_TYP, type:string, comment:null), FieldSchema(name:MRKT_AGG_FLG, type:int, comment:null), FieldSchema(name:DT_SRC_DTL_NM, type:string, comment:null), FieldSchema(name:SUPUP, type:string, comment:null), FieldSchema(name:COUNTRY, type:string, comment:null), FieldSchema(name:CS_MARKET_TYPE, type:string, comment:null), FieldSchema(name:CS_MARKET_SET, type:string, comment:null), FieldSchema(name:SUPPRESSION, type:string, comment:null), FieldSchema(name:DEF_MRKT_FLG, type:string, comment:null), FieldSchema(name:GEOGRAPHY, type:string, comment:null), FieldSchema(name:CS_CHANNEL, type:string, comment:null), FieldSchema(name:CS_MARKET_SET_DETAIL, type:string, comment:null), FieldSchema(name:CS_MARKET_TYPE_SET, type:string, comment:null), FieldSchema(name:CS_TA_CHANNEL, type:string, comment:null), FieldSchema(name:MRKT_TT_DMG_CD, type:int, comment:null), FieldSchema(name:MRKT_TYPE_HS, type:string, comment:null), FieldSchema(name:TA_REM, type:string, comment:null), FieldSchema(name:XAOC_TA_REM, type:string, comment:null), FieldSchema(name:KO_MARKET_PARENT_CHANNEL, type:string, comment:null), FieldSchema(name:KO_MARKET_PARENT_1, type:string, comment:null), FieldSchema(name:KO_MARKET_PARENT_2, type:string, comment:null), FieldSchema(name:KO_MARKET_PARENT_3, type:string, comment:null), FieldSchema(name:KO_MARKET_PARENT_4, type:string, comment:null), FieldSchema(name:KO_MARKET_LEVEL_CHANNEL, type:string, comment:null), FieldSchema(name:KO_MARKET_LEVEL_1, type:string, comment:null), FieldSchema(name:KO_MARKET_LEVEL_2, type:string, comment:null), FieldSchema(name:KO_MARKET_LEVEL_3, type:string, comment:null), FieldSchema(name:KO_MARKET_LEVEL_4, type:string, comment:null), FieldSchema(name:KO_MARKET_ALTERNATE_MKTS, type:string, comment:null), FieldSchema(name:KO_MARKET_ALL_CHILD_MKTS, type:string, comment:null), FieldSchema(name:CM_TA_RM_ALL, type:string, comment:null), FieldSchema(name:XAOC_CM_TA_RM_ALL, type:string, comment:null), FieldSchema(name:ODMP_DISP_NAME, type:string, comment:null), FieldSchema(name:MARKET_DISPLAY_NAME_KO, type:string, comment:null), FieldSchema(name:DYN_MRKT_FLG, type:string, comment:null)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, parameters:{path=adl://connectedsystemsuatdl.azuredatalakestore.net/user/hive/warehouse/bd_output_1000255.db/mrkt_dim, serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{spark.sql.sources.schema.part.1=ta":{}},{"name":"KO_MARKET_LEVEL_4","type":"string","nullable":true,"metadata":{}},{"name":"KO_MARKET_ALTERNATE_MKTS","type":"string","nullable":true,"metadata":{}},{"name":"KO_MARKET_ALL_CHILD_MKTS","type":"string","nullable":true,"metadata":{}},{"name":"CM_TA_RM_ALL","type":"string","nullable":true,"metadata":{}},{"name":"XAOC_CM_TA_RM_ALL","type":"string","nullable":true,"metadata":{}},{"name":"ODMP_DISP_NAME","type":"string","nullable":true,"metadata":{}},{"name":"MARKET_DISPLAY_NAME_KO","type":"string","nullable":true,"metadata":{}},{"name":"DYN_MRKT_FLG","type":"string","nullable":true,"metadata":{}}]}, spark.sql.sources.schema.part.0={"type":"struct","fields":[{"name":"CLIENT_ID","type":"integer","nullable":true,"metadata":{}},{"name":"MRKT_DSC_SHRT","type":"string","nullable":true,"metadata":{}},{"name":"MRKT_DSC_LNG","type":"string","nullable":true,"metadata":{}},{"name":"SAMS_RSTR_IND","type":"integer","nullable":true,"metadata":{}},{"name":"CSL_BLK_DT","type":"timestamp","nullable":true,"metadata":{}},{"name":"POD_RLS_IND","type":"string","nullable":true,"metadata":{}},{"name":"PRJCTD_IND","type":"integer","nullable":true,"metadata":{}},{"name":"MRKT_TYP","type":"string","nullable":true,"metadata":{}},{"name":"MRKT_RPTBL_STRT_DT","type":"timestamp","nullable":true,"metadata":{}},{"name":"MRKT_RPTBL_STOP_DT","type":"timestamp","nullable":true,"metadata":{}},{"name":"TAGRS","type":"string","nullable":true,"metadata":{}},{"name":"MRKT_CD","type":"string","nullable":true,"metadata":{}},{"name":"ACV_ADJ_IND","type":"integer","nullable":true,"metadata":{}},{"name":"MKT_DISP_NAME","type":"string","nullable":true,"metadata":{}},{"name":"MRKT_DRILL_IND","type":"string","nullable":true,"metadata":{}},{"name":"STR_PL_RSTR","type":"integer","nullable":true,"metadata":{}},{"name":"WGHT_RULE_NUM","type":"integer","nullable":true,"metadata":{}},{"name":"XDIM_MRKT2STR","type":"string","nullable":true,"metadata":{}},{"name":"XAOC_COMP_MKT_KEY","type":"integer","nullable":true,"metadata":{}},{"name":"COMP_MKT_KEY","type":"integer","nullable":true,"metadata":{}},{"name":"MRKT_KEY","type":"integer","nullable":true,"metadata":{}},{"name":"COMP_MKT","type":"string","nullable":true,"metadata":{}},{"name":"XAOC_COMP_MKT","type":"string","nullable":true,"metadata":{}},{"name":"MARKET_TYPE","type":"string","nullable":true,"metadata":{}},{"name":"MRKT_RSTR_ID","type":"integer","nullable":true,"metadata":{}},{"name":"MRKT_RSTR_POS","type":"integer","nullable":true,"metadata":{}},{"name":"MRKT_IND","type":"string","nullable":true,"metadata":{}},{"name":"CHANNEL","type":"string","nullable":true,"metadata":{}},{"name":"SHR_MRKT_KEY","type":"string","nullable":true,"metadata":{}},{"name":"ETH_MKT_TYP","type":"string","nullable":true,"metadata":{}},{"name":"MRKT_AGG_FLG","type":"integer","nullable":true,"metadata":{}},{"name":"DT_SRC_DTL_NM","type":"string","nullable":true,"metadata":{}},{"name":"SUPUP","type":"string","nullable":true,"metadata":{}},{"name":"COUNTRY","type":"string","nullable":true,"metadata":{}},{"name":"CS_MARKET_TYPE","type":"string","nullable":true,"metadata":{}},{"name":"CS_MARKET_SET","type":"string","nullable":true,"metadata":{}},{"name":"SUPPRESSION","type":"string","nullable":true,"metadata":{}},{"name":"DEF_MRKT_FLG","type":"string","nullable":true,"metadata":{}},{"name":"GEOGRAPHY","type":"string","nullable":true,"metadata":{}},{"name":"CS_CHANNEL","type":"string","nullable":true,"metadata":{}},{"name":"CS_MARKET_SET_DETAIL","type":"string","nullable":true,"metadata":{}},{"name":"CS_MARKET_TYPE_SET","type":"string","nullable":true,"metadata":{}},{"name":"CS_TA_CHANNEL","type":"string","nullable":true,"metadata":{}},{"name":"MRKT_TT_DMG_CD","type":"integer","nullable":true,"metadata":{}},{"name":"MRKT_TYPE_HS","type":"string","nullable":true,"metadata":{}},{"name":"TA_REM","type":"string","nullable":true,"metadata":{}},{"name":"XAOC_TA_REM","type":"string","nullable":true,"metadata":{}},{"name":"KO_MARKET_PARENT_CHANNEL","type":"string","nullable":true,"metadata":{}},{"name":"KO_MARKET_PARENT_1","type":"string","nullable":true,"metadata":{}},{"name":"KO_MARKET_PARENT_2","type":"string","nullable":true,"metadata":{}},{"name":"KO_MARKET_PARENT_3","type":"string","nullable":true,"metadata":{}},{"name":"KO_MARKET_PARENT_4","type":"string","nullable":true,"metadata":{}},{"name":"KO_MARKET_LEVEL_CHANNEL","type":"string","nullable":true,"metadata":{}},{"name":"KO_MARKET_LEVEL_1","type":"string","nullable":true,"metadata":{}},{"name":"KO_MARKET_LEVEL_2","type":"string","nullable":true,"metadata":{}},{"name":"KO_MARKET_LEVEL_3","type":"string","nullable":true,"metada, spark.sql.sources.schema.numParts=2, spark.sql.sources.provider=parquet, spark.sql.create.version=2.4.0}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{root=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:root, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:root, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:root, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:root, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null))	
19/10/17 21:09:39 INFO FileToDimension: >>>>>>>>>> Writing table bd_output_1000255.MRKT_DIM Completed.
19/10/17 21:09:39 INFO FileToDimension: FileToDimension Some(MarketCSV) for MRKT_DIM input file wasbs://teuatcontainer01@teuatstorage.blob.core.windows.net//bd_output_1000255/MRKT_DIM.csv>>>>> Completed
19/10/17 21:09:39 INFO FileToDimension: FileToDimension Some(PeriodCSV) for PRD_DIM input file wasbs://teuatcontainer01@teuatstorage.blob.core.windows.net//bd_output_1000255/PRD_DIM.csv>>>>> Started
19/10/17 21:09:39 INFO SQLAppStatusListener: Execution ID: 19 Total Executor Run Time: 2930
19/10/17 21:09:39 INFO InMemoryFileIndex: Start listing leaf files and directories. Size of Paths: 1; threshold: 32
19/10/17 21:09:39 INFO InMemoryFileIndex: Start listing leaf files and directories. Size of Paths: 0; threshold: 32
19/10/17 21:09:39 INFO InMemoryFileIndex: Start listing leaf files and directories. Size of Paths: 1; threshold: 32
19/10/17 21:09:39 INFO InMemoryFileIndex: Start listing leaf files and directories. Size of Paths: 0; threshold: 32
19/10/17 21:09:39 INFO FileSourceStrategy: Pruning directories with: 
19/10/17 21:09:39 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#428, None)) > 0)
19/10/17 21:09:39 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
19/10/17 21:09:39 INFO FileSourceScanExec: Pushed Filters: 
19/10/17 21:09:39 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 649.6 KB, free 54.2 GB)
19/10/17 21:09:39 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 46.9 KB, free 54.2 GB)
19/10/17 21:09:39 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 10.139.64.37:33311 (size: 46.9 KB, free: 54.2 GB)
19/10/17 21:09:39 INFO SparkContext: Created broadcast 6 from csv at FileToDimension.scala:30
19/10/17 21:09:39 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
19/10/17 21:09:39 INFO SparkContext: Starting job: csv at FileToDimension.scala:30
19/10/17 21:09:39 INFO DAGScheduler: Got job 3 (csv at FileToDimension.scala:30) with 1 output partitions
19/10/17 21:09:39 INFO DAGScheduler: Final stage: ResultStage 3 (csv at FileToDimension.scala:30)
19/10/17 21:09:39 INFO DAGScheduler: Parents of final stage: List()
19/10/17 21:09:39 INFO DAGScheduler: Missing parents: List()
19/10/17 21:09:39 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[11] at csv at FileToDimension.scala:30), which has no missing parents
19/10/17 21:09:39 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 14.1 KB, free 54.2 GB)
19/10/17 21:09:39 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 7.0 KB, free 54.2 GB)
19/10/17 21:09:39 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 10.139.64.37:33311 (size: 7.0 KB, free: 54.2 GB)
19/10/17 21:09:39 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1316
19/10/17 21:09:39 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[11] at csv at FileToDimension.scala:30) (first 15 tasks are for partitions Vector(0))
19/10/17 21:09:39 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks
19/10/17 21:09:39 INFO TaskSetManager: Jars for session None: Map(spark://10.139.64.37:32805/jars/addedFile5514790601964570712dom4j_dom4j_1_6_1-3b35e.jar -> 1571346483602, spark://10.139.64.37:32805/jars/addedFile9193930907825338753xml_apis_xml_apis_1_0_b2-167e6.jar -> 1571346483463, spark://10.139.64.37:32805/jars/addedFile4063054051174557443dbfs__FileStore_saak9001_pqm_core_models_1_0-b0ccd.jar -> 1571346483976, spark://10.139.64.37:32805/jars/addedFile7430104289198212865io_spray_spray_json_2_11_1_3_5-d5f78.jar -> 1571346484084, spark://10.139.64.37:32805/jars/addedFile921809598074381246com_typesafe_config_1_3_1-e3e42.jar -> 1571346483296, spark://10.139.64.37:32805/jars/addedFile5190065922075015895org_scalaj_scalaj_http_2_11_2_3_0-d69e4.jar -> 1571346484274, spark://10.139.64.37:32805/jars/addedFile2730237718112202115dbfs__FileStore_saak9001_transengine_1_0_SNAPSHOT-ab9b9.jar -> 1571346484193, spark://10.139.64.37:32805/jars/addedFile3437214070662729572org_rogach_scallop_2_11_3_3_1-ecc25.jar -> 1571346483129)
19/10/17 21:09:39 INFO TaskSetManager: Files for session None: Map(spark://10.139.64.37:32805/files/addedFile7430104289198212865io_spray_spray_json_2_11_1_3_5-d5f78.jar -> 1571346484077, spark://10.139.64.37:32805/files/addedFile9193930907825338753xml_apis_xml_apis_1_0_b2-167e6.jar -> 1571346483455, spark://10.139.64.37:32805/files/addedFile3437214070662729572org_rogach_scallop_2_11_3_3_1-ecc25.jar -> 1571346483114, spark://10.139.64.37:32805/files/addedFile5514790601964570712dom4j_dom4j_1_6_1-3b35e.jar -> 1571346483594, spark://10.139.64.37:32805/files/addedFile921809598074381246com_typesafe_config_1_3_1-e3e42.jar -> 1571346483288, spark://10.139.64.37:32805/files/addedFile2730237718112202115dbfs__FileStore_saak9001_transengine_1_0_SNAPSHOT-ab9b9.jar -> 1571346484186, spark://10.139.64.37:32805/files/addedFile4063054051174557443dbfs__FileStore_saak9001_pqm_core_models_1_0-b0ccd.jar -> 1571346483968, spark://10.139.64.37:32805/files/addedFile5190065922075015895org_scalaj_scalaj_http_2_11_2_3_0-d69e4.jar -> 1571346484265)
19/10/17 21:09:39 WARN FairSchedulableBuilder: A job was submitted with scheduler pool 4521178943219552436, which has not been configured. This can happen when the file that pools are read from isn't set, or when that file doesn't contain 4521178943219552436. Created 4521178943219552436 with default configuration (schedulingMode: FIFO, minShare: 0, weight: 1)
19/10/17 21:09:39 INFO FairSchedulableBuilder: Added task set TaskSet_3.0 tasks to pool 4521178943219552436
19/10/17 21:09:39 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3, 10.139.64.32, executor 0, partition 0, PROCESS_LOCAL, 6057 bytes)
19/10/17 21:09:39 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 10.139.64.32:33807 (size: 7.0 KB, free: 50.8 GB)
19/10/17 21:09:39 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 10.139.64.32:33807 (size: 46.9 KB, free: 50.8 GB)
19/10/17 21:09:39 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 161 ms on 10.139.64.32 (executor 0) (1/1)
19/10/17 21:09:39 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 4521178943219552436
19/10/17 21:09:39 INFO DAGScheduler: ResultStage 3 (csv at FileToDimension.scala:30) finished in 0.175 s
19/10/17 21:09:39 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
19/10/17 21:09:39 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
19/10/17 21:09:39 INFO DAGScheduler: Job 3 finished: csv at FileToDimension.scala:30, took 0.182644 s
19/10/17 21:09:39 INFO FileSourceStrategy: Pruning directories with: 
19/10/17 21:09:39 INFO FileSourceStrategy: Post-Scan Filters: 
19/10/17 21:09:39 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
19/10/17 21:09:39 INFO FileSourceScanExec: Pushed Filters: 
19/10/17 21:09:39 INFO SQLAppStatusListener: Execution ID: 20 Total Executor Run Time: 130
19/10/17 21:09:39 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 649.6 KB, free 54.2 GB)
19/10/17 21:09:39 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 46.9 KB, free 54.2 GB)
19/10/17 21:09:39 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 10.139.64.37:33311 (size: 46.9 KB, free: 54.2 GB)
19/10/17 21:09:39 INFO SparkContext: Created broadcast 8 from csv at FileToDimension.scala:30
19/10/17 21:09:39 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
19/10/17 21:09:39 INFO SparkContext: Starting job: csv at FileToDimension.scala:30
19/10/17 21:09:39 INFO DAGScheduler: Got job 4 (csv at FileToDimension.scala:30) with 1 output partitions
19/10/17 21:09:39 INFO DAGScheduler: Final stage: ResultStage 4 (csv at FileToDimension.scala:30)
19/10/17 21:09:39 INFO DAGScheduler: Parents of final stage: List()
19/10/17 21:09:39 INFO DAGScheduler: Missing parents: List()
19/10/17 21:09:39 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[16] at csv at FileToDimension.scala:30), which has no missing parents
19/10/17 21:09:39 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 17.9 KB, free 54.2 GB)
19/10/17 21:09:39 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 9.0 KB, free 54.2 GB)
19/10/17 21:09:39 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 10.139.64.37:33311 (size: 9.0 KB, free: 54.2 GB)
19/10/17 21:09:39 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1316
19/10/17 21:09:39 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[16] at csv at FileToDimension.scala:30) (first 15 tasks are for partitions Vector(0))
19/10/17 21:09:39 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks
19/10/17 21:09:39 INFO TaskSetManager: Jars for session None: Map(spark://10.139.64.37:32805/jars/addedFile5514790601964570712dom4j_dom4j_1_6_1-3b35e.jar -> 1571346483602, spark://10.139.64.37:32805/jars/addedFile9193930907825338753xml_apis_xml_apis_1_0_b2-167e6.jar -> 1571346483463, spark://10.139.64.37:32805/jars/addedFile4063054051174557443dbfs__FileStore_saak9001_pqm_core_models_1_0-b0ccd.jar -> 1571346483976, spark://10.139.64.37:32805/jars/addedFile7430104289198212865io_spray_spray_json_2_11_1_3_5-d5f78.jar -> 1571346484084, spark://10.139.64.37:32805/jars/addedFile921809598074381246com_typesafe_config_1_3_1-e3e42.jar -> 1571346483296, spark://10.139.64.37:32805/jars/addedFile5190065922075015895org_scalaj_scalaj_http_2_11_2_3_0-d69e4.jar -> 1571346484274, spark://10.139.64.37:32805/jars/addedFile2730237718112202115dbfs__FileStore_saak9001_transengine_1_0_SNAPSHOT-ab9b9.jar -> 1571346484193, spark://10.139.64.37:32805/jars/addedFile3437214070662729572org_rogach_scallop_2_11_3_3_1-ecc25.jar -> 1571346483129)
19/10/17 21:09:39 INFO TaskSetManager: Files for session None: Map(spark://10.139.64.37:32805/files/addedFile7430104289198212865io_spray_spray_json_2_11_1_3_5-d5f78.jar -> 1571346484077, spark://10.139.64.37:32805/files/addedFile9193930907825338753xml_apis_xml_apis_1_0_b2-167e6.jar -> 1571346483455, spark://10.139.64.37:32805/files/addedFile3437214070662729572org_rogach_scallop_2_11_3_3_1-ecc25.jar -> 1571346483114, spark://10.139.64.37:32805/files/addedFile5514790601964570712dom4j_dom4j_1_6_1-3b35e.jar -> 1571346483594, spark://10.139.64.37:32805/files/addedFile921809598074381246com_typesafe_config_1_3_1-e3e42.jar -> 1571346483288, spark://10.139.64.37:32805/files/addedFile2730237718112202115dbfs__FileStore_saak9001_transengine_1_0_SNAPSHOT-ab9b9.jar -> 1571346484186, spark://10.139.64.37:32805/files/addedFile4063054051174557443dbfs__FileStore_saak9001_pqm_core_models_1_0-b0ccd.jar -> 1571346483968, spark://10.139.64.37:32805/files/addedFile5190065922075015895org_scalaj_scalaj_http_2_11_2_3_0-d69e4.jar -> 1571346484265)
19/10/17 21:09:39 INFO FairSchedulableBuilder: Added task set TaskSet_4.0 tasks to pool 4521178943219552436
19/10/17 21:09:39 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4, 10.139.64.32, executor 0, partition 0, PROCESS_LOCAL, 6057 bytes)
19/10/17 21:09:39 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 10.139.64.32:33807 (size: 9.0 KB, free: 50.8 GB)
19/10/17 21:09:39 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 10.139.64.32:33807 (size: 46.9 KB, free: 50.8 GB)
19/10/17 21:09:39 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 229 ms on 10.139.64.32 (executor 0) (1/1)
19/10/17 21:09:39 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 4521178943219552436
19/10/17 21:09:39 INFO DAGScheduler: ResultStage 4 (csv at FileToDimension.scala:30) finished in 0.236 s
19/10/17 21:09:39 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
19/10/17 21:09:39 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
19/10/17 21:09:39 INFO DAGScheduler: Job 4 finished: csv at FileToDimension.scala:30, took 0.243571 s
19/10/17 21:09:39 INFO FileToDimension: >>>>>>>>>> Writing table bd_output_1000255.PRD_DIM Started.
19/10/17 21:09:39 INFO HiveMetaStore: 1: get_table : db=bd_output_1000255 tbl=prd_dim
19/10/17 21:09:39 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=bd_output_1000255 tbl=prd_dim	
19/10/17 21:09:39 INFO FileSourceStrategy: Pruning directories with: 
19/10/17 21:09:39 INFO FileSourceStrategy: Post-Scan Filters: 
19/10/17 21:09:39 INFO FileSourceStrategy: Output Data Schema: struct<QUARTERS: string, QUARTER_IN_YEAR_DSC: string, HYTD: string, PRD_SP: int, WEEKS12: string ... 69 more fields>
19/10/17 21:09:39 INFO FileSourceScanExec: Pushed Filters: 
19/10/17 21:09:39 INFO HiveMetaStore: 1: get_table : db=bd_output_1000255 tbl=prd_dim
19/10/17 21:09:39 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=bd_output_1000255 tbl=prd_dim	
19/10/17 21:09:39 INFO HiveMetaStore: 1: get_database: bd_output_1000255
19/10/17 21:09:39 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: bd_output_1000255	
19/10/17 21:09:39 INFO HiveMetaStore: 1: get_database: bd_output_1000255
19/10/17 21:09:39 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: bd_output_1000255	
19/10/17 21:09:39 DEBUG HttpTransport: HTTPRequest,Failed,cReqId:89ad70a3-9471-4f69-b34b-5011601a8e34.0,lat:8,err:HTTP404(FileNotFoundException),Reqlen:0,Resplen:269,token_ns:6100,sReqId:f3e151ed-d866-4781-b0bd-76c394ccb3a3,path:/user/hive/warehouse/bd_output_1000255.db/prd_dim,qp:op=GETFILESTATUS&tooid=true&api-version=2017-08-01
19/10/17 21:09:39 INFO HiveMetaStore: 1: get_database: bd_output_1000255
19/10/17 21:09:39 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: bd_output_1000255	
19/10/17 21:09:39 INFO HiveMetaStore: 1: get_database: bd_output_1000255
19/10/17 21:09:39 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: bd_output_1000255	
19/10/17 21:09:39 DEBUG HttpTransport: HTTPRequest,Failed,cReqId:24941061-0ca2-45f3-9818-29b6cc6dc251.0,lat:7,err:HTTP404(FileNotFoundException),Reqlen:0,Resplen:269,token_ns:3600,sReqId:8180f144-5650-4723-97bf-8cdc877d43e2,path:/user/hive/warehouse/bd_output_1000255.db/prd_dim,qp:op=GETFILESTATUS&tooid=true&api-version=2017-08-01
19/10/17 21:09:39 INFO ParquetFileFormat: Using user defined output committer for Parquet: org.apache.spark.sql.parquet.DirectParquetOutputCommitter
19/10/17 21:09:39 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 649.4 KB, free 54.2 GB)
19/10/17 21:09:39 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 46.9 KB, free 54.2 GB)
19/10/17 21:09:39 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 10.139.64.37:33311 (size: 46.9 KB, free: 54.2 GB)
19/10/17 21:09:39 INFO SparkContext: Created broadcast 10 from saveAsTable at DataFrameDecorator.scala:17
19/10/17 21:09:39 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
19/10/17 21:09:40 INFO SparkContext: Starting job: saveAsTable at DataFrameDecorator.scala:17
19/10/17 21:09:40 INFO DAGScheduler: Got job 5 (saveAsTable at DataFrameDecorator.scala:17) with 1 output partitions
19/10/17 21:09:40 INFO DAGScheduler: Final stage: ResultStage 5 (saveAsTable at DataFrameDecorator.scala:17)
19/10/17 21:09:40 INFO DAGScheduler: Parents of final stage: List()
19/10/17 21:09:40 INFO DAGScheduler: Missing parents: List()
19/10/17 21:09:40 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[18] at saveAsTable at DataFrameDecorator.scala:17), which has no missing parents
19/10/17 21:09:40 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 326.3 KB, free 54.2 GB)
19/10/17 21:09:40 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 106.6 KB, free 54.2 GB)
19/10/17 21:09:40 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 10.139.64.37:33311 (size: 106.6 KB, free: 54.2 GB)
19/10/17 21:09:40 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1316
19/10/17 21:09:40 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[18] at saveAsTable at DataFrameDecorator.scala:17) (first 15 tasks are for partitions Vector(0))
19/10/17 21:09:40 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks
19/10/17 21:09:40 INFO TaskSetManager: Jars for session None: Map(spark://10.139.64.37:32805/jars/addedFile5514790601964570712dom4j_dom4j_1_6_1-3b35e.jar -> 1571346483602, spark://10.139.64.37:32805/jars/addedFile9193930907825338753xml_apis_xml_apis_1_0_b2-167e6.jar -> 1571346483463, spark://10.139.64.37:32805/jars/addedFile4063054051174557443dbfs__FileStore_saak9001_pqm_core_models_1_0-b0ccd.jar -> 1571346483976, spark://10.139.64.37:32805/jars/addedFile7430104289198212865io_spray_spray_json_2_11_1_3_5-d5f78.jar -> 1571346484084, spark://10.139.64.37:32805/jars/addedFile921809598074381246com_typesafe_config_1_3_1-e3e42.jar -> 1571346483296, spark://10.139.64.37:32805/jars/addedFile5190065922075015895org_scalaj_scalaj_http_2_11_2_3_0-d69e4.jar -> 1571346484274, spark://10.139.64.37:32805/jars/addedFile2730237718112202115dbfs__FileStore_saak9001_transengine_1_0_SNAPSHOT-ab9b9.jar -> 1571346484193, spark://10.139.64.37:32805/jars/addedFile3437214070662729572org_rogach_scallop_2_11_3_3_1-ecc25.jar -> 1571346483129)
19/10/17 21:09:40 INFO TaskSetManager: Files for session None: Map(spark://10.139.64.37:32805/files/addedFile7430104289198212865io_spray_spray_json_2_11_1_3_5-d5f78.jar -> 1571346484077, spark://10.139.64.37:32805/files/addedFile9193930907825338753xml_apis_xml_apis_1_0_b2-167e6.jar -> 1571346483455, spark://10.139.64.37:32805/files/addedFile3437214070662729572org_rogach_scallop_2_11_3_3_1-ecc25.jar -> 1571346483114, spark://10.139.64.37:32805/files/addedFile5514790601964570712dom4j_dom4j_1_6_1-3b35e.jar -> 1571346483594, spark://10.139.64.37:32805/files/addedFile921809598074381246com_typesafe_config_1_3_1-e3e42.jar -> 1571346483288, spark://10.139.64.37:32805/files/addedFile2730237718112202115dbfs__FileStore_saak9001_transengine_1_0_SNAPSHOT-ab9b9.jar -> 1571346484186, spark://10.139.64.37:32805/files/addedFile4063054051174557443dbfs__FileStore_saak9001_pqm_core_models_1_0-b0ccd.jar -> 1571346483968, spark://10.139.64.37:32805/files/addedFile5190065922075015895org_scalaj_scalaj_http_2_11_2_3_0-d69e4.jar -> 1571346484265)
19/10/17 21:09:40 INFO FairSchedulableBuilder: Added task set TaskSet_5.0 tasks to pool 4521178943219552436
19/10/17 21:09:40 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5, 10.139.64.32, executor 0, partition 0, PROCESS_LOCAL, 6058 bytes)
19/10/17 21:09:40 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 10.139.64.32:33807 (size: 106.6 KB, free: 50.8 GB)
19/10/17 21:09:40 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 10.139.64.32:33807 (size: 46.9 KB, free: 50.8 GB)
19/10/17 21:09:40 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 722 ms on 10.139.64.32 (executor 0) (1/1)
19/10/17 21:09:40 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 4521178943219552436
19/10/17 21:09:40 INFO DAGScheduler: ResultStage 5 (saveAsTable at DataFrameDecorator.scala:17) finished in 0.752 s
19/10/17 21:09:40 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
19/10/17 21:09:40 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
19/10/17 21:09:40 INFO DAGScheduler: Job 5 finished: saveAsTable at DataFrameDecorator.scala:17, took 0.757433 s
19/10/17 21:09:40 INFO DirectoryAtomicCommitProtocol: Committing job 50addd10-8cae-4556-903d-173ed6488e4f
19/10/17 21:09:40 INFO FileSystem: FS_OP_CREATE FILE[adl://connectedsystemsuatdl.azuredatalakestore.net/user/hive/warehouse/bd_output_1000255.db/prd_dim/_committed_2583835388008042733] Creating output stream; permission: rw-r--r--, overwrite: true, bufferSize: 65536
19/10/17 21:09:40 INFO AdlFsOutputStream: FS_OP_CREATE FILE[/user/hive/warehouse/bd_output_1000255.db/prd_dim/_committed_2583835388008042733] Closing stream; size: 120
19/10/17 21:09:40 INFO AdlFsOutputStream: FS_OP_CREATE FILE[/user/hive/warehouse/bd_output_1000255.db/prd_dim/_committed_2583835388008042733] Upload complete; size: 120
19/10/17 21:09:40 INFO AdlFsOutputStream: FS_OP_CREATE FILE[/user/hive/warehouse/bd_output_1000255.db/prd_dim/_committed_2583835388008042733] Closing stream; size: 120
19/10/17 21:09:40 INFO AdlFsOutputStream: FS_OP_CREATE FILE[/user/hive/warehouse/bd_output_1000255.db/prd_dim/_committed_2583835388008042733] Upload complete; size: 120
19/10/17 21:09:40 INFO FileSystem: FS_OP_CREATE FILE[adl://connectedsystemsuatdl.azuredatalakestore.net/user/hive/warehouse/bd_output_1000255.db/prd_dim/_SUCCESS] Creating output stream; permission: rw-r--r--, overwrite: true, bufferSize: 65536
19/10/17 21:09:41 INFO AdlFsOutputStream: FS_OP_CREATE FILE[/user/hive/warehouse/bd_output_1000255.db/prd_dim/_SUCCESS] Closing stream; size: 0
19/10/17 21:09:41 INFO AdlFsOutputStream: FS_OP_CREATE FILE[/user/hive/warehouse/bd_output_1000255.db/prd_dim/_SUCCESS] Upload complete; size: 0
19/10/17 21:09:41 INFO FileSystem: FS_OP_CREATE FILE[adl://connectedsystemsuatdl.azuredatalakestore.net/user/hive/warehouse/bd_output_1000255.db/prd_dim/_SUCCESS] Creating output stream; permission: rw-r--r--, overwrite: true, bufferSize: 65536
19/10/17 21:09:41 INFO AdlFsOutputStream: FS_OP_CREATE FILE[/user/hive/warehouse/bd_output_1000255.db/prd_dim/_SUCCESS] Closing stream; size: 0
19/10/17 21:09:41 INFO AdlFsOutputStream: FS_OP_CREATE FILE[/user/hive/warehouse/bd_output_1000255.db/prd_dim/_SUCCESS] Upload complete; size: 0
19/10/17 21:09:41 INFO DirectoryAtomicCommitProtocol: Job commit completed for 50addd10-8cae-4556-903d-173ed6488e4f
19/10/17 21:09:41 INFO DirectoryAtomicCommitProtocol: Auto-vacuuming directories updated by 50addd10-8cae-4556-903d-173ed6488e4f
19/10/17 21:09:41 INFO DirectoryAtomicCommitProtocol: Started VACUUM on List(adl://connectedsystemsuatdl.azuredatalakestore.net/user/hive/warehouse/bd_output_1000255.db/prd_dim) with data horizon 1571173781203 (now - 48.0 hours), metadata horizon 1571344781203 (now - 0.5 hours)
19/10/17 21:09:41 WARN FileOperations: method deleteMultiple doesn't exist. Falling back to individual deletion.
19/10/17 21:09:41 INFO FileFormatWriter: Write Job 652ee032-f3c5-4c0e-bd25-2024ccbdd7f8 committed.
19/10/17 21:09:41 INFO FileFormatWriter: Finished processing stats for write job 652ee032-f3c5-4c0e-bd25-2024ccbdd7f8.
19/10/17 21:09:41 DEBUG HttpTransport: HTTPRequest,Failed,cReqId:9aaa77ce-3069-4dca-b00a-df98d030db59.0,lat:9,err:HTTP404(FileNotFoundException),Reqlen:0,Resplen:285,token_ns:4500,sReqId:e7be1afb-0fd0-4084-89a9-d7f3db644408,path:/user/hive/warehouse/bd_output_1000255.db/prd_dim/_spark_metadata,qp:op=GETFILESTATUS&tooid=true&api-version=2017-08-01
19/10/17 21:09:41 INFO InMemoryFileIndex: Start listing leaf files and directories. Size of Paths: 1; threshold: 32
19/10/17 21:09:41 INFO InMemoryFileIndex: Start listing leaf files and directories. Size of Paths: 0; threshold: 32
19/10/17 21:09:41 INFO HiveMetaStore: 1: get_database: bd_output_1000255
19/10/17 21:09:41 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: bd_output_1000255	
19/10/17 21:09:41 INFO HiveMetaStore: 1: get_table : db=bd_output_1000255 tbl=prd_dim
19/10/17 21:09:41 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=bd_output_1000255 tbl=prd_dim	
19/10/17 21:09:41 INFO HiveMetaStore: 1: get_database: bd_output_1000255
19/10/17 21:09:41 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: bd_output_1000255	
19/10/17 21:09:41 INFO HiveMetaStore: 1: get_table : db=bd_output_1000255 tbl=prd_dim
19/10/17 21:09:41 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=bd_output_1000255 tbl=prd_dim	
19/10/17 21:09:41 INFO SecuredHiveExternalCatalog: Persisting file based data source table `bd_output_1000255`.`prd_dim` into Hive metastore in Hive compatible format.
19/10/17 21:09:41 INFO HiveMetaStore: 1: create_table: Table(tableName:prd_dim, dbName:bd_output_1000255, owner:root, createTime:1571346579, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:QUARTERS, type:string, comment:null), FieldSchema(name:QUARTER_IN_YEAR_DSC, type:string, comment:null), FieldSchema(name:HYTD, type:string, comment:null), FieldSchema(name:PRD_SP, type:int, comment:null), FieldSchema(name:WEEKS12, type:string, comment:null), FieldSchema(name:QTD, type:string, comment:null), FieldSchema(name:PRD_DSP_ORDER, type:int, comment:null), FieldSchema(name:MONTHS6, type:string, comment:null), FieldSchema(name:PRD_RLS_IND, type:string, comment:null), FieldSchema(name:HOLIDAY_PERIODS, type:string, comment:null), FieldSchema(name:WEEKS4, type:string, comment:null), FieldSchema(name:YEAR_ORD, type:int, comment:null), FieldSchema(name:YTD, type:string, comment:null), FieldSchema(name:WEEKS24, type:string, comment:null), FieldSchema(name:PMONTH_DSC, type:string, comment:null), FieldSchema(name:FQUARTER_IN_YEAR_DSC, type:string, comment:null), FieldSchema(name:PYEAR_ORD, type:int, comment:null), FieldSchema(name:PWEEK_ORD, type:int, comment:null), FieldSchema(name:PRD_KEY, type:int, comment:null), FieldSchema(name:WEEK_ORD, type:int, comment:null), FieldSchema(name:MONTHS3, type:string, comment:null), FieldSchema(name:CUME_RUL_TYP, type:string, comment:null), FieldSchema(name:PRD_ID, type:int, comment:null), FieldSchema(name:MONTHLY, type:string, comment:null), FieldSchema(name:MONTHS12, type:string, comment:null), FieldSchema(name:RPT_MONTHS, type:string, comment:null), FieldSchema(name:PYEAR_DSC, type:string, comment:null), FieldSchema(name:HALF_YEARS_RTA, type:string, comment:null), FieldSchema(name:WK_ENDING_DSC, type:string, comment:null), FieldSchema(name:MONTHS, type:string, comment:null), FieldSchema(name:PRD_DSC_SHORT, type:string, comment:null), FieldSchema(name:SQUARTER_IN_YEAR_DSC, type:string, comment:null), FieldSchema(name:PRD_DSC_LONG, type:string, comment:null), FieldSchema(name:PPERIOD_DSC, type:string, comment:null), FieldSchema(name:PCKG_ID, type:int, comment:null), FieldSchema(name:PWEEK_DSC, type:string, comment:null), FieldSchema(name:PPERIOD_ORD, type:int, comment:null), FieldSchema(name:SWEEK_IN_YEAR_DSC, type:string, comment:null), FieldSchema(name:CUME_END_DT, type:timestamp, comment:null), FieldSchema(name:MONTH_IN_QUARTER_DSC, type:string, comment:null), FieldSchema(name:PRD_DET_LVL, type:string, comment:null), FieldSchema(name:SMONTH_IN_QUARTER_DSC, type:string, comment:null), FieldSchema(name:PRD_ID_YAGO, type:int, comment:null), FieldSchema(name:WEEKS52, type:string, comment:null), FieldSchema(name:CURRENT_PERIODS, type:string, comment:null), FieldSchema(name:WEEK_IN_MONTH_DSC, type:string, comment:null), FieldSchema(name:WEEKS26, type:string, comment:null), FieldSchema(name:MONTH_ORD, type:int, comment:null), FieldSchema(name:PQUARTER_DSC, type:string, comment:null), FieldSchema(name:FILTER_NAME, type:string, comment:null), FieldSchema(name:PRD_DSP_FOLDER, type:string, comment:null), FieldSchema(name:PRD_SRC_ID, type:int, comment:null), FieldSchema(name:WEEKS4_ORD, type:int, comment:null), FieldSchema(name:DIM_TYP, type:string, comment:null), FieldSchema(name:HALF_YEARS, type:string, comment:null), FieldSchema(name:PQUARTER_ORD, type:int, comment:null), FieldSchema(name:MONTHS1, type:string, comment:null), FieldSchema(name:HOLIDAYS, type:string, comment:null), FieldSchema(name:FYEAR_ORD, type:int, comment:null), FieldSchema(name:PRD_TAG, type:string, comment:null), FieldSchema(name:WEEKS13, type:string, comment:null), FieldSchema(name:SYEAR_DSC, type:string, comment:null), FieldSchema(name:SPERIOD_DSC, type:string, comment:null), FieldSchema(name:NUM_OF_WKS, type:int, comment:null), FieldSchema(name:WK_ENDING_DT, type:timestamp, comment:null), FieldSchema(name:YEAR_DSC, type:string, comment:null), FieldSchema(name:QUARTER_ORD, type:int, comment:null), FieldSchema(name:PERIOD_TO_DATE, type:string, comment:null), FieldSchema(name:DSP_FOLDER_AD, type:string, comment:null), FieldSchema(name:FYEAR_DSC, type:string, comment:null), FieldSchema(name:QUARTERS_RTA, type:string, comment:null)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, parameters:{path=adl://connectedsystemsuatdl.azuredatalakestore.net/user/hive/warehouse/bd_output_1000255.db/prd_dim, serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{spark.sql.sources.schema.part.1=adata":{}},{"name":"FYEAR_ORD","type":"integer","nullable":true,"metadata":{}},{"name":"PRD_TAG","type":"string","nullable":true,"metadata":{}},{"name":"WEEKS13","type":"string","nullable":true,"metadata":{}},{"name":"SYEAR_DSC","type":"string","nullable":true,"metadata":{}},{"name":"SPERIOD_DSC","type":"string","nullable":true,"metadata":{}},{"name":"NUM_OF_WKS","type":"integer","nullable":true,"metadata":{}},{"name":"WK_ENDING_DT","type":"timestamp","nullable":true,"metadata":{}},{"name":"YEAR_DSC","type":"string","nullable":true,"metadata":{}},{"name":"QUARTER_ORD","type":"integer","nullable":true,"metadata":{}},{"name":"PERIOD_TO_DATE","type":"string","nullable":true,"metadata":{}},{"name":"DSP_FOLDER_AD","type":"string","nullable":true,"metadata":{}},{"name":"FYEAR_DSC","type":"string","nullable":true,"metadata":{}},{"name":"QUARTERS_RTA","type":"string","nullable":true,"metadata":{}}]}, spark.sql.sources.schema.part.0={"type":"struct","fields":[{"name":"QUARTERS","type":"string","nullable":true,"metadata":{}},{"name":"QUARTER_IN_YEAR_DSC","type":"string","nullable":true,"metadata":{}},{"name":"HYTD","type":"string","nullable":true,"metadata":{}},{"name":"PRD_SP","type":"integer","nullable":true,"metadata":{}},{"name":"WEEKS12","type":"string","nullable":true,"metadata":{}},{"name":"QTD","type":"string","nullable":true,"metadata":{}},{"name":"PRD_DSP_ORDER","type":"integer","nullable":true,"metadata":{}},{"name":"MONTHS6","type":"string","nullable":true,"metadata":{}},{"name":"PRD_RLS_IND","type":"string","nullable":true,"metadata":{}},{"name":"HOLIDAY_PERIODS","type":"string","nullable":true,"metadata":{}},{"name":"WEEKS4","type":"string","nullable":true,"metadata":{}},{"name":"YEAR_ORD","type":"integer","nullable":true,"metadata":{}},{"name":"YTD","type":"string","nullable":true,"metadata":{}},{"name":"WEEKS24","type":"string","nullable":true,"metadata":{}},{"name":"PMONTH_DSC","type":"string","nullable":true,"metadata":{}},{"name":"FQUARTER_IN_YEAR_DSC","type":"string","nullable":true,"metadata":{}},{"name":"PYEAR_ORD","type":"integer","nullable":true,"metadata":{}},{"name":"PWEEK_ORD","type":"integer","nullable":true,"metadata":{}},{"name":"PRD_KEY","type":"integer","nullable":true,"metadata":{}},{"name":"WEEK_ORD","type":"integer","nullable":true,"metadata":{}},{"name":"MONTHS3","type":"string","nullable":true,"metadata":{}},{"name":"CUME_RUL_TYP","type":"string","nullable":true,"metadata":{}},{"name":"PRD_ID","type":"integer","nullable":true,"metadata":{}},{"name":"MONTHLY","type":"string","nullable":true,"metadata":{}},{"name":"MONTHS12","type":"string","nullable":true,"metadata":{}},{"name":"RPT_MONTHS","type":"string","nullable":true,"metadata":{}},{"name":"PYEAR_DSC","type":"string","nullable":true,"metadata":{}},{"name":"HALF_YEARS_RTA","type":"string","nullable":true,"metadata":{}},{"name":"WK_ENDING_DSC","type":"string","nullable":true,"metadata":{}},{"name":"MONTHS","type":"string","nullable":true,"metadata":{}},{"name":"PRD_DSC_SHORT","type":"string","nullable":true,"metadata":{}},{"name":"SQUARTER_IN_YEAR_DSC","type":"string","nullable":true,"metadata":{}},{"name":"PRD_DSC_LONG","type":"string","nullable":true,"metadata":{}},{"name":"PPERIOD_DSC","type":"string","nullable":true,"metadata":{}},{"name":"PCKG_ID","type":"integer","nullable":true,"metadata":{}},{"name":"PWEEK_DSC","type":"string","nullable":true,"metadata":{}},{"name":"PPERIOD_ORD","type":"integer","nullable":true,"metadata":{}},{"name":"SWEEK_IN_YEAR_DSC","type":"string","nullable":true,"metadata":{}},{"name":"CUME_END_DT","type":"timestamp","nullable":true,"metadata":{}},{"name":"MONTH_IN_QUARTER_DSC","type":"string","nullable":true,"metadata":{}},{"name":"PRD_DET_LVL","type":"string","nullable":true,"metadata":{}},{"name":"SMONTH_IN_QUARTER_DSC","type":"string","nullable":true,"metadata":{}},{"name":"PRD_ID_YAGO","type":"integer","nullable":true,"metadata":{}},{"name":"WEEKS52","type":"string","nullable":true,"metadata":{}},{"name":"CURRENT_PERIODS","type":"string","nullable":true,"metadata":{}},{"name":"WEEK_IN_MONTH_DSC","type":"string","nullable":true,"metadata":{}},{"name":"WEEKS26","type":"string","nullable":true,"metadata":{}},{"name":"MONTH_ORD","type":"integer","nullable":true,"metadata":{}},{"name":"PQUARTER_DSC","type":"string","nullable":true,"metadata":{}},{"name":"FILTER_NAME","type":"string","nullable":true,"metadata":{}},{"name":"PRD_DSP_FOLDER","type":"string","nullable":true,"metadata":{}},{"name":"PRD_SRC_ID","type":"integer","nullable":true,"metadata":{}},{"name":"WEEKS4_ORD","type":"integer","nullable":true,"metadata":{}},{"name":"DIM_TYP","type":"string","nullable":true,"metadata":{}},{"name":"HALF_YEARS","type":"string","nullable":true,"metadata":{}},{"name":"PQUARTER_ORD","type":"integer","nullable":true,"metadata":{}},{"name":"MONTHS1","type":"string","nullable":true,"metadata":{}},{"name":"HOLIDAYS","type":"string","nullable":true,"met, spark.sql.sources.schema.numParts=2, spark.sql.sources.provider=parquet, spark.sql.create.version=2.4.0}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{root=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:root, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:root, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:root, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:root, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null))
19/10/17 21:09:41 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=create_table: Table(tableName:prd_dim, dbName:bd_output_1000255, owner:root, createTime:1571346579, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:QUARTERS, type:string, comment:null), FieldSchema(name:QUARTER_IN_YEAR_DSC, type:string, comment:null), FieldSchema(name:HYTD, type:string, comment:null), FieldSchema(name:PRD_SP, type:int, comment:null), FieldSchema(name:WEEKS12, type:string, comment:null), FieldSchema(name:QTD, type:string, comment:null), FieldSchema(name:PRD_DSP_ORDER, type:int, comment:null), FieldSchema(name:MONTHS6, type:string, comment:null), FieldSchema(name:PRD_RLS_IND, type:string, comment:null), FieldSchema(name:HOLIDAY_PERIODS, type:string, comment:null), FieldSchema(name:WEEKS4, type:string, comment:null), FieldSchema(name:YEAR_ORD, type:int, comment:null), FieldSchema(name:YTD, type:string, comment:null), FieldSchema(name:WEEKS24, type:string, comment:null), FieldSchema(name:PMONTH_DSC, type:string, comment:null), FieldSchema(name:FQUARTER_IN_YEAR_DSC, type:string, comment:null), FieldSchema(name:PYEAR_ORD, type:int, comment:null), FieldSchema(name:PWEEK_ORD, type:int, comment:null), FieldSchema(name:PRD_KEY, type:int, comment:null), FieldSchema(name:WEEK_ORD, type:int, comment:null), FieldSchema(name:MONTHS3, type:string, comment:null), FieldSchema(name:CUME_RUL_TYP, type:string, comment:null), FieldSchema(name:PRD_ID, type:int, comment:null), FieldSchema(name:MONTHLY, type:string, comment:null), FieldSchema(name:MONTHS12, type:string, comment:null), FieldSchema(name:RPT_MONTHS, type:string, comment:null), FieldSchema(name:PYEAR_DSC, type:string, comment:null), FieldSchema(name:HALF_YEARS_RTA, type:string, comment:null), FieldSchema(name:WK_ENDING_DSC, type:string, comment:null), FieldSchema(name:MONTHS, type:string, comment:null), FieldSchema(name:PRD_DSC_SHORT, type:string, comment:null), FieldSchema(name:SQUARTER_IN_YEAR_DSC, type:string, comment:null), FieldSchema(name:PRD_DSC_LONG, type:string, comment:null), FieldSchema(name:PPERIOD_DSC, type:string, comment:null), FieldSchema(name:PCKG_ID, type:int, comment:null), FieldSchema(name:PWEEK_DSC, type:string, comment:null), FieldSchema(name:PPERIOD_ORD, type:int, comment:null), FieldSchema(name:SWEEK_IN_YEAR_DSC, type:string, comment:null), FieldSchema(name:CUME_END_DT, type:timestamp, comment:null), FieldSchema(name:MONTH_IN_QUARTER_DSC, type:string, comment:null), FieldSchema(name:PRD_DET_LVL, type:string, comment:null), FieldSchema(name:SMONTH_IN_QUARTER_DSC, type:string, comment:null), FieldSchema(name:PRD_ID_YAGO, type:int, comment:null), FieldSchema(name:WEEKS52, type:string, comment:null), FieldSchema(name:CURRENT_PERIODS, type:string, comment:null), FieldSchema(name:WEEK_IN_MONTH_DSC, type:string, comment:null), FieldSchema(name:WEEKS26, type:string, comment:null), FieldSchema(name:MONTH_ORD, type:int, comment:null), FieldSchema(name:PQUARTER_DSC, type:string, comment:null), FieldSchema(name:FILTER_NAME, type:string, comment:null), FieldSchema(name:PRD_DSP_FOLDER, type:string, comment:null), FieldSchema(name:PRD_SRC_ID, type:int, comment:null), FieldSchema(name:WEEKS4_ORD, type:int, comment:null), FieldSchema(name:DIM_TYP, type:string, comment:null), FieldSchema(name:HALF_YEARS, type:string, comment:null), FieldSchema(name:PQUARTER_ORD, type:int, comment:null), FieldSchema(name:MONTHS1, type:string, comment:null), FieldSchema(name:HOLIDAYS, type:string, comment:null), FieldSchema(name:FYEAR_ORD, type:int, comment:null), FieldSchema(name:PRD_TAG, type:string, comment:null), FieldSchema(name:WEEKS13, type:string, comment:null), FieldSchema(name:SYEAR_DSC, type:string, comment:null), FieldSchema(name:SPERIOD_DSC, type:string, comment:null), FieldSchema(name:NUM_OF_WKS, type:int, comment:null), FieldSchema(name:WK_ENDING_DT, type:timestamp, comment:null), FieldSchema(name:YEAR_DSC, type:string, comment:null), FieldSchema(name:QUARTER_ORD, type:int, comment:null), FieldSchema(name:PERIOD_TO_DATE, type:string, comment:null), FieldSchema(name:DSP_FOLDER_AD, type:string, comment:null), FieldSchema(name:FYEAR_DSC, type:string, comment:null), FieldSchema(name:QUARTERS_RTA, type:string, comment:null)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, parameters:{path=adl://connectedsystemsuatdl.azuredatalakestore.net/user/hive/warehouse/bd_output_1000255.db/prd_dim, serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{spark.sql.sources.schema.part.1=adata":{}},{"name":"FYEAR_ORD","type":"integer","nullable":true,"metadata":{}},{"name":"PRD_TAG","type":"string","nullable":true,"metadata":{}},{"name":"WEEKS13","type":"string","nullable":true,"metadata":{}},{"name":"SYEAR_DSC","type":"string","nullable":true,"metadata":{}},{"name":"SPERIOD_DSC","type":"string","nullable":true,"metadata":{}},{"name":"NUM_OF_WKS","type":"integer","nullable":true,"metadata":{}},{"name":"WK_ENDING_DT","type":"timestamp","nullable":true,"metadata":{}},{"name":"YEAR_DSC","type":"string","nullable":true,"metadata":{}},{"name":"QUARTER_ORD","type":"integer","nullable":true,"metadata":{}},{"name":"PERIOD_TO_DATE","type":"string","nullable":true,"metadata":{}},{"name":"DSP_FOLDER_AD","type":"string","nullable":true,"metadata":{}},{"name":"FYEAR_DSC","type":"string","nullable":true,"metadata":{}},{"name":"QUARTERS_RTA","type":"string","nullable":true,"metadata":{}}]}, spark.sql.sources.schema.part.0={"type":"struct","fields":[{"name":"QUARTERS","type":"string","nullable":true,"metadata":{}},{"name":"QUARTER_IN_YEAR_DSC","type":"string","nullable":true,"metadata":{}},{"name":"HYTD","type":"string","nullable":true,"metadata":{}},{"name":"PRD_SP","type":"integer","nullable":true,"metadata":{}},{"name":"WEEKS12","type":"string","nullable":true,"metadata":{}},{"name":"QTD","type":"string","nullable":true,"metadata":{}},{"name":"PRD_DSP_ORDER","type":"integer","nullable":true,"metadata":{}},{"name":"MONTHS6","type":"string","nullable":true,"metadata":{}},{"name":"PRD_RLS_IND","type":"string","nullable":true,"metadata":{}},{"name":"HOLIDAY_PERIODS","type":"string","nullable":true,"metadata":{}},{"name":"WEEKS4","type":"string","nullable":true,"metadata":{}},{"name":"YEAR_ORD","type":"integer","nullable":true,"metadata":{}},{"name":"YTD","type":"string","nullable":true,"metadata":{}},{"name":"WEEKS24","type":"string","nullable":true,"metadata":{}},{"name":"PMONTH_DSC","type":"string","nullable":true,"metadata":{}},{"name":"FQUARTER_IN_YEAR_DSC","type":"string","nullable":true,"metadata":{}},{"name":"PYEAR_ORD","type":"integer","nullable":true,"metadata":{}},{"name":"PWEEK_ORD","type":"integer","nullable":true,"metadata":{}},{"name":"PRD_KEY","type":"integer","nullable":true,"metadata":{}},{"name":"WEEK_ORD","type":"integer","nullable":true,"metadata":{}},{"name":"MONTHS3","type":"string","nullable":true,"metadata":{}},{"name":"CUME_RUL_TYP","type":"string","nullable":true,"metadata":{}},{"name":"PRD_ID","type":"integer","nullable":true,"metadata":{}},{"name":"MONTHLY","type":"string","nullable":true,"metadata":{}},{"name":"MONTHS12","type":"string","nullable":true,"metadata":{}},{"name":"RPT_MONTHS","type":"string","nullable":true,"metadata":{}},{"name":"PYEAR_DSC","type":"string","nullable":true,"metadata":{}},{"name":"HALF_YEARS_RTA","type":"string","nullable":true,"metadata":{}},{"name":"WK_ENDING_DSC","type":"string","nullable":true,"metadata":{}},{"name":"MONTHS","type":"string","nullable":true,"metadata":{}},{"name":"PRD_DSC_SHORT","type":"string","nullable":true,"metadata":{}},{"name":"SQUARTER_IN_YEAR_DSC","type":"string","nullable":true,"metadata":{}},{"name":"PRD_DSC_LONG","type":"string","nullable":true,"metadata":{}},{"name":"PPERIOD_DSC","type":"string","nullable":true,"metadata":{}},{"name":"PCKG_ID","type":"integer","nullable":true,"metadata":{}},{"name":"PWEEK_DSC","type":"string","nullable":true,"metadata":{}},{"name":"PPERIOD_ORD","type":"integer","nullable":true,"metadata":{}},{"name":"SWEEK_IN_YEAR_DSC","type":"string","nullable":true,"metadata":{}},{"name":"CUME_END_DT","type":"timestamp","nullable":true,"metadata":{}},{"name":"MONTH_IN_QUARTER_DSC","type":"string","nullable":true,"metadata":{}},{"name":"PRD_DET_LVL","type":"string","nullable":true,"metadata":{}},{"name":"SMONTH_IN_QUARTER_DSC","type":"string","nullable":true,"metadata":{}},{"name":"PRD_ID_YAGO","type":"integer","nullable":true,"metadata":{}},{"name":"WEEKS52","type":"string","nullable":true,"metadata":{}},{"name":"CURRENT_PERIODS","type":"string","nullable":true,"metadata":{}},{"name":"WEEK_IN_MONTH_DSC","type":"string","nullable":true,"metadata":{}},{"name":"WEEKS26","type":"string","nullable":true,"metadata":{}},{"name":"MONTH_ORD","type":"integer","nullable":true,"metadata":{}},{"name":"PQUARTER_DSC","type":"string","nullable":true,"metadata":{}},{"name":"FILTER_NAME","type":"string","nullable":true,"metadata":{}},{"name":"PRD_DSP_FOLDER","type":"string","nullable":true,"metadata":{}},{"name":"PRD_SRC_ID","type":"integer","nullable":true,"metadata":{}},{"name":"WEEKS4_ORD","type":"integer","nullable":true,"metadata":{}},{"name":"DIM_TYP","type":"string","nullable":true,"metadata":{}},{"name":"HALF_YEARS","type":"string","nullable":true,"metadata":{}},{"name":"PQUARTER_ORD","type":"integer","nullable":true,"metadata":{}},{"name":"MONTHS1","type":"string","nullable":true,"metadata":{}},{"name":"HOLIDAYS","type":"string","nullable":true,"met, spark.sql.sources.schema.numParts=2, spark.sql.sources.provider=parquet, spark.sql.create.version=2.4.0}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{root=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:root, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:root, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:root, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:root, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null))	
19/10/17 21:09:41 INFO FileToDimension: >>>>>>>>>> Writing table bd_output_1000255.PRD_DIM Completed.
19/10/17 21:09:41 INFO FileToDimension: FileToDimension Some(PeriodCSV) for PRD_DIM input file wasbs://teuatcontainer01@teuatstorage.blob.core.windows.net//bd_output_1000255/PRD_DIM.csv>>>>> Completed
19/10/17 21:09:41 INFO SQLAppStatusListener: Execution ID: 21 Total Executor Run Time: 687
19/10/17 21:09:41 INFO HiveMetaStore: 1: get_table : db=default tbl=bd_output_1000255
19/10/17 21:09:41 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=default tbl=bd_output_1000255	
19/10/17 21:09:41 ERROR ScalaDriverLocal: User Code Stack Trace: 
org.apache.spark.sql.AnalysisException: Table or view not found: bd_output_1000255; line 1 pos 15
	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:47)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableFromCatalog(Analyzer.scala:750)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.resolveRelation(Analyzer.scala:695)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$8.applyOrElse(Analyzer.scala:731)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$8.applyOrElse(Analyzer.scala:724)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$apply$1.apply(AnalysisHelper.scala:90)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$apply$1.apply(AnalysisHelper.scala:90)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:77)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:89)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$8.apply(TreeNode.scala:351)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:208)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:349)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:87)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:724)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:664)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:105)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:102)
	at scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:124)
	at scala.collection.immutable.List.foldLeft(List.scala:84)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:102)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:94)
	at scala.collection.immutable.List.foreach(List.scala:381)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:94)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:136)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:130)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:102)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$executeAndTrack$1.apply(RuleExecutor.scala:80)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$executeAndTrack$1.apply(RuleExecutor.scala:80)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:88)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:79)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:114)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:113)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:113)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$analyzed$1.apply(QueryExecution.scala:82)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$analyzed$1.apply(QueryExecution.scala:80)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:80)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:80)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:72)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:88)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:696)
	at com.nielsen.te.v3.executor.SQLStepExecutor.executeStep(StepExecutor.scala:29)
	at com.nielsen.te.v3.executor.StepProcessor$$anonfun$stepProcessor$1.apply(StepProcessor.scala:129)
	at com.nielsen.te.v3.executor.StepProcessor$$anonfun$stepProcessor$1.apply(StepProcessor.scala:127)
	at scala.collection.immutable.List.foreach(List.scala:381)
	at com.nielsen.te.v3.executor.StepProcessor$.stepProcessor(StepProcessor.scala:127)
	at com.nielsen.te.v3.TEv3Main$.appMain(TEv3Main.scala:47)
	at com.nielsen.te.v3.TEv3Main$.main(TEv3Main.scala:51)
	at line1befe054149d4284b87dacf6847fefc225.$read$$iw$$iw$$iw$$iw$$iw$$iw.<init>(command--1:1)
	at line1befe054149d4284b87dacf6847fefc225.$read$$iw$$iw$$iw$$iw$$iw.<init>(command--1:44)
	at line1befe054149d4284b87dacf6847fefc225.$read$$iw$$iw$$iw$$iw.<init>(command--1:46)
	at line1befe054149d4284b87dacf6847fefc225.$read$$iw$$iw$$iw.<init>(command--1:48)
	at line1befe054149d4284b87dacf6847fefc225.$read$$iw$$iw.<init>(command--1:50)
	at line1befe054149d4284b87dacf6847fefc225.$read$$iw.<init>(command--1:52)
	at line1befe054149d4284b87dacf6847fefc225.$read.<init>(command--1:54)
	at line1befe054149d4284b87dacf6847fefc225.$read$.<init>(command--1:58)
	at line1befe054149d4284b87dacf6847fefc225.$read$.<clinit>(command--1)
	at line1befe054149d4284b87dacf6847fefc225.$eval$.$print$lzycompute(<notebook>:7)
	at line1befe054149d4284b87dacf6847fefc225.$eval$.$print(<notebook>:6)
	at line1befe054149d4284b87dacf6847fefc225.$eval.$print(<notebook>)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:786)
	at scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:1047)
	at scala.tools.nsc.interpreter.IMain$WrappedRequest$$anonfun$loadAndRunReq$1.apply(IMain.scala:638)
	at scala.tools.nsc.interpreter.IMain$WrappedRequest$$anonfun$loadAndRunReq$1.apply(IMain.scala:637)
	at scala.reflect.internal.util.ScalaClassLoader$class.asContext(ScalaClassLoader.scala:31)
	at scala.reflect.internal.util.AbstractFileClassLoader.asContext(AbstractFileClassLoader.scala:19)
	at scala.tools.nsc.interpreter.IMain$WrappedRequest.loadAndRunReq(IMain.scala:637)
	at scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:569)
	at scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:565)
	at com.databricks.backend.daemon.driver.DriverILoop.execute(DriverILoop.scala:199)
	at com.databricks.backend.daemon.driver.ScalaDriverLocal$$anonfun$repl$1.apply$mcV$sp(ScalaDriverLocal.scala:189)
	at com.databricks.backend.daemon.driver.ScalaDriverLocal$$anonfun$repl$1.apply(ScalaDriverLocal.scala:189)
	at com.databricks.backend.daemon.driver.ScalaDriverLocal$$anonfun$repl$1.apply(ScalaDriverLocal.scala:189)
	at com.databricks.backend.daemon.driver.DriverLocal$TrapExitInternal$.trapExit(DriverLocal.scala:587)
	at com.databricks.backend.daemon.driver.DriverLocal$TrapExit$.apply(DriverLocal.scala:542)
	at com.databricks.backend.daemon.driver.ScalaDriverLocal.repl(ScalaDriverLocal.scala:189)
	at com.databricks.backend.daemon.driver.DriverLocal$$anonfun$execute$7.apply(DriverLocal.scala:324)
	at com.databricks.backend.daemon.driver.DriverLocal$$anonfun$execute$7.apply(DriverLocal.scala:304)
	at com.databricks.logging.UsageLogging$$anonfun$withAttributionContext$1.apply(UsageLogging.scala:235)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at com.databricks.logging.UsageLogging$class.withAttributionContext(UsageLogging.scala:230)
	at com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:45)
	at com.databricks.logging.UsageLogging$class.withAttributionTags(UsageLogging.scala:268)
	at com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:45)
	at com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:304)
	at com.databricks.backend.daemon.driver.DriverWrapper$$anonfun$tryExecutingCommand$2.apply(DriverWrapper.scala:589)
	at com.databricks.backend.daemon.driver.DriverWrapper$$anonfun$tryExecutingCommand$2.apply(DriverWrapper.scala:589)
	at scala.util.Try$.apply(Try.scala:192)
	at com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:584)
	at com.databricks.backend.daemon.driver.DriverWrapper.getCommandOutputAndError(DriverWrapper.scala:475)
	at com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:542)
	at com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:381)
	at com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:328)
	at com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:215)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.catalyst.analysis.NoSuchTableException: Table or view 'bd_output_1000255' not found in database 'default';
	at org.apache.spark.sql.hive.client.HiveClient$$anonfun$getTable$1.apply(HiveClient.scala:81)
	at org.apache.spark.sql.hive.client.HiveClient$$anonfun$getTable$1.apply(HiveClient.scala:81)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.hive.client.HiveClient$class.getTable(HiveClient.scala:81)
	at org.apache.spark.sql.hive.client.HiveClientImpl.getTable(HiveClientImpl.scala:84)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$getRawTable$1.apply(HiveExternalCatalog.scala:176)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$getRawTable$1.apply(HiveExternalCatalog.scala:176)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$withClient$1$$anonfun$apply$1.apply(HiveExternalCatalog.scala:141)
	at org.apache.spark.sql.hive.HiveExternalCatalog.org$apache$spark$sql$hive$HiveExternalCatalog$$maybeSynchronized(HiveExternalCatalog.scala:110)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$withClient$1.apply(HiveExternalCatalog.scala:139)
	at com.databricks.backend.daemon.driver.ProgressReporter$.withStatusCode(ProgressReporter.scala:345)
	at com.databricks.backend.daemon.driver.ProgressReporter$.withStatusCode(ProgressReporter.scala:331)
	at com.databricks.spark.util.SparkDatabricksProgressReporter$.withStatusCode(ProgressReporter.scala:34)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:137)
	at org.apache.spark.sql.hive.HiveExternalCatalog.getRawTable(HiveExternalCatalog.scala:175)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$getTable$1.apply(HiveExternalCatalog.scala:763)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$getTable$1.apply(HiveExternalCatalog.scala:763)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$withClient$1$$anonfun$apply$1.apply(HiveExternalCatalog.scala:141)
	at org.apache.spark.sql.hive.HiveExternalCatalog.org$apache$spark$sql$hive$HiveExternalCatalog$$maybeSynchronized(HiveExternalCatalog.scala:110)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$withClient$1.apply(HiveExternalCatalog.scala:139)
	at com.databricks.backend.daemon.driver.ProgressReporter$.withStatusCode(ProgressReporter.scala:345)
	at com.databricks.backend.daemon.driver.ProgressReporter$.withStatusCode(ProgressReporter.scala:331)
	at com.databricks.spark.util.SparkDatabricksProgressReporter$.withStatusCode(ProgressReporter.scala:34)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:137)
	at org.apache.spark.sql.hive.HiveExternalCatalog.getTable(HiveExternalCatalog.scala:762)
	at org.apache.spark.sql.catalyst.catalog.ExternalCatalogWithListener.getTable(ExternalCatalogWithListener.scala:143)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.lookupRelation(SessionCatalog.scala:736)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableFromCatalog$1.apply(Analyzer.scala:747)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableFromCatalog$1.apply(Analyzer.scala:747)
	at scala.collection.mutable.MapLike$class.getOrElseUpdate(MapLike.scala:194)
	at scala.collection.mutable.AbstractMap.getOrElseUpdate(Map.scala:80)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableFromCatalog(Analyzer.scala:746)
	... 108 more
19/10/17 21:09:41 INFO ProgressReporter$: Removed result fetcher for 4521178943219552436_6721938277465988443_job-68230-run-1-action-72418
19/10/17 21:09:43 INFO DriverCorral$: Cleaning the wrapper ReplId-3ebe7-44b6c-93d0b-4 (currently in status Idle(ReplId-3ebe7-44b6c-93d0b-4))
19/10/17 21:09:43 INFO DriverCorral$: sending shutdown signal for REPL ReplId-3ebe7-44b6c-93d0b-4
19/10/17 21:09:43 INFO DriverCorral$: sending the interrupt signal for REPL ReplId-3ebe7-44b6c-93d0b-4
19/10/17 21:09:43 INFO DriverCorral$: waiting for localThread to stop for REPL ReplId-3ebe7-44b6c-93d0b-4
19/10/17 21:09:43 INFO DriverCorral$: ReplId-3ebe7-44b6c-93d0b-4 successfully discarded
19/10/17 21:09:45 ERROR TaskSchedulerImpl: Lost executor 0 on 10.139.64.32: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
19/10/17 21:09:45 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20191017210748-0000/0 is now LOST (worker lost)
19/10/17 21:09:45 INFO StandaloneSchedulerBackend: Executor app-20191017210748-0000/0 removed: worker lost
19/10/17 21:09:45 INFO StandaloneAppClient$ClientEndpoint: Master removed worker worker-20191017210749-10.139.64.32-35989: 10.139.64.32:35989 got disassociated
19/10/17 21:09:45 INFO DAGScheduler: Executor lost: 0 (epoch 0)
19/10/17 21:09:45 INFO StandaloneSchedulerBackend: Worker worker-20191017210749-10.139.64.32-35989 removed: 10.139.64.32:35989 got disassociated
19/10/17 21:09:45 INFO BlockManagerMasterEndpoint: Trying to remove executor 0 from BlockManagerMaster.
19/10/17 21:09:45 INFO BlockManagerMasterEndpoint: Removing block manager BlockManagerId(0, 10.139.64.32, 33807, None)
19/10/17 21:09:45 INFO BlockManagerMasterEndpoint: Trying to remove executor 0 from BlockManagerMaster.
19/10/17 21:09:45 INFO BlockManagerMaster: Removed 0 successfully in removeExecutor
19/10/17 21:09:45 INFO BlockManagerMaster: Removal of executor 0 requested
19/10/17 21:09:45 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asked to remove non-existent executor 0
19/10/17 21:09:45 INFO TaskSchedulerImpl: Handle removed worker worker-20191017210749-10.139.64.32-35989: 10.139.64.32:35989 got disassociated
19/10/17 21:09:45 INFO DAGScheduler: Shuffle files lost for worker worker-20191017210749-10.139.64.32-35989 on host 10.139.64.32
