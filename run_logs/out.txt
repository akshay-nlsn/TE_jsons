19/10/14 21:52:42 INFO StaticConf$: DB_HOME: /databricks
19/10/14 21:52:42 INFO DriverDaemon$: ========== driver starting up ==========
19/10/14 21:52:42 INFO DriverDaemon$: Java: Oracle Corporation 1.8.0_191
19/10/14 21:52:42 INFO DriverDaemon$: OS: Linux/amd64 4.15.0-1050-azure
19/10/14 21:52:42 INFO DriverDaemon$: CWD: /databricks/driver
19/10/14 21:52:42 INFO DriverDaemon$: Mem: Max: 97.3G loaded GCs: PS Scavenge, PS MarkSweep
19/10/14 21:52:42 INFO DriverDaemon$: Logging multibyte characters: âœ“
19/10/14 21:52:42 INFO DriverDaemon$: 'publicFile' appender in root logger: class com.databricks.logging.RedactionRollingFileAppender
19/10/14 21:52:42 INFO DriverDaemon$: 'org.apache.log4j.Appender' appender in root logger: class com.codahale.metrics.log4j.InstrumentedAppender
19/10/14 21:52:42 INFO DriverDaemon$: 'null' appender in root logger: class com.databricks.logging.RequestTracker
19/10/14 21:52:42 INFO DriverDaemon$: == Modules:
19/10/14 21:52:43 INFO DriverDaemon$: Starting prometheus metrics log export timer
19/10/14 21:52:43 INFO DriverDaemon$: Universe Git Hash: 7201322412393895429516835306b6ef047d0688
19/10/14 21:52:43 INFO DriverDaemon$: Spark Git Hash: a937effdb77c94e1293f1fa405f29834568a066a
19/10/14 21:52:43 WARN RunHelpers$: Missing tag isolation client: java.util.NoSuchElementException: key not found: TagDefinition(clientType,The client type for a request, used for isolating resources for the request.)
19/10/14 21:52:43 INFO DatabricksILoop$: Creating throwaway interpreter
19/10/14 21:52:43 INFO SparkConfUtils$: Customize spark config according to file /tmp/custom-spark.conf
19/10/14 21:52:43 INFO SparkConfUtils$: new spark config: spark.sql.warehouse.dir -> adl://connectedsystemsuatdl.azuredatalakestore.net/user/hive/warehouse
19/10/14 21:52:43 INFO SparkConfUtils$: new spark config: spark.hadoop.javax.jdo.option.ConnectionPassword -> 560I890@fd80709
19/10/14 21:52:43 INFO SparkConfUtils$: new spark config: spark.hadoop.dfs.adls.oauth2.credential -> viUHaEfNg8cHQRJSRNMzpIv1T6goiUg+c9R/XSbYBFA=
19/10/14 21:52:43 INFO SparkConfUtils$: new spark config: spark.hadoop.javax.jdo.option.ConnectionDriverName -> org.postgresql.Driver
19/10/14 21:52:43 INFO SparkConfUtils$: new spark config: spark.databricks.acl.dfAclsEnabled -> false
19/10/14 21:52:43 INFO SparkConfUtils$: new spark config: spark.driver.tempDirectory -> /local_disk0/tmp
19/10/14 21:52:43 INFO SparkConfUtils$: new spark config: spark.hadoop.datanucleus.connectionPool.maxIdle -> 1
19/10/14 21:52:43 INFO SparkConfUtils$: new spark config: spark.hadoop.dfs.adls.oauth2.client.id -> 003106c3-ff8c-4c62-a060-24b8b32ea512
19/10/14 21:52:43 INFO SparkConfUtils$: new spark config: spark.hadoop.dfs.adls.oauth2.refresh.url -> https://login.microsoftonline.com/6ac7a1f4-5fb1-4153-bb4f-12d2020a1f7d/oauth2/token
19/10/14 21:52:43 INFO SparkConfUtils$: new spark config: spark.executor.tempDirectory -> /local_disk0/tmp
19/10/14 21:52:43 INFO SparkConfUtils$: new spark config: spark.databricks.delta.preview.enabled -> true
19/10/14 21:52:43 INFO SparkConfUtils$: new spark config: spark.task.maxFailures -> 10
19/10/14 21:52:43 INFO SparkConfUtils$: new spark config: spark.sql.hive.metastore.jars -> maven
19/10/14 21:52:43 INFO SparkConfUtils$: new spark config: spark.databricks.aggressiveWindowDownS -> 300
19/10/14 21:52:43 INFO SparkConfUtils$: new spark config: spark.hadoop.javax.jdo.option.ConnectionUserName -> tedbuat
19/10/14 21:52:43 INFO SparkConfUtils$: new spark config: spark.databricks.sql.hive.msck.batchSize -> 200
19/10/14 21:52:43 INFO SparkConfUtils$: new spark config: spark.hadoop.javax.jdo.option.ConnectionURL -> jdbc:postgresql://uathmsdb.trans.enterprisecsp.net:5432/hive_adls
19/10/14 21:52:43 INFO SparkConfUtils$: new spark config: datanucleus.fixedDatastore -> false
19/10/14 21:52:43 INFO SparkConfUtils$: new spark config: spark.databricks.sql.hive.msck.parallelism -> 20
19/10/14 21:52:43 INFO SparkConfUtils$: new spark config: spark.hadoop.datanucleus.connectionPool.maxPoolSize -> 30
19/10/14 21:52:43 INFO SparkConfUtils$: new spark config: spark.hadoop.datanucleus.connectionPool.minPoolSize -> 15
19/10/14 21:52:43 INFO SparkConfUtils$: new spark config: spark.sql.parquet.filterPushdown -> false
19/10/14 21:52:43 INFO SparkConfUtils$: new spark config: spark.hadoop.dfs.adls.oauth2.access.token.provider.type -> ClientCredential
19/10/14 21:52:43 INFO SparkConfUtils$: new spark config: spark.sql.shuffle.partitions -> 2000
19/10/14 21:52:43 INFO SparkConfUtils$: new spark config: spark.driver.maxResultSize -> 0
19/10/14 21:52:43 INFO SparkConfUtils$: new spark config: fs.azure.account.key.teuatsa.blob.core.windows.net -> IsrxMnnzLeO9N44beU4eMw0LaiKJdi1hHmKZFUiPa8nFNKZAFyhH0Hv/UCoC4qhgcdq63zwTRGFhUKV9d4ZsBw==
19/10/14 21:52:43 INFO SparkConfUtils$: new spark config: datanucleus.autoCreateSchema -> true
19/10/14 21:52:43 INFO SparkConfUtils$: new spark config: spark.hadoop.fs.adl.impl.disable.cache -> false
19/10/14 21:52:43 INFO SparkConfUtils$: new spark config: spark.sql.hive.metastore.version -> 2.3.0
19/10/14 21:52:43 WARN MetastoreMonitor$: Uri scheme postgresql is not supported.
19/10/14 21:52:43 WARN MetastoreMonitor$: Unexpected partial metastore configuration (userOpt=Some(tedbuat), pw.isDefined=true, uriOpt=None)
19/10/14 21:52:43 INFO MetastoreMonitor$: Generic external metastore configurd (config=jdbc:postgresql://uathmsdb.trans.enterprisecsp.net:5432/hive_adls)
19/10/14 21:52:43 INFO MetastoreMonitor: Not monitoring ExternalGenericMetastore
19/10/14 21:52:43 INFO JettyClient$: Creating new HttpClient with SSLContextFactory=None,maxRequestHeaderSize=65536, namePrefix=Some(DriverDaemon), idleTimeout=2 hours
19/10/14 21:52:43 INFO DriverCorral: Creating the driver context
19/10/14 21:52:43 INFO DatabricksILoop$: Class Server Dir: /local_disk0/tmp/repl/spark-1156843031774725531-5aec7ff1-3b4f-4426-9d62-f0ce3984d79a
19/10/14 21:52:43 INFO SparkConfUtils$: Customize spark config according to file /tmp/custom-spark.conf
19/10/14 21:52:43 INFO SparkConfUtils$: new spark config: spark.sql.warehouse.dir -> adl://connectedsystemsuatdl.azuredatalakestore.net/user/hive/warehouse
19/10/14 21:52:43 INFO SparkConfUtils$: new spark config: spark.hadoop.javax.jdo.option.ConnectionPassword -> 560I890@fd80709
19/10/14 21:52:43 INFO SparkConfUtils$: new spark config: spark.hadoop.dfs.adls.oauth2.credential -> viUHaEfNg8cHQRJSRNMzpIv1T6goiUg+c9R/XSbYBFA=
19/10/14 21:52:43 INFO SparkConfUtils$: new spark config: spark.hadoop.javax.jdo.option.ConnectionDriverName -> org.postgresql.Driver
19/10/14 21:52:43 INFO SparkConfUtils$: new spark config: spark.databricks.acl.dfAclsEnabled -> false
19/10/14 21:52:43 INFO SparkConfUtils$: new spark config: spark.driver.tempDirectory -> /local_disk0/tmp
19/10/14 21:52:43 INFO SparkConfUtils$: new spark config: spark.hadoop.datanucleus.connectionPool.maxIdle -> 1
19/10/14 21:52:43 INFO SparkConfUtils$: new spark config: spark.hadoop.dfs.adls.oauth2.client.id -> 003106c3-ff8c-4c62-a060-24b8b32ea512
19/10/14 21:52:43 INFO SparkConfUtils$: new spark config: spark.hadoop.dfs.adls.oauth2.refresh.url -> https://login.microsoftonline.com/6ac7a1f4-5fb1-4153-bb4f-12d2020a1f7d/oauth2/token
19/10/14 21:52:43 INFO SparkConfUtils$: new spark config: spark.executor.tempDirectory -> /local_disk0/tmp
19/10/14 21:52:43 INFO SparkConfUtils$: new spark config: spark.databricks.delta.preview.enabled -> true
19/10/14 21:52:43 INFO SparkConfUtils$: new spark config: spark.task.maxFailures -> 10
19/10/14 21:52:43 INFO SparkConfUtils$: new spark config: spark.sql.hive.metastore.jars -> maven
19/10/14 21:52:43 INFO SparkConfUtils$: new spark config: spark.databricks.aggressiveWindowDownS -> 300
19/10/14 21:52:43 INFO SparkConfUtils$: new spark config: spark.hadoop.javax.jdo.option.ConnectionUserName -> tedbuat
19/10/14 21:52:43 INFO SparkConfUtils$: new spark config: spark.databricks.sql.hive.msck.batchSize -> 200
19/10/14 21:52:43 INFO SparkConfUtils$: new spark config: spark.hadoop.javax.jdo.option.ConnectionURL -> jdbc:postgresql://uathmsdb.trans.enterprisecsp.net:5432/hive_adls
19/10/14 21:52:43 INFO SparkConfUtils$: new spark config: datanucleus.fixedDatastore -> false
19/10/14 21:52:43 INFO SparkConfUtils$: new spark config: spark.databricks.sql.hive.msck.parallelism -> 20
19/10/14 21:52:43 INFO SparkConfUtils$: new spark config: spark.hadoop.datanucleus.connectionPool.maxPoolSize -> 30
19/10/14 21:52:43 INFO SparkConfUtils$: new spark config: spark.hadoop.datanucleus.connectionPool.minPoolSize -> 15
19/10/14 21:52:43 INFO SparkConfUtils$: new spark config: spark.sql.parquet.filterPushdown -> false
19/10/14 21:52:43 INFO SparkConfUtils$: new spark config: spark.hadoop.dfs.adls.oauth2.access.token.provider.type -> ClientCredential
19/10/14 21:52:43 INFO SparkConfUtils$: new spark config: spark.sql.shuffle.partitions -> 2000
19/10/14 21:52:43 INFO SparkConfUtils$: new spark config: spark.driver.maxResultSize -> 0
19/10/14 21:52:43 INFO SparkConfUtils$: new spark config: fs.azure.account.key.teuatsa.blob.core.windows.net -> IsrxMnnzLeO9N44beU4eMw0LaiKJdi1hHmKZFUiPa8nFNKZAFyhH0Hv/UCoC4qhgcdq63zwTRGFhUKV9d4ZsBw==
19/10/14 21:52:43 INFO SparkConfUtils$: new spark config: datanucleus.autoCreateSchema -> true
19/10/14 21:52:43 INFO SparkConfUtils$: new spark config: spark.hadoop.fs.adl.impl.disable.cache -> false
19/10/14 21:52:43 INFO SparkConfUtils$: new spark config: spark.sql.hive.metastore.version -> 2.3.0
19/10/14 21:52:43 WARN SparkConf: The configuration key 'spark.scheduler.listenerbus.eventqueue.size' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.scheduler.listenerbus.eventqueue.capacity' instead.
19/10/14 21:52:43 WARN SparkConf: The configuration key 'spark.akka.frameSize' has been deprecated as of Spark 1.6 and may be removed in the future. Please use the new key 'spark.rpc.message.maxSize' instead.
19/10/14 21:52:44 INFO SparkContext: Running Spark version 2.4.0
19/10/14 21:52:44 WARN SparkConf: Detected deprecated memory fraction settings: [spark.storage.memoryFraction, spark.shuffle.memoryFraction]. As of Spark 1.6, execution and storage memory management are unified. All memory fractions used in the old model are now deprecated and no longer read. If you wish to use the old memory management, you may explicitly enable `spark.memory.useLegacyMode` (not recommended).
19/10/14 21:52:44 INFO SparkContext: Submitted application: Databricks Shell
19/10/14 21:52:44 INFO SparkContext: Spark configuration:
datanucleus.autoCreateSchema=true
datanucleus.fixedDatastore=false
eventLog.rolloverIntervalSeconds=3600
fs.azure.account.key.teuatsa.blob.core.windows.net=IsrxMnnzLeO9N44beU4eMw0LaiKJdi1hHmKZFUiPa8nFNKZAFyhH0Hv/UCoC4qhgcdq63zwTRGFhUKV9d4ZsBw==
spark.akka.frameSize=256
spark.app.name=Databricks Shell
spark.cleaner.referenceTracking.blocking=false
spark.databricks.acl.client=com.databricks.spark.sql.acl.client.SparkSqlAclClient
spark.databricks.acl.dfAclsEnabled=false
spark.databricks.acl.provider=com.databricks.sql.acl.ReflectionBackedAclProvider
spark.databricks.aggressiveWindowDownS=300
spark.databricks.cloudProvider=Azure
spark.databricks.clusterSource=JOB
spark.databricks.clusterUsageTags.autoTerminationMinutes=0
spark.databricks.clusterUsageTags.clusterAggressiveAutoscalingWindowDownSeconds=300
spark.databricks.clusterUsageTags.clusterAllTags=[{"key":"Vendor","value":"Databricks"},{"key":"Creator","value":"basheer.ahamed@nielsen.com"},{"key":"ClusterName","value":"job-67412-run-1"},{"key":"ClusterId","value":"1014-214842-claw384"},{"key":"DatabricksEnvironment","value":"workerenv-5614106561397268"}]
spark.databricks.clusterUsageTags.clusterCreator=JobLauncher
spark.databricks.clusterUsageTags.clusterGeneration=0
spark.databricks.clusterUsageTags.clusterId=1014-214842-claw384
spark.databricks.clusterUsageTags.clusterLogDeliveryEnabled=true
spark.databricks.clusterUsageTags.clusterLogDestination=dbfs:/mnt/te_mount/uat_logs/
spark.databricks.clusterUsageTags.clusterMaxWorkers=5
spark.databricks.clusterUsageTags.clusterMetastoreAccessType=RDS_DIRECT
spark.databricks.clusterUsageTags.clusterMinWorkers=1
spark.databricks.clusterUsageTags.clusterName=job-67412-run-1
spark.databricks.clusterUsageTags.clusterNoDriverDaemon=false
spark.databricks.clusterUsageTags.clusterNodeType=Standard_E16s_v3
spark.databricks.clusterUsageTags.clusterNumSshKeys=0
spark.databricks.clusterUsageTags.clusterOwnerOrgId=5614106561397268
spark.databricks.clusterUsageTags.clusterOwnerUserId=*********(redacted)
spark.databricks.clusterUsageTags.clusterPinned=false
spark.databricks.clusterUsageTags.clusterPythonVersion=2
spark.databricks.clusterUsageTags.clusterResourceClass=default
spark.databricks.clusterUsageTags.clusterScalingType=autoscaling
spark.databricks.clusterUsageTags.clusterSku=STANDARD_SKU
spark.databricks.clusterUsageTags.clusterState=Pending
spark.databricks.clusterUsageTags.clusterStateMessage=Starting Spark
spark.databricks.clusterUsageTags.clusterTargetWorkers=1
spark.databricks.clusterUsageTags.clusterWorkers=1
spark.databricks.clusterUsageTags.containerType=LXC
spark.databricks.clusterUsageTags.driverContainerId=04aca46b85624594bbc16ccb68c7362f
spark.databricks.clusterUsageTags.driverContainerPrivateIp=10.139.64.38
spark.databricks.clusterUsageTags.driverInstanceId=318ec7f388e54930a867363d1ed58997
spark.databricks.clusterUsageTags.driverInstancePrivateIp=10.139.0.37
spark.databricks.clusterUsageTags.driverNodeType=Standard_E16s_v3
spark.databricks.clusterUsageTags.driverPublicDns=52.167.200.60
spark.databricks.clusterUsageTags.enableCredentialPassthrough=*********(redacted)
spark.databricks.clusterUsageTags.enableDfAcls=false
spark.databricks.clusterUsageTags.enableElasticDisk=true
spark.databricks.clusterUsageTags.enableJdbcAutoStart=true
spark.databricks.clusterUsageTags.enableJobsAutostart=true
spark.databricks.clusterUsageTags.enableSqlAclsOnly=false
spark.databricks.clusterUsageTags.isSingleUserCluster=*********(redacted)
spark.databricks.clusterUsageTags.numPerClusterInitScriptsV2=0
spark.databricks.clusterUsageTags.sparkVersion=5.2.x-scala2.11
spark.databricks.clusterUsageTags.userProvidedRemoteVolumeCount=*********(redacted)
spark.databricks.clusterUsageTags.userProvidedRemoteVolumeSizeGb=*********(redacted)
spark.databricks.clusterUsageTags.userProvidedRemoteVolumeType=*********(redacted)
spark.databricks.clusterUsageTags.workerEnvironmentId=workerenv-5614106561397268
spark.databricks.credential.redactor=*********(redacted)
spark.databricks.delta.logStore.crossCloud.fatal=true
spark.databricks.delta.multiClusterWrites.enabled=true
spark.databricks.delta.preview.enabled=true
spark.databricks.driverNodeTypeId=Standard_E16s_v3
spark.databricks.eventLog.dir=eventlogs
spark.databricks.io.directoryCommit.enableLogicalDelete=false
spark.databricks.overrideDefaultCommitProtocol=org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol
spark.databricks.passthrough.adls.tokenProviderClassName=*********(redacted)
spark.databricks.passthrough.s3a.tokenProviderClassName=*********(redacted)
spark.databricks.preemption.enabled=true
spark.databricks.r.cleanWorkspace=true
spark.databricks.redactor=com.databricks.spark.util.DatabricksSparkLogRedactorProxy
spark.databricks.session.share=false
spark.databricks.sparkContextId=1156843031774725531
spark.databricks.sql.hive.msck.batchSize=200
spark.databricks.sql.hive.msck.parallelism=20
spark.databricks.tahoe.logStore.aws.class=com.databricks.tahoe.store.MultiClusterLogStore
spark.databricks.tahoe.logStore.azure.class=com.databricks.tahoe.store.AzureLogStore
spark.databricks.tahoe.logStore.class=com.databricks.tahoe.store.DelegatingLogStore
spark.databricks.workerNodeTypeId=Standard_E16s_v3
spark.driver.allowMultipleContexts=false
spark.driver.maxResultSize=0
spark.driver.tempDirectory=/local_disk0/tmp
spark.eventLog.enabled=false
spark.executor.extraClassPath=*********(redacted)
spark.executor.extraJavaOptions=-Djava.io.tmpdir=/local_disk0/tmp -XX:ReservedCodeCacheSize=256m -XX:+UseCodeCacheFlushing -Djava.security.properties=/databricks/spark/dbconf/java/extra.security -XX:+PrintFlagsFinal -XX:+PrintGCDateStamps -verbose:gc -XX:+PrintGCDetails -Xss4m -Djavax.xml.datatype.DatatypeFactory=com.sun.org.apache.xerces.internal.jaxp.datatype.DatatypeFactoryImpl -Djavax.xml.parsers.DocumentBuilderFactory=com.sun.org.apache.xerces.internal.jaxp.DocumentBuilderFactoryImpl -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -Djavax.xml.validation.SchemaFactory:http://www.w3.org/2001/XMLSchema=com.sun.org.apache.xerces.internal.jaxp.validation.XMLSchemaFactory -Dorg.xml.sax.driver=com.sun.org.apache.xerces.internal.parsers.SAXParser -Dorg.w3c.dom.DOMImplementationSourceList=com.sun.org.apache.xerces.internal.dom.DOMXSImplementationSourceImpl -Ddatabricks.serviceName=spark-executor-1
spark.executor.memory=97871m
spark.executor.tempDirectory=/local_disk0/tmp
spark.extraListeners=com.databricks.backend.daemon.driver.DBCEventLoggingListener
spark.files.fetchFailure.unRegisterOutputOnHost=true
spark.files.overwrite=true
spark.files.useFetchCache=false
spark.hadoop.databricks.dbfs.client.version=v2
spark.hadoop.databricks.s3commit.client.sslTrustAll=false
spark.hadoop.datanucleus.connectionPool.maxIdle=1
spark.hadoop.datanucleus.connectionPool.maxPoolSize=30
spark.hadoop.datanucleus.connectionPool.minPoolSize=15
spark.hadoop.dfs.adls.oauth2.access.token.provider.type=*********(redacted)
spark.hadoop.dfs.adls.oauth2.client.id=*********(redacted)
spark.hadoop.dfs.adls.oauth2.credential=*********(redacted)
spark.hadoop.dfs.adls.oauth2.refresh.url=*********(redacted)
spark.hadoop.fs.abfs.impl=shaded.databricks.v20180920_b33d810.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem
spark.hadoop.fs.abfs.impl.disable.cache=true
spark.hadoop.fs.abfss.impl=shaded.databricks.v20180920_b33d810.org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem
spark.hadoop.fs.abfss.impl.disable.cache=true
spark.hadoop.fs.adl.impl=com.databricks.adl.AdlFileSystem
spark.hadoop.fs.adl.impl.disable.cache=false
spark.hadoop.fs.azure.skip.metrics=true
spark.hadoop.fs.s3.impl=com.databricks.s3a.S3AFileSystem
spark.hadoop.fs.s3a.connection.maximum=200
spark.hadoop.fs.s3a.fast.upload=true
spark.hadoop.fs.s3a.fast.upload.default=true
spark.hadoop.fs.s3a.impl=com.databricks.s3a.S3AFileSystem
spark.hadoop.fs.s3a.multipart.size=10485760
spark.hadoop.fs.s3a.multipart.threshold=104857600
spark.hadoop.fs.s3a.threads.max=136
spark.hadoop.fs.s3n.impl=com.databricks.s3a.S3AFileSystem
spark.hadoop.fs.wasb.impl=shaded.databricks.org.apache.hadoop.fs.azure.NativeAzureFileSystem
spark.hadoop.fs.wasb.impl.disable.cache=true
spark.hadoop.fs.wasbs.impl=shaded.databricks.org.apache.hadoop.fs.azure.NativeAzureFileSystem
spark.hadoop.fs.wasbs.impl.disable.cache=true
spark.hadoop.hive.server2.idle.operation.timeout=7200000
spark.hadoop.hive.server2.idle.session.timeout=900000
spark.hadoop.hive.server2.keystore.password=*********(redacted)
spark.hadoop.hive.server2.keystore.path=/databricks/keys/jetty-ssl-driver-keystore.jks
spark.hadoop.hive.server2.session.check.interval=60000
spark.hadoop.hive.server2.thrift.http.port=10000
spark.hadoop.hive.server2.transport.mode=http
spark.hadoop.hive.server2.use.SSL=true
spark.hadoop.hive.warehouse.subdir.inherit.perms=false
spark.hadoop.javax.jdo.option.ConnectionDriverName=org.postgresql.Driver
spark.hadoop.javax.jdo.option.ConnectionPassword=*********(redacted)
spark.hadoop.javax.jdo.option.ConnectionURL=*********(redacted)
spark.hadoop.javax.jdo.option.ConnectionUserName=*********(redacted)
spark.hadoop.mapred.output.committer.class=com.databricks.backend.daemon.data.client.DirectOutputCommitter
spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version=2
spark.hadoop.parquet.memory.pool.ratio=0.5
spark.hadoop.spark.sql.parquet.output.committer.class=org.apache.spark.sql.parquet.DirectParquetOutputCommitter
spark.hadoop.spark.sql.sources.outputCommitterClass=com.databricks.backend.daemon.data.client.MapReduceDirectOutputCommitter
spark.hadoop.spark.thriftserver.customHeadersToProperties=*********(redacted)
spark.home=/databricks/spark
spark.logConf=true
spark.master=spark://10.139.64.38:7077
spark.metrics.conf=/databricks/spark/conf/metrics.properties
spark.r.backendConnectionTimeout=604800
spark.r.numRBackendThreads=1
spark.rdd.compress=true
spark.repl.class.outputDir=/local_disk0/tmp/repl/spark-1156843031774725531-5aec7ff1-3b4f-4426-9d62-f0ce3984d79a
spark.scheduler.listenerbus.eventqueue.size=20000
spark.scheduler.mode=FAIR
spark.serializer.objectStreamReset=100
spark.shuffle.manager=SORT
spark.shuffle.memoryFraction=0.2
spark.shuffle.reduceLocality.enabled=false
spark.shuffle.service.enabled=true
spark.shuffle.service.port=4048
spark.sparkr.use.daemon=false
spark.speculation=false
spark.speculation.multiplier=3
spark.speculation.quantile=0.9
spark.sql.allowMultipleContexts=false
spark.sql.hive.convertCTAS=true
spark.sql.hive.convertMetastoreParquet=true
spark.sql.hive.metastore.jars=maven
spark.sql.hive.metastore.sharedPrefixes=org.mariadb.jdbc,com.mysql.jdbc,org.postgresql,com.microsoft.sqlserver,microsoft.sql.DateTimeOffset,microsoft.sql.Types,com.databricks,com.codahale,com.fasterxml.jackson,shaded.databricks
spark.sql.hive.metastore.version=2.3.0
spark.sql.parquet.cacheMetadata=true
spark.sql.parquet.compression.codec=snappy
spark.sql.parquet.filterPushdown=false
spark.sql.shuffle.partitions=2000
spark.sql.sources.commitProtocolClass=com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol
spark.sql.streaming.checkpointFileManagerClass=com.databricks.spark.sql.streaming.DatabricksCheckpointFileManager
spark.sql.ui.retainedExecutions=100
spark.sql.warehouse.dir=*********(redacted)
spark.storage.blockManagerTimeoutIntervalMs=300000
spark.storage.memoryFraction=0.5
spark.streaming.driver.writeAheadLog.allowBatching=true
spark.streaming.driver.writeAheadLog.closeFileAfterWrite=true
spark.task.maxFailures=10
spark.task.reaper.enabled=true
spark.task.reaper.killTimeout=60s
spark.ui.port=46995
spark.worker.cleanup.enabled=false
19/10/14 21:52:44 INFO SecurityManager: Changing view acls to: root
19/10/14 21:52:44 INFO SecurityManager: Changing modify acls to: root
19/10/14 21:52:44 INFO SecurityManager: Changing view acls groups to: 
19/10/14 21:52:44 INFO SecurityManager: Changing modify acls groups to: 
19/10/14 21:52:44 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
19/10/14 21:52:44 INFO Utils: Successfully started service 'sparkDriver' on port 42579.
19/10/14 21:52:44 INFO SparkEnv: Registering MapOutputTracker
19/10/14 21:52:44 INFO SparkEnv: Registering BlockManagerMaster
19/10/14 21:52:44 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
19/10/14 21:52:44 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
19/10/14 21:52:44 INFO DiskBlockManager: Created local directory at /local_disk0/blockmgr-37c12db9-26ca-4212-a24f-c5d0eee37ac7
19/10/14 21:52:44 INFO MemoryStore: MemoryStore started with capacity 54.2 GB
19/10/14 21:52:44 INFO SparkEnv: Registering OutputCommitCoordinator
19/10/14 21:52:44 WARN MetricsSystem: Using default name SparkStatusTracker for source because neither spark.metrics.namespace nor spark.app.id is set.
19/10/14 21:52:45 INFO log: Logging initialized @4598ms
19/10/14 21:52:45 INFO Server: jetty-9.3.20.v20170531
19/10/14 21:52:45 INFO Server: Started @4703ms
19/10/14 21:52:45 INFO AbstractConnector: Started ServerConnector@7343922c{HTTP/1.1,[http/1.1]}{10.139.64.38:46995}
19/10/14 21:52:45 INFO Utils: Successfully started service 'SparkUI' on port 46995.
19/10/14 21:52:45 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@6056232d{/jobs,null,AVAILABLE,@Spark}
19/10/14 21:52:45 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@46e64760{/jobs/json,null,AVAILABLE,@Spark}
19/10/14 21:52:45 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@3e5fd2b1{/jobs/job,null,AVAILABLE,@Spark}
19/10/14 21:52:45 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@5b275811{/jobs/job/json,null,AVAILABLE,@Spark}
19/10/14 21:52:45 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@2f0ed952{/stages,null,AVAILABLE,@Spark}
19/10/14 21:52:45 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@6f044c58{/stages/json,null,AVAILABLE,@Spark}
19/10/14 21:52:45 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@6d074b14{/stages/stage,null,AVAILABLE,@Spark}
19/10/14 21:52:45 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@18371d89{/stages/stage/json,null,AVAILABLE,@Spark}
19/10/14 21:52:45 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@4f3faa70{/stages/pool,null,AVAILABLE,@Spark}
19/10/14 21:52:45 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@4832f03b{/stages/pool/json,null,AVAILABLE,@Spark}
19/10/14 21:52:45 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@7af3874e{/storage,null,AVAILABLE,@Spark}
19/10/14 21:52:45 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@5a8816cc{/storage/json,null,AVAILABLE,@Spark}
19/10/14 21:52:45 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@68565bc7{/storage/rdd,null,AVAILABLE,@Spark}
19/10/14 21:52:45 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@37baddde{/storage/rdd/json,null,AVAILABLE,@Spark}
19/10/14 21:52:45 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@5115f590{/environment,null,AVAILABLE,@Spark}
19/10/14 21:52:45 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@4b31a708{/environment/json,null,AVAILABLE,@Spark}
19/10/14 21:52:45 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@11e355ca{/executors,null,AVAILABLE,@Spark}
19/10/14 21:52:45 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@70d63e05{/executors/json,null,AVAILABLE,@Spark}
19/10/14 21:52:45 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@e11ecfa{/executors/threadDump,null,AVAILABLE,@Spark}
19/10/14 21:52:45 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@485e13d7{/executors/threadDump/json,null,AVAILABLE,@Spark}
19/10/14 21:52:45 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@43984213{/executors/heapHistogram,null,AVAILABLE,@Spark}
19/10/14 21:52:45 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@2ff7a73{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
19/10/14 21:52:45 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@38830ea{/static,null,AVAILABLE,@Spark}
19/10/14 21:52:45 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@2f4ba1ae{/,null,AVAILABLE,@Spark}
19/10/14 21:52:45 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@1391af3b{/api,null,AVAILABLE,@Spark}
19/10/14 21:52:45 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@1b6924cb{/jobs/job/kill,null,AVAILABLE,@Spark}
19/10/14 21:52:45 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@3a8d467e{/stages/stage/kill,null,AVAILABLE,@Spark}
19/10/14 21:52:45 INFO SparkUI: Bound SparkUI to 10.139.64.38, and started at http://10.139.64.38:46995
19/10/14 21:52:45 WARN FairSchedulableBuilder: Fair Scheduler configuration file not found so jobs will be scheduled in FIFO order. To use fair scheduling, configure pools in fairscheduler.xml or set spark.scheduler.allocation.file to a file that contains the configuration.
19/10/14 21:52:45 INFO FairSchedulableBuilder: Created default pool: default, schedulingMode: FIFO, minShare: 0, weight: 1
19/10/14 21:52:45 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://10.139.64.38:7077...
19/10/14 21:52:45 INFO TransportClientFactory: Successfully created connection to /10.139.64.38:7077 after 44 ms (0 ms spent in bootstraps)
19/10/14 21:52:45 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20191014215245-0000
19/10/14 21:52:45 INFO TaskSchedulerImpl: Task preemption enabled.
19/10/14 21:52:45 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20191014215245-0000/0 on worker-20191014215244-10.139.64.42-37163 (10.139.64.42:37163) with 16 core(s)
19/10/14 21:52:45 INFO StandaloneSchedulerBackend: Granted executor ID app-20191014215245-0000/0 on hostPort 10.139.64.42:37163 with 16 core(s), 95.6 GB RAM
19/10/14 21:52:45 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42249.
19/10/14 21:52:45 INFO NettyBlockTransferService: Server created on 10.139.64.38:42249
19/10/14 21:52:45 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
19/10/14 21:52:45 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.139.64.38, 42249, None)
19/10/14 21:52:45 INFO BlockManagerMasterEndpoint: Registering block manager 10.139.64.38:42249 with 54.2 GB RAM, BlockManagerId(driver, 10.139.64.38, 42249, None)
19/10/14 21:52:45 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.139.64.38, 42249, None)
19/10/14 21:52:45 INFO BlockManager: external shuffle service port = 4048
19/10/14 21:52:45 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.139.64.38, 42249, None)
19/10/14 21:52:46 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20191014215245-0000/0 is now RUNNING
19/10/14 21:52:46 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@294f9d50{/metrics/json,null,AVAILABLE,@Spark}
19/10/14 21:52:46 INFO DBCEventLoggingListener: Initializing DBCEventLoggingListener
19/10/14 21:52:46 INFO DBCEventLoggingListener: Logging events to eventlogs/1156843031774725531/eventlog
19/10/14 21:52:46 INFO SparkContext: Registered listener com.databricks.backend.daemon.driver.DBCEventLoggingListener
19/10/14 21:52:46 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
19/10/14 21:52:46 INFO SparkContext: Loading Spark Service RPC Server
19/10/14 21:52:46 INFO SparkServiceRPCServer: Spark Service RPC Server is disabled.
19/10/14 21:52:46 INFO DatabricksILoop$: Successfully registered spark metrics in Prometheus registry
19/10/14 21:52:46 INFO DatabricksILoop$: Successfully initialized SparkContext
19/10/14 21:52:46 INFO SharedState: Scheduler stats enabled.
19/10/14 21:52:46 INFO SharedState: loading hive config file: file:/databricks/hive/conf/hive-site.xml
19/10/14 21:52:46 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('adl://connectedsystemsuatdl.azuredatalakestore.net/user/hive/warehouse').
19/10/14 21:52:46 INFO SharedState: Warehouse path is 'adl://connectedsystemsuatdl.azuredatalakestore.net/user/hive/warehouse'.
19/10/14 21:52:46 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@606a1bc4{/SQL,null,AVAILABLE,@Spark}
19/10/14 21:52:46 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@6a15b73{/SQL/json,null,AVAILABLE,@Spark}
19/10/14 21:52:46 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@29896529{/SQL/execution,null,AVAILABLE,@Spark}
19/10/14 21:52:46 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@e29f4f6{/SQL/execution/json,null,AVAILABLE,@Spark}
19/10/14 21:52:46 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@4a1a256d{/static/sql,null,AVAILABLE,@Spark}
19/10/14 21:52:46 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@c1050f2{/storage/iocache,null,AVAILABLE,@Spark}
19/10/14 21:52:46 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@67bb4dcd{/storage/iocache/json,null,AVAILABLE,@Spark}
19/10/14 21:52:46 INFO LogStore: LogStore class: class com.databricks.tahoe.store.DelegatingLogStore
19/10/14 21:52:47 INFO DatabricksILoop$: Finished creating throwaway interpreter
19/10/14 21:52:47 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
19/10/14 21:52:48 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.139.64.42:52224) with ID 0
19/10/14 21:52:48 INFO BlockManagerMasterEndpoint: Registering block manager 10.139.64.42:35707 with 50.8 GB RAM, BlockManagerId(0, 10.139.64.42, 35707, None)
19/10/14 21:52:48 INFO HiveUtils: Initializing execution hive, version 1.2.1
19/10/14 21:52:49 INFO HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
19/10/14 21:52:49 INFO ObjectStore: ObjectStore, initialize called
19/10/14 21:52:49 INFO Persistence: Property spark.hadoop.datanucleus.connectionPool.maxIdle unknown - will be ignored
19/10/14 21:52:49 INFO Persistence: Property spark.hadoop.datanucleus.connectionPool.maxPoolSize unknown - will be ignored
19/10/14 21:52:49 INFO Persistence: Property spark.hadoop.datanucleus.connectionPool.minPoolSize unknown - will be ignored
19/10/14 21:52:49 INFO Persistence: Property spark.hadoop.javax.jdo.option.ConnectionDriverName unknown - will be ignored
19/10/14 21:52:49 INFO Persistence: Property spark.hadoop.javax.jdo.option.ConnectionURL unknown - will be ignored
19/10/14 21:52:49 INFO Persistence: Property spark.hadoop.javax.jdo.option.ConnectionUserName unknown - will be ignored
19/10/14 21:52:49 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
19/10/14 21:52:49 INFO Persistence: Property spark.hadoop.javax.jdo.option.ConnectionPassword unknown - will be ignored
19/10/14 21:52:49 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
19/10/14 21:52:49 INFO Persistence: Property datanucleus.schema.autoCreateAll unknown - will be ignored
19/10/14 21:52:51 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
19/10/14 21:52:52 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
19/10/14 21:52:52 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
19/10/14 21:52:53 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
19/10/14 21:52:53 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
19/10/14 21:52:53 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
19/10/14 21:52:53 INFO ObjectStore: Initialized ObjectStore
19/10/14 21:52:53 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0
19/10/14 21:52:53 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
19/10/14 21:52:54 INFO HiveMetaStore: Added admin role in metastore
19/10/14 21:52:54 INFO HiveMetaStore: Added public role in metastore
19/10/14 21:52:54 INFO HiveMetaStore: No user is added in admin role, since config is empty
19/10/14 21:52:54 INFO HiveMetaStore: 0: get_all_databases
19/10/14 21:52:54 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_all_databases	
19/10/14 21:52:54 INFO HiveMetaStore: 0: get_functions: db=default pat=*
19/10/14 21:52:54 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
19/10/14 21:52:54 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MResourceUri" is tagged as "embedded-only" so does not have its own datastore table.
19/10/14 21:52:54 INFO JettyClient$: Creating new HttpClient with SSLContextFactory=None,maxRequestHeaderSize=65536, namePrefix=Some(DBFSV2), idleTimeout=2 hours
19/10/14 21:52:55 WARN EC2MetadataUtils: Unable to retrieve the requested metadata.
19/10/14 21:52:55 INFO deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum
19/10/14 21:52:55 INFO DatabricksFileSystemV2Factory: Creating wasbs file system for wasbs://root@dbstorage5cunvudtvyhjm.blob.core.windows.net
19/10/14 21:52:55 INFO DBFS: Initialized DBFS with DBFSV2 as the delegate.
19/10/14 21:52:55 INFO SessionState: Created local directory: /local_disk0/tmp/root
19/10/14 21:52:55 INFO SessionState: Created local directory: /local_disk0/tmp/8f58c164-1d30-40c1-9dde-0fa5c624be18_resources
19/10/14 21:52:55 INFO SessionState: Created HDFS directory: /tmp/hive/root/8f58c164-1d30-40c1-9dde-0fa5c624be18
19/10/14 21:52:55 INFO SessionState: Created local directory: /local_disk0/tmp/root/8f58c164-1d30-40c1-9dde-0fa5c624be18
19/10/14 21:52:55 INFO SessionState: Created HDFS directory: /tmp/hive/root/8f58c164-1d30-40c1-9dde-0fa5c624be18/_tmp_space.db
19/10/14 21:52:55 INFO HiveClientImpl: Warehouse location for Hive client (version 1.2.2) is adl://connectedsystemsuatdl.azuredatalakestore.net/user/hive/warehouse
19/10/14 21:52:55 INFO SessionManager: Operation log root directory is created: /local_disk0/tmp/root/operation_logs
19/10/14 21:52:55 INFO SessionManager: HiveServer2: Background operation thread pool size: 100
19/10/14 21:52:55 INFO SessionManager: HiveServer2: Background operation thread wait queue size: 100
19/10/14 21:52:55 INFO SessionManager: HiveServer2: Background operation thread keepalive time: 10 seconds
19/10/14 21:52:55 INFO AbstractService: Service:OperationManager is inited.
19/10/14 21:52:55 INFO AbstractService: Service:SessionManager is inited.
19/10/14 21:52:55 INFO AbstractService: Service: CLIService is inited.
19/10/14 21:52:55 INFO AbstractService: Service:ThriftHttpCLIService is inited.
19/10/14 21:52:55 INFO AbstractService: Service: HiveServer2 is inited.
19/10/14 21:52:55 INFO AbstractService: Service:OperationManager is started.
19/10/14 21:52:55 INFO AbstractService: Service:SessionManager is started.
19/10/14 21:52:55 INFO AbstractService: Service:CLIService is started.
19/10/14 21:52:55 INFO ObjectStore: ObjectStore, initialize called
19/10/14 21:52:55 INFO Query: Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
19/10/14 21:52:55 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
19/10/14 21:52:55 INFO ObjectStore: Initialized ObjectStore
19/10/14 21:52:55 INFO HiveMetaStore: 0: get_databases: default
19/10/14 21:52:55 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_databases: default	
19/10/14 21:52:55 INFO HiveMetaStore: 0: Shutting down the object store...
19/10/14 21:52:55 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=Shutting down the object store...	
19/10/14 21:52:55 INFO HiveMetaStore: 0: Metastore shutdown complete.
19/10/14 21:52:55 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=Metastore shutdown complete.	
19/10/14 21:52:55 INFO AbstractService: Service:ThriftHttpCLIService is started.
19/10/14 21:52:55 INFO AbstractService: Service:HiveServer2 is started.
19/10/14 21:52:55 INFO ThriftCLIService: HTTP Server SSL: adding excluded protocols: [SSLv2, SSLv3]
19/10/14 21:52:55 INFO ThriftCLIService: HTTP Server SSL: SslContextFactory.getExcludeProtocols = [SSL, SSLv2, SSLv2Hello, SSLv3]
19/10/14 21:52:55 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@3829306d{/sqlserver,null,AVAILABLE,@Spark}
19/10/14 21:52:55 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@28effa3f{/sqlserver/json,null,AVAILABLE,@Spark}
19/10/14 21:52:55 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@b867869{/sqlserver/session,null,AVAILABLE,@Spark}
19/10/14 21:52:55 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@7b6b8cea{/sqlserver/session/json,null,AVAILABLE,@Spark}
19/10/14 21:52:55 INFO DriverDaemon: Starting driver daemon...
19/10/14 21:52:55 INFO SparkConfUtils$: Customize spark config according to file /tmp/custom-spark.conf
19/10/14 21:52:55 INFO SparkConfUtils$: new spark config: spark.sql.warehouse.dir -> adl://connectedsystemsuatdl.azuredatalakestore.net/user/hive/warehouse
19/10/14 21:52:55 INFO SparkConfUtils$: new spark config: spark.hadoop.javax.jdo.option.ConnectionPassword -> 560I890@fd80709
19/10/14 21:52:55 INFO SparkConfUtils$: new spark config: spark.hadoop.dfs.adls.oauth2.credential -> viUHaEfNg8cHQRJSRNMzpIv1T6goiUg+c9R/XSbYBFA=
19/10/14 21:52:55 INFO SparkConfUtils$: new spark config: spark.hadoop.javax.jdo.option.ConnectionDriverName -> org.postgresql.Driver
19/10/14 21:52:55 INFO SparkConfUtils$: new spark config: spark.databricks.acl.dfAclsEnabled -> false
19/10/14 21:52:55 INFO SparkConfUtils$: new spark config: spark.driver.tempDirectory -> /local_disk0/tmp
19/10/14 21:52:55 INFO SparkConfUtils$: new spark config: spark.hadoop.datanucleus.connectionPool.maxIdle -> 1
19/10/14 21:52:55 INFO SparkConfUtils$: new spark config: spark.hadoop.dfs.adls.oauth2.client.id -> 003106c3-ff8c-4c62-a060-24b8b32ea512
19/10/14 21:52:55 INFO SparkConfUtils$: new spark config: spark.hadoop.dfs.adls.oauth2.refresh.url -> https://login.microsoftonline.com/6ac7a1f4-5fb1-4153-bb4f-12d2020a1f7d/oauth2/token
19/10/14 21:52:55 INFO SparkConfUtils$: new spark config: spark.executor.tempDirectory -> /local_disk0/tmp
19/10/14 21:52:55 INFO SparkConfUtils$: new spark config: spark.databricks.delta.preview.enabled -> true
19/10/14 21:52:55 INFO SparkConfUtils$: new spark config: spark.task.maxFailures -> 10
19/10/14 21:52:55 INFO SparkConfUtils$: new spark config: spark.sql.hive.metastore.jars -> maven
19/10/14 21:52:55 INFO SparkConfUtils$: new spark config: spark.databricks.aggressiveWindowDownS -> 300
19/10/14 21:52:55 INFO SparkConfUtils$: new spark config: spark.hadoop.javax.jdo.option.ConnectionUserName -> tedbuat
19/10/14 21:52:55 INFO SparkConfUtils$: new spark config: spark.databricks.sql.hive.msck.batchSize -> 200
19/10/14 21:52:55 INFO SparkConfUtils$: new spark config: spark.hadoop.javax.jdo.option.ConnectionURL -> jdbc:postgresql://uathmsdb.trans.enterprisecsp.net:5432/hive_adls
19/10/14 21:52:55 INFO SparkConfUtils$: new spark config: datanucleus.fixedDatastore -> false
19/10/14 21:52:55 INFO SparkConfUtils$: new spark config: spark.databricks.sql.hive.msck.parallelism -> 20
19/10/14 21:52:55 INFO SparkConfUtils$: new spark config: spark.hadoop.datanucleus.connectionPool.maxPoolSize -> 30
19/10/14 21:52:55 INFO SparkConfUtils$: new spark config: spark.hadoop.datanucleus.connectionPool.minPoolSize -> 15
19/10/14 21:52:55 INFO SparkConfUtils$: new spark config: spark.sql.parquet.filterPushdown -> false
19/10/14 21:52:55 INFO SparkConfUtils$: new spark config: spark.hadoop.dfs.adls.oauth2.access.token.provider.type -> ClientCredential
19/10/14 21:52:55 INFO SparkConfUtils$: new spark config: spark.sql.shuffle.partitions -> 2000
19/10/14 21:52:55 INFO SparkConfUtils$: new spark config: spark.driver.maxResultSize -> 0
19/10/14 21:52:55 INFO SparkConfUtils$: new spark config: fs.azure.account.key.teuatsa.blob.core.windows.net -> IsrxMnnzLeO9N44beU4eMw0LaiKJdi1hHmKZFUiPa8nFNKZAFyhH0Hv/UCoC4qhgcdq63zwTRGFhUKV9d4ZsBw==
19/10/14 21:52:55 INFO SparkConfUtils$: new spark config: datanucleus.autoCreateSchema -> true
19/10/14 21:52:55 INFO SparkConfUtils$: new spark config: spark.hadoop.fs.adl.impl.disable.cache -> false
19/10/14 21:52:55 INFO SparkConfUtils$: new spark config: spark.sql.hive.metastore.version -> 2.3.0
19/10/14 21:52:55 WARN SparkConf: The configuration key 'spark.scheduler.listenerbus.eventqueue.size' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.scheduler.listenerbus.eventqueue.capacity' instead.
19/10/14 21:52:55 WARN SparkConf: The configuration key 'spark.akka.frameSize' has been deprecated as of Spark 1.6 and may be removed in the future. Please use the new key 'spark.rpc.message.maxSize' instead.
19/10/14 21:52:55 INFO DriverDaemon$$anon$1: Message out thread ready
19/10/14 21:52:55 INFO Server: jetty-9.3.20.v20170531
19/10/14 21:52:55 INFO AbstractConnector: Started ServerConnector@419ece7a{HTTP/1.1,[http/1.1]}{0.0.0.0:6061}
19/10/14 21:52:55 INFO Server: Started @15444ms
19/10/14 21:52:55 INFO DriverDaemon: Driver daemon started.
19/10/14 21:52:55 INFO Server: jetty-9.3.20.v20170531
19/10/14 21:52:55 WARN SecurityHandler: ServletContext@o.e.j.s.ServletContextHandler@4d706a66{/,null,STARTING} has uncovered http methods for path: /*
19/10/14 21:52:55 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@4d706a66{/,null,AVAILABLE}
19/10/14 21:52:55 INFO SslContextFactory: x509=X509@1a301644(1,h=[databrickscloud.com],w=[]) for SslContextFactory@7ad71245(file:///databricks/keys/jetty-ssl-driver-keystore.jks,null)
19/10/14 21:52:56 INFO AbstractConnector: Started ServerConnector@2c7716bd{SSL,[ssl, http/1.1]}{0.0.0.0:10000}
19/10/14 21:52:56 INFO Server: Started @15568ms
19/10/14 21:52:56 INFO ThriftCLIService: Started ThriftHttpCLIService in https mode on port 10000 path=/cliservice/* with 5...500 worker threads
19/10/14 21:52:56 INFO DriverCorral: Loading the root classloader
19/10/14 21:52:56 INFO DriverCorral: Starting repl ReplId-95e06-238ec-a52ed
19/10/14 21:52:56 INFO DriverCorral: Starting repl ReplId-116e6-f4539-58b05-9
19/10/14 21:52:56 INFO SQLDriverWrapper: setupRepl:ReplId-95e06-238ec-a52ed: finished to load
19/10/14 21:52:56 INFO DriverCorral: Starting repl ReplId-32766-8d0cb-913b0-d
19/10/14 21:52:56 INFO SQLDriverWrapper: setupRepl:ReplId-116e6-f4539-58b05-9: finished to load
19/10/14 21:52:56 INFO SQLDriverWrapper: setupRepl:ReplId-32766-8d0cb-913b0-d: finished to load
19/10/14 21:52:56 INFO DriverCorral: Starting repl ReplId-5a88c-fbdb3-bc21b-8
19/10/14 21:52:56 INFO SQLDriverWrapper: setupRepl:ReplId-5a88c-fbdb3-bc21b-8: finished to load
19/10/14 21:52:56 INFO DriverCorral: Starting repl ReplId-4a68b-d3b3f-e725e-3
19/10/14 21:52:56 INFO SQLDriverWrapper: setupRepl:ReplId-4a68b-d3b3f-e725e-3: finished to load
19/10/14 21:52:56 INFO DriverCorral: Starting repl ReplId-1a574-fa6e7-ede0b-6
19/10/14 21:52:56 INFO RDriverLocal: 1. RDriverLocal.7125e13b-1f95-4783-a416-22507be359ef: object created with for ReplId-1a574-fa6e7-ede0b-6.
19/10/14 21:52:56 INFO RDriverLocal: 2. RDriverLocal.7125e13b-1f95-4783-a416-22507be359ef: initializing ...
19/10/14 21:52:56 INFO RDriverLocal: 3. RDriverLocal.7125e13b-1f95-4783-a416-22507be359ef: started RBackend thread on port 34987
19/10/14 21:52:56 INFO RDriverLocal: 4. RDriverLocal.7125e13b-1f95-4783-a416-22507be359ef: waiting for SparkR to be installed ...
19/10/14 21:53:02 INFO RDriverLocal$: SparkR installation completed.
19/10/14 21:53:02 INFO RDriverLocal: 5. RDriverLocal.7125e13b-1f95-4783-a416-22507be359ef: launching R process ...
19/10/14 21:53:02 INFO RDriverLocal: 6. RDriverLocal.7125e13b-1f95-4783-a416-22507be359ef: cgroup isolation disabled, not placing R process in REPL cgroup.
19/10/14 21:53:02 INFO RDriverLocal: 7. RDriverLocal.7125e13b-1f95-4783-a416-22507be359ef: starting R process on port 26903 (attempt 1) ...
19/10/14 21:53:04 INFO RDriverLocal: 8. RDriverLocal.7125e13b-1f95-4783-a416-22507be359ef: R process started with RServe listening on port 26903.
19/10/14 21:53:04 INFO RDriverLocal: 9. RDriverLocal.7125e13b-1f95-4783-a416-22507be359ef: setting up BufferedStreamThread with bufferSize: 100.
19/10/14 21:53:05 INFO RDriverLocal: 10. RDriverLocal.7125e13b-1f95-4783-a416-22507be359ef: starting interpreter to talk to R process ...
19/10/14 21:53:05 INFO LibraryDownloadManager: Downloading a library that was not in the cache: UploadedLibrary(org/rogach/scallop_2.11-3.3.1.jar,dbfs:/FileStore/jars/maven/org/rogach/scallop_2.11-3.3.1.jar,scala)
19/10/14 21:53:05 INFO LibraryDownloadManager: Downloaded library org/rogach/scallop_2.11-3.3.1.jar as local file /local_disk0/tmp/addedFile9175644853842256239org_rogach_scallop_2_11_3_3_1-ecc25.jar
19/10/14 21:53:05 INFO SharedDriverContext: Adding libraries that haven't been added before: Set(/local_disk0/tmp/addedFile9175644853842256239org_rogach_scallop_2_11_3_3_1-ecc25.jar)
19/10/14 21:53:05 INFO SparkContext: Added file /local_disk0/tmp/addedFile9175644853842256239org_rogach_scallop_2_11_3_3_1-ecc25.jar at spark://10.139.64.38:42579/files/addedFile9175644853842256239org_rogach_scallop_2_11_3_3_1-ecc25.jar with timestamp 1571089985649
19/10/14 21:53:05 INFO Utils: Copying /local_disk0/tmp/addedFile9175644853842256239org_rogach_scallop_2_11_3_3_1-ecc25.jar to /local_disk0/spark-e806244c-29d6-435a-8c83-5f5624a96f00/userFiles-4ef8e035-1ebc-4db1-af54-2d3c9c23d05e/addedFile9175644853842256239org_rogach_scallop_2_11_3_3_1-ecc25.jar
19/10/14 21:53:05 WARN SparkContext: Using an existing SparkContext; some configuration may not take effect.
19/10/14 21:53:05 INFO SparkContext: Added JAR /local_disk0/tmp/addedFile9175644853842256239org_rogach_scallop_2_11_3_3_1-ecc25.jar at spark://10.139.64.38:42579/jars/addedFile9175644853842256239org_rogach_scallop_2_11_3_3_1-ecc25.jar with timestamp 1571089985667
19/10/14 21:53:05 INFO LibraryDownloadManager: Downloading a library that was not in the cache: UploadedLibrary(com/typesafe/config-1.3.1.jar,dbfs:/FileStore/jars/maven/com/typesafe/config-1.3.1.jar,scala)
19/10/14 21:53:05 INFO RDriverLocal: 11. RDriverLocal.7125e13b-1f95-4783-a416-22507be359ef: R interpretter is connected.
19/10/14 21:53:05 INFO RDriverWrapper: setupRepl:ReplId-1a574-fa6e7-ede0b-6: finished to load
19/10/14 21:53:05 INFO LibraryDownloadManager: Downloaded library com/typesafe/config-1.3.1.jar as local file /local_disk0/tmp/addedFile5971024327773765715com_typesafe_config_1_3_1-e3e42.jar
19/10/14 21:53:05 INFO SharedDriverContext: Adding libraries that haven't been added before: Set(/local_disk0/tmp/addedFile5971024327773765715com_typesafe_config_1_3_1-e3e42.jar)
19/10/14 21:53:05 INFO SparkContext: Added file /local_disk0/tmp/addedFile5971024327773765715com_typesafe_config_1_3_1-e3e42.jar at spark://10.139.64.38:42579/files/addedFile5971024327773765715com_typesafe_config_1_3_1-e3e42.jar with timestamp 1571089985895
19/10/14 21:53:05 INFO Utils: Copying /local_disk0/tmp/addedFile5971024327773765715com_typesafe_config_1_3_1-e3e42.jar to /local_disk0/spark-e806244c-29d6-435a-8c83-5f5624a96f00/userFiles-4ef8e035-1ebc-4db1-af54-2d3c9c23d05e/addedFile5971024327773765715com_typesafe_config_1_3_1-e3e42.jar
19/10/14 21:53:05 INFO SparkContext: Added JAR /local_disk0/tmp/addedFile5971024327773765715com_typesafe_config_1_3_1-e3e42.jar at spark://10.139.64.38:42579/jars/addedFile5971024327773765715com_typesafe_config_1_3_1-e3e42.jar with timestamp 1571089985905
19/10/14 21:53:05 INFO LibraryDownloadManager: Downloading a library that was not in the cache: UploadedLibrary(xml-apis/xml-apis-1.0.b2.jar,dbfs:/FileStore/jars/maven/xml-apis/xml-apis-1.0.b2.jar,scala)
19/10/14 21:53:06 INFO LibraryDownloadManager: Downloaded library xml-apis/xml-apis-1.0.b2.jar as local file /local_disk0/tmp/addedFile1209439005989882887xml_apis_xml_apis_1_0_b2-167e6.jar
19/10/14 21:53:06 INFO SharedDriverContext: Adding libraries that haven't been added before: Set(/local_disk0/tmp/addedFile1209439005989882887xml_apis_xml_apis_1_0_b2-167e6.jar)
19/10/14 21:53:06 INFO SparkContext: Added file /local_disk0/tmp/addedFile1209439005989882887xml_apis_xml_apis_1_0_b2-167e6.jar at spark://10.139.64.38:42579/files/addedFile1209439005989882887xml_apis_xml_apis_1_0_b2-167e6.jar with timestamp 1571089986017
19/10/14 21:53:06 INFO Utils: Copying /local_disk0/tmp/addedFile1209439005989882887xml_apis_xml_apis_1_0_b2-167e6.jar to /local_disk0/spark-e806244c-29d6-435a-8c83-5f5624a96f00/userFiles-4ef8e035-1ebc-4db1-af54-2d3c9c23d05e/addedFile1209439005989882887xml_apis_xml_apis_1_0_b2-167e6.jar
19/10/14 21:53:06 INFO SparkContext: Added JAR /local_disk0/tmp/addedFile1209439005989882887xml_apis_xml_apis_1_0_b2-167e6.jar at spark://10.139.64.38:42579/jars/addedFile1209439005989882887xml_apis_xml_apis_1_0_b2-167e6.jar with timestamp 1571089986025
19/10/14 21:53:06 INFO LibraryDownloadManager: Downloading a library that was not in the cache: UploadedLibrary(dom4j/dom4j-1.6.1.jar,dbfs:/FileStore/jars/maven/dom4j/dom4j-1.6.1.jar,scala)
19/10/14 21:53:06 INFO LibraryDownloadManager: Downloaded library dom4j/dom4j-1.6.1.jar as local file /local_disk0/tmp/addedFile7957955701981159152dom4j_dom4j_1_6_1-3b35e.jar
19/10/14 21:53:06 INFO SharedDriverContext: Adding libraries that haven't been added before: Set(/local_disk0/tmp/addedFile7957955701981159152dom4j_dom4j_1_6_1-3b35e.jar)
19/10/14 21:53:06 INFO SparkContext: Added file /local_disk0/tmp/addedFile7957955701981159152dom4j_dom4j_1_6_1-3b35e.jar at spark://10.139.64.38:42579/files/addedFile7957955701981159152dom4j_dom4j_1_6_1-3b35e.jar with timestamp 1571089986099
19/10/14 21:53:06 INFO Utils: Copying /local_disk0/tmp/addedFile7957955701981159152dom4j_dom4j_1_6_1-3b35e.jar to /local_disk0/spark-e806244c-29d6-435a-8c83-5f5624a96f00/userFiles-4ef8e035-1ebc-4db1-af54-2d3c9c23d05e/addedFile7957955701981159152dom4j_dom4j_1_6_1-3b35e.jar
19/10/14 21:53:06 INFO SparkContext: Added JAR /local_disk0/tmp/addedFile7957955701981159152dom4j_dom4j_1_6_1-3b35e.jar at spark://10.139.64.38:42579/jars/addedFile7957955701981159152dom4j_dom4j_1_6_1-3b35e.jar with timestamp 1571089986107
19/10/14 21:53:06 INFO LibraryDownloadManager: Downloading a library that was not in the cache: UploadedLibrary(dbfs:/FileStore/saak9001/pqm_core_models-1.0.jar,dbfs:/FileStore/saak9001/pqm_core_models-1.0.jar,scala)
19/10/14 21:53:06 INFO LibraryDownloadManager: Downloaded library dbfs:/FileStore/saak9001/pqm_core_models-1.0.jar as local file /local_disk0/tmp/addedFile8338063297904481498dbfs__FileStore_saak9001_pqm_core_models_1_0-b0ccd.jar
19/10/14 21:53:06 INFO SharedDriverContext: Adding libraries that haven't been added before: Set(/local_disk0/tmp/addedFile8338063297904481498dbfs__FileStore_saak9001_pqm_core_models_1_0-b0ccd.jar)
19/10/14 21:53:06 INFO SparkContext: Added file /local_disk0/tmp/addedFile8338063297904481498dbfs__FileStore_saak9001_pqm_core_models_1_0-b0ccd.jar at spark://10.139.64.38:42579/files/addedFile8338063297904481498dbfs__FileStore_saak9001_pqm_core_models_1_0-b0ccd.jar with timestamp 1571089986258
19/10/14 21:53:06 INFO Utils: Copying /local_disk0/tmp/addedFile8338063297904481498dbfs__FileStore_saak9001_pqm_core_models_1_0-b0ccd.jar to /local_disk0/spark-e806244c-29d6-435a-8c83-5f5624a96f00/userFiles-4ef8e035-1ebc-4db1-af54-2d3c9c23d05e/addedFile8338063297904481498dbfs__FileStore_saak9001_pqm_core_models_1_0-b0ccd.jar
19/10/14 21:53:06 INFO SparkContext: Added JAR /local_disk0/tmp/addedFile8338063297904481498dbfs__FileStore_saak9001_pqm_core_models_1_0-b0ccd.jar at spark://10.139.64.38:42579/jars/addedFile8338063297904481498dbfs__FileStore_saak9001_pqm_core_models_1_0-b0ccd.jar with timestamp 1571089986266
19/10/14 21:53:06 INFO LibraryDownloadManager: Downloading a library that was not in the cache: UploadedLibrary(io/spray/spray-json_2.11-1.3.5.jar,dbfs:/FileStore/jars/maven/io/spray/spray-json_2.11-1.3.5.jar,scala)
19/10/14 21:53:06 INFO LibraryDownloadManager: Downloaded library io/spray/spray-json_2.11-1.3.5.jar as local file /local_disk0/tmp/addedFile8178506233776720821io_spray_spray_json_2_11_1_3_5-d5f78.jar
19/10/14 21:53:06 INFO SharedDriverContext: Adding libraries that haven't been added before: Set(/local_disk0/tmp/addedFile8178506233776720821io_spray_spray_json_2_11_1_3_5-d5f78.jar)
19/10/14 21:53:06 INFO SparkContext: Added file /local_disk0/tmp/addedFile8178506233776720821io_spray_spray_json_2_11_1_3_5-d5f78.jar at spark://10.139.64.38:42579/files/addedFile8178506233776720821io_spray_spray_json_2_11_1_3_5-d5f78.jar with timestamp 1571089986344
19/10/14 21:53:06 INFO Utils: Copying /local_disk0/tmp/addedFile8178506233776720821io_spray_spray_json_2_11_1_3_5-d5f78.jar to /local_disk0/spark-e806244c-29d6-435a-8c83-5f5624a96f00/userFiles-4ef8e035-1ebc-4db1-af54-2d3c9c23d05e/addedFile8178506233776720821io_spray_spray_json_2_11_1_3_5-d5f78.jar
19/10/14 21:53:06 INFO SparkContext: Added JAR /local_disk0/tmp/addedFile8178506233776720821io_spray_spray_json_2_11_1_3_5-d5f78.jar at spark://10.139.64.38:42579/jars/addedFile8178506233776720821io_spray_spray_json_2_11_1_3_5-d5f78.jar with timestamp 1571089986352
19/10/14 21:53:06 INFO LibraryDownloadManager: Downloading a library that was not in the cache: UploadedLibrary(dbfs:/FileStore/saak9001/transengine-1.0-SNAPSHOT.jar,dbfs:/FileStore/saak9001/transengine-1.0-SNAPSHOT.jar,scala)
19/10/14 21:53:06 INFO LibraryDownloadManager: Downloaded library dbfs:/FileStore/saak9001/transengine-1.0-SNAPSHOT.jar as local file /local_disk0/tmp/addedFile5394604733753664878dbfs__FileStore_saak9001_transengine_1_0_SNAPSHOT-ab9b9.jar
19/10/14 21:53:06 INFO SharedDriverContext: Adding libraries that haven't been added before: Set(/local_disk0/tmp/addedFile5394604733753664878dbfs__FileStore_saak9001_transengine_1_0_SNAPSHOT-ab9b9.jar)
19/10/14 21:53:06 INFO SparkContext: Added file /local_disk0/tmp/addedFile5394604733753664878dbfs__FileStore_saak9001_transengine_1_0_SNAPSHOT-ab9b9.jar at spark://10.139.64.38:42579/files/addedFile5394604733753664878dbfs__FileStore_saak9001_transengine_1_0_SNAPSHOT-ab9b9.jar with timestamp 1571089986425
19/10/14 21:53:06 INFO Utils: Copying /local_disk0/tmp/addedFile5394604733753664878dbfs__FileStore_saak9001_transengine_1_0_SNAPSHOT-ab9b9.jar to /local_disk0/spark-e806244c-29d6-435a-8c83-5f5624a96f00/userFiles-4ef8e035-1ebc-4db1-af54-2d3c9c23d05e/addedFile5394604733753664878dbfs__FileStore_saak9001_transengine_1_0_SNAPSHOT-ab9b9.jar
19/10/14 21:53:06 INFO SparkContext: Added JAR /local_disk0/tmp/addedFile5394604733753664878dbfs__FileStore_saak9001_transengine_1_0_SNAPSHOT-ab9b9.jar at spark://10.139.64.38:42579/jars/addedFile5394604733753664878dbfs__FileStore_saak9001_transengine_1_0_SNAPSHOT-ab9b9.jar with timestamp 1571089986435
19/10/14 21:53:06 INFO LibraryDownloadManager: Downloading a library that was not in the cache: UploadedLibrary(org/scalaj/scalaj-http_2.11-2.3.0.jar,dbfs:/FileStore/jars/maven/org/scalaj/scalaj-http_2.11-2.3.0.jar,scala)
19/10/14 21:53:06 INFO LibraryDownloadManager: Downloaded library org/scalaj/scalaj-http_2.11-2.3.0.jar as local file /local_disk0/tmp/addedFile8136647889272239402org_scalaj_scalaj_http_2_11_2_3_0-d69e4.jar
19/10/14 21:53:06 INFO SharedDriverContext: Adding libraries that haven't been added before: Set(/local_disk0/tmp/addedFile8136647889272239402org_scalaj_scalaj_http_2_11_2_3_0-d69e4.jar)
19/10/14 21:53:06 INFO SparkContext: Added file /local_disk0/tmp/addedFile8136647889272239402org_scalaj_scalaj_http_2_11_2_3_0-d69e4.jar at spark://10.139.64.38:42579/files/addedFile8136647889272239402org_scalaj_scalaj_http_2_11_2_3_0-d69e4.jar with timestamp 1571089986503
19/10/14 21:53:06 INFO Utils: Copying /local_disk0/tmp/addedFile8136647889272239402org_scalaj_scalaj_http_2_11_2_3_0-d69e4.jar to /local_disk0/spark-e806244c-29d6-435a-8c83-5f5624a96f00/userFiles-4ef8e035-1ebc-4db1-af54-2d3c9c23d05e/addedFile8136647889272239402org_scalaj_scalaj_http_2_11_2_3_0-d69e4.jar
19/10/14 21:53:06 INFO SparkContext: Added JAR /local_disk0/tmp/addedFile8136647889272239402org_scalaj_scalaj_http_2_11_2_3_0-d69e4.jar at spark://10.139.64.38:42579/jars/addedFile8136647889272239402org_scalaj_scalaj_http_2_11_2_3_0-d69e4.jar with timestamp 1571089986511
19/10/14 21:53:41 INFO DriverCorral: Starting repl ReplId-1e5c7-4477f-40ebf-2
19/10/14 21:53:41 INFO LogicalPlanStats: Setting LogicalPlanStats visitor to com.databricks.sql.optimizer.statsEstimation.DatabricksLogicalPlanStatsVisitor$
19/10/14 21:53:42 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.0 using maven.
19/10/14 21:53:42 INFO IsolatedClientLoader: Initiating download of metastore jars from maven. This may take a while and is not recommended for production use. Please follow the instructions here: https://docs.databricks.com/user-guide/advanced/external-hive-metastore.html#spark-options on how to download the jars just once and use them in your cluster configuration. A log message beginning with 'Downloaded metastore jars' will print once the download is complete.
19/10/14 21:54:39 INFO IsolatedClientLoader: Downloaded metastore jars to /local_disk0/tmp/hive-v2_3-5bc3a090-fa8f-4e49-a2bb-cd7706a1bd9f
19/10/14 21:54:39 INFO HiveConf: Found configuration file null
19/10/14 21:54:39 INFO SessionState: Created HDFS directory: /tmp/hive/root/59d7b4c6-8b84-4ea2-866a-b727bbd8ebe9
19/10/14 21:54:39 INFO SessionState: Created local directory: /local_disk0/tmp/root/59d7b4c6-8b84-4ea2-866a-b727bbd8ebe9
19/10/14 21:54:39 INFO SessionState: Created HDFS directory: /tmp/hive/root/59d7b4c6-8b84-4ea2-866a-b727bbd8ebe9/_tmp_space.db
19/10/14 21:54:39 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.3) is adl://connectedsystemsuatdl.azuredatalakestore.net/user/hive/warehouse
19/10/14 21:54:40 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
19/10/14 21:54:40 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
19/10/14 21:54:40 INFO HiveMetaStore: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
19/10/14 21:54:40 INFO ObjectStore: ObjectStore, initialize called
19/10/14 21:54:40 INFO Persistence: Property spark.hadoop.datanucleus.connectionPool.maxIdle unknown - will be ignored
19/10/14 21:54:40 INFO Persistence: Property spark.hadoop.datanucleus.connectionPool.maxPoolSize unknown - will be ignored
19/10/14 21:54:40 INFO Persistence: Property datanucleus.fixedDatastore unknown - will be ignored
19/10/14 21:54:40 INFO Persistence: Property spark.hadoop.datanucleus.connectionPool.minPoolSize unknown - will be ignored
19/10/14 21:54:40 INFO Persistence: Property spark.hadoop.javax.jdo.option.ConnectionDriverName unknown - will be ignored
19/10/14 21:54:40 INFO Persistence: Property spark.hadoop.javax.jdo.option.ConnectionURL unknown - will be ignored
19/10/14 21:54:40 INFO Persistence: Property spark.hadoop.javax.jdo.option.ConnectionUserName unknown - will be ignored
19/10/14 21:54:40 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
19/10/14 21:54:40 INFO Persistence: Property spark.hadoop.javax.jdo.option.ConnectionPassword unknown - will be ignored
19/10/14 21:54:40 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
19/10/14 21:54:41 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
19/10/14 21:54:42 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is POSTGRES
19/10/14 21:54:42 INFO ObjectStore: Initialized ObjectStore
19/10/14 21:54:42 INFO HiveMetaStore: Added admin role in metastore
19/10/14 21:54:42 INFO HiveMetaStore: Added public role in metastore
19/10/14 21:54:42 INFO HiveMetaStore: No user is added in admin role, since config is empty
19/10/14 21:54:42 INFO HiveMetaStore: 0: get_all_functions
19/10/14 21:54:42 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_all_functions	
19/10/14 21:54:42 INFO Hive: Registering function collect brickhouse.udf.collect.CollectUDAF
19/10/14 21:54:42 INFO Hive: Registering function collect brickhouse.udf.collect.CollectUDAF
19/10/14 21:54:42 INFO HiveMetaStore: 0: get_database: default
19/10/14 21:54:42 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
19/10/14 21:54:42 INFO SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=59d7b4c6-8b84-4ea2-866a-b727bbd8ebe9, clientType=HIVECLI]
19/10/14 21:54:42 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.
19/10/14 21:54:42 INFO metastore: Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
19/10/14 21:54:42 INFO HiveMetaStore: 0: Cleaning up thread local RawStore...
19/10/14 21:54:42 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
19/10/14 21:54:42 INFO HiveMetaStore: 0: Done cleaning up thread local RawStore
19/10/14 21:54:42 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
19/10/14 21:54:42 INFO SessionState: Added [/local_disk0/tmp/addedFile1209439005989882887xml_apis_xml_apis_1_0_b2-167e6.jar] to class path
19/10/14 21:54:42 INFO SessionState: Added resources: [/local_disk0/tmp/addedFile1209439005989882887xml_apis_xml_apis_1_0_b2-167e6.jar]
19/10/14 21:54:42 WARN SparkContext: The jar /local_disk0/tmp/addedFile1209439005989882887xml_apis_xml_apis_1_0_b2-167e6.jar has been added already. Overwriting of added jars is not supported in the current version.
19/10/14 21:54:42 INFO SessionState: Added [/local_disk0/tmp/addedFile8136647889272239402org_scalaj_scalaj_http_2_11_2_3_0-d69e4.jar] to class path
19/10/14 21:54:42 INFO SessionState: Added resources: [/local_disk0/tmp/addedFile8136647889272239402org_scalaj_scalaj_http_2_11_2_3_0-d69e4.jar]
19/10/14 21:54:42 WARN SparkContext: The jar /local_disk0/tmp/addedFile8136647889272239402org_scalaj_scalaj_http_2_11_2_3_0-d69e4.jar has been added already. Overwriting of added jars is not supported in the current version.
19/10/14 21:54:42 INFO SessionState: Added [/local_disk0/tmp/addedFile5394604733753664878dbfs__FileStore_saak9001_transengine_1_0_SNAPSHOT-ab9b9.jar] to class path
19/10/14 21:54:42 INFO SessionState: Added resources: [/local_disk0/tmp/addedFile5394604733753664878dbfs__FileStore_saak9001_transengine_1_0_SNAPSHOT-ab9b9.jar]
19/10/14 21:54:42 WARN SparkContext: The jar /local_disk0/tmp/addedFile5394604733753664878dbfs__FileStore_saak9001_transengine_1_0_SNAPSHOT-ab9b9.jar has been added already. Overwriting of added jars is not supported in the current version.
19/10/14 21:54:42 INFO SessionState: Added [/local_disk0/tmp/addedFile9175644853842256239org_rogach_scallop_2_11_3_3_1-ecc25.jar] to class path
19/10/14 21:54:42 INFO SessionState: Added resources: [/local_disk0/tmp/addedFile9175644853842256239org_rogach_scallop_2_11_3_3_1-ecc25.jar]
19/10/14 21:54:42 WARN SparkContext: The jar /local_disk0/tmp/addedFile9175644853842256239org_rogach_scallop_2_11_3_3_1-ecc25.jar has been added already. Overwriting of added jars is not supported in the current version.
19/10/14 21:54:42 INFO SessionState: Added [/local_disk0/tmp/addedFile8338063297904481498dbfs__FileStore_saak9001_pqm_core_models_1_0-b0ccd.jar] to class path
19/10/14 21:54:42 INFO SessionState: Added resources: [/local_disk0/tmp/addedFile8338063297904481498dbfs__FileStore_saak9001_pqm_core_models_1_0-b0ccd.jar]
19/10/14 21:54:42 WARN SparkContext: The jar /local_disk0/tmp/addedFile8338063297904481498dbfs__FileStore_saak9001_pqm_core_models_1_0-b0ccd.jar has been added already. Overwriting of added jars is not supported in the current version.
19/10/14 21:54:42 INFO SessionState: Added [/local_disk0/tmp/addedFile5971024327773765715com_typesafe_config_1_3_1-e3e42.jar] to class path
19/10/14 21:54:42 INFO SessionState: Added resources: [/local_disk0/tmp/addedFile5971024327773765715com_typesafe_config_1_3_1-e3e42.jar]
19/10/14 21:54:42 WARN SparkContext: The jar /local_disk0/tmp/addedFile5971024327773765715com_typesafe_config_1_3_1-e3e42.jar has been added already. Overwriting of added jars is not supported in the current version.
19/10/14 21:54:42 INFO SessionState: Added [/local_disk0/tmp/addedFile8178506233776720821io_spray_spray_json_2_11_1_3_5-d5f78.jar] to class path
19/10/14 21:54:42 INFO SessionState: Added resources: [/local_disk0/tmp/addedFile8178506233776720821io_spray_spray_json_2_11_1_3_5-d5f78.jar]
19/10/14 21:54:42 WARN SparkContext: The jar /local_disk0/tmp/addedFile8178506233776720821io_spray_spray_json_2_11_1_3_5-d5f78.jar has been added already. Overwriting of added jars is not supported in the current version.
19/10/14 21:54:42 INFO SessionState: Added [/local_disk0/tmp/addedFile7957955701981159152dom4j_dom4j_1_6_1-3b35e.jar] to class path
19/10/14 21:54:42 INFO SessionState: Added resources: [/local_disk0/tmp/addedFile7957955701981159152dom4j_dom4j_1_6_1-3b35e.jar]
19/10/14 21:54:42 WARN SparkContext: The jar /local_disk0/tmp/addedFile7957955701981159152dom4j_dom4j_1_6_1-3b35e.jar has been added already. Overwriting of added jars is not supported in the current version.
19/10/14 21:54:42 INFO ScalaDriverWrapper: setupRepl:ReplId-1e5c7-4477f-40ebf-2: finished to load
19/10/14 21:54:42 WARN ScalaDriverLocal: User entity information is incomplete, missing userId,orgId,user.
19/10/14 21:54:42 INFO ProgressReporter$: Added result fetcher for 2187751369421679602_6599199323551964695_58a17abdafa24186b09b672cf4ae95b9
19/10/14 21:54:42 INFO SignalUtils: Registered signal handler for INT
19/10/14 21:54:43 INFO SQLAppStatusListener: Execution ID: 0 Total Executor Run Time: 0
19/10/14 21:54:43 INFO SQLAppStatusListener: Execution ID: 1 Total Executor Run Time: 0
19/10/14 21:54:43 INFO SQLAppStatusListener: Execution ID: 2 Total Executor Run Time: 0
19/10/14 21:54:43 INFO SQLAppStatusListener: Execution ID: 3 Total Executor Run Time: 0
19/10/14 21:54:43 INFO SQLAppStatusListener: Execution ID: 4 Total Executor Run Time: 0
19/10/14 21:54:43 INFO SQLAppStatusListener: Execution ID: 5 Total Executor Run Time: 0
19/10/14 21:54:43 INFO SQLAppStatusListener: Execution ID: 6 Total Executor Run Time: 0
19/10/14 21:54:43 INFO SQLAppStatusListener: Execution ID: 7 Total Executor Run Time: 0
19/10/14 21:54:44 INFO DriverILoop: Set class prefix to: line7305a6cc660b44979d6f9456263fd2e2
19/10/14 21:54:44 INFO DriverILoop: set ContextClassLoader
19/10/14 21:54:44 INFO DriverILoop: initialized intp
19/10/14 21:54:47 INFO ProgressReporter$: Removed result fetcher for 2187751369421679602_6599199323551964695_58a17abdafa24186b09b672cf4ae95b9
19/10/14 21:54:50 INFO DriverCorral: Starting repl ReplId-41832-333bd-63fac-b
19/10/14 21:54:50 INFO LogicalPlanStats: Setting LogicalPlanStats visitor to com.databricks.sql.optimizer.statsEstimation.DatabricksLogicalPlanStatsVisitor$
19/10/14 21:54:50 INFO SessionState: Added [/local_disk0/tmp/addedFile1209439005989882887xml_apis_xml_apis_1_0_b2-167e6.jar] to class path
19/10/14 21:54:50 INFO SessionState: Added resources: [/local_disk0/tmp/addedFile1209439005989882887xml_apis_xml_apis_1_0_b2-167e6.jar]
19/10/14 21:54:50 WARN SparkContext: The jar /local_disk0/tmp/addedFile1209439005989882887xml_apis_xml_apis_1_0_b2-167e6.jar has been added already. Overwriting of added jars is not supported in the current version.
19/10/14 21:54:50 INFO SQLAppStatusListener: Execution ID: 8 Total Executor Run Time: 0
19/10/14 21:54:50 INFO SessionState: Added [/local_disk0/tmp/addedFile8136647889272239402org_scalaj_scalaj_http_2_11_2_3_0-d69e4.jar] to class path
19/10/14 21:54:50 INFO SessionState: Added resources: [/local_disk0/tmp/addedFile8136647889272239402org_scalaj_scalaj_http_2_11_2_3_0-d69e4.jar]
19/10/14 21:54:50 WARN SparkContext: The jar /local_disk0/tmp/addedFile8136647889272239402org_scalaj_scalaj_http_2_11_2_3_0-d69e4.jar has been added already. Overwriting of added jars is not supported in the current version.
19/10/14 21:54:50 INFO SQLAppStatusListener: Execution ID: 9 Total Executor Run Time: 0
19/10/14 21:54:50 INFO SessionState: Added [/local_disk0/tmp/addedFile5394604733753664878dbfs__FileStore_saak9001_transengine_1_0_SNAPSHOT-ab9b9.jar] to class path
19/10/14 21:54:50 INFO SessionState: Added resources: [/local_disk0/tmp/addedFile5394604733753664878dbfs__FileStore_saak9001_transengine_1_0_SNAPSHOT-ab9b9.jar]
19/10/14 21:54:50 WARN SparkContext: The jar /local_disk0/tmp/addedFile5394604733753664878dbfs__FileStore_saak9001_transengine_1_0_SNAPSHOT-ab9b9.jar has been added already. Overwriting of added jars is not supported in the current version.
19/10/14 21:54:50 INFO SQLAppStatusListener: Execution ID: 10 Total Executor Run Time: 0
19/10/14 21:54:50 INFO SessionState: Added [/local_disk0/tmp/addedFile9175644853842256239org_rogach_scallop_2_11_3_3_1-ecc25.jar] to class path
19/10/14 21:54:50 INFO SessionState: Added resources: [/local_disk0/tmp/addedFile9175644853842256239org_rogach_scallop_2_11_3_3_1-ecc25.jar]
19/10/14 21:54:50 WARN SparkContext: The jar /local_disk0/tmp/addedFile9175644853842256239org_rogach_scallop_2_11_3_3_1-ecc25.jar has been added already. Overwriting of added jars is not supported in the current version.
19/10/14 21:54:50 INFO SQLAppStatusListener: Execution ID: 11 Total Executor Run Time: 0
19/10/14 21:54:50 INFO SessionState: Added [/local_disk0/tmp/addedFile8338063297904481498dbfs__FileStore_saak9001_pqm_core_models_1_0-b0ccd.jar] to class path
19/10/14 21:54:50 INFO SessionState: Added resources: [/local_disk0/tmp/addedFile8338063297904481498dbfs__FileStore_saak9001_pqm_core_models_1_0-b0ccd.jar]
19/10/14 21:54:50 WARN SparkContext: The jar /local_disk0/tmp/addedFile8338063297904481498dbfs__FileStore_saak9001_pqm_core_models_1_0-b0ccd.jar has been added already. Overwriting of added jars is not supported in the current version.
19/10/14 21:54:50 INFO SQLAppStatusListener: Execution ID: 12 Total Executor Run Time: 0
19/10/14 21:54:50 INFO SessionState: Added [/local_disk0/tmp/addedFile5971024327773765715com_typesafe_config_1_3_1-e3e42.jar] to class path
19/10/14 21:54:50 INFO SessionState: Added resources: [/local_disk0/tmp/addedFile5971024327773765715com_typesafe_config_1_3_1-e3e42.jar]
19/10/14 21:54:50 WARN SparkContext: The jar /local_disk0/tmp/addedFile5971024327773765715com_typesafe_config_1_3_1-e3e42.jar has been added already. Overwriting of added jars is not supported in the current version.
19/10/14 21:54:50 INFO SQLAppStatusListener: Execution ID: 13 Total Executor Run Time: 0
19/10/14 21:54:50 INFO SessionState: Added [/local_disk0/tmp/addedFile8178506233776720821io_spray_spray_json_2_11_1_3_5-d5f78.jar] to class path
19/10/14 21:54:50 INFO SessionState: Added resources: [/local_disk0/tmp/addedFile8178506233776720821io_spray_spray_json_2_11_1_3_5-d5f78.jar]
19/10/14 21:54:50 WARN SparkContext: The jar /local_disk0/tmp/addedFile8178506233776720821io_spray_spray_json_2_11_1_3_5-d5f78.jar has been added already. Overwriting of added jars is not supported in the current version.
19/10/14 21:54:50 INFO SQLAppStatusListener: Execution ID: 14 Total Executor Run Time: 0
19/10/14 21:54:50 INFO SessionState: Added [/local_disk0/tmp/addedFile7957955701981159152dom4j_dom4j_1_6_1-3b35e.jar] to class path
19/10/14 21:54:50 INFO SessionState: Added resources: [/local_disk0/tmp/addedFile7957955701981159152dom4j_dom4j_1_6_1-3b35e.jar]
19/10/14 21:54:50 WARN SparkContext: The jar /local_disk0/tmp/addedFile7957955701981159152dom4j_dom4j_1_6_1-3b35e.jar has been added already. Overwriting of added jars is not supported in the current version.
19/10/14 21:54:50 INFO ScalaDriverWrapper: setupRepl:ReplId-41832-333bd-63fac-b: finished to load
19/10/14 21:54:50 INFO SQLAppStatusListener: Execution ID: 15 Total Executor Run Time: 0
19/10/14 21:54:50 WARN ScalaDriverLocal: User entity information is incomplete, missing userId,user.
19/10/14 21:54:50 INFO ProgressReporter$: Added result fetcher for 4720655539542162123_5433164671276069350_job-67412-run-1-action-71566
19/10/14 21:54:51 INFO DriverILoop: Set class prefix to: line190cbeb1cfbc404da176cd057f8a5f2e
19/10/14 21:54:51 INFO DriverILoop: set ContextClassLoader
19/10/14 21:54:51 INFO DriverILoop: initialized intp
19/10/14 21:54:54 WARN SparkContext: Using an existing SparkContext; some configuration may not take effect.
19/10/14 21:54:54 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
19/10/14 21:54:54 INFO Instructions: Instruction File has been Read Successfully.>>>>>>>>>>>
19/10/14 21:54:55 INFO HiveMetaStore: 1: get_database: rms_1000233_pqm
19/10/14 21:54:55 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: rms_1000233_pqm	
19/10/14 21:54:55 WARN HiveConf: HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist
19/10/14 21:54:55 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
19/10/14 21:54:55 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
19/10/14 21:54:55 INFO HiveMetaStore: 1: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
19/10/14 21:54:55 INFO ObjectStore: ObjectStore, initialize called
19/10/14 21:54:55 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is POSTGRES
19/10/14 21:54:55 INFO ObjectStore: Initialized ObjectStore
19/10/14 21:54:55 WARN ObjectStore: Failed to get database rms_1000233_pqm, returning NoSuchObjectException
19/10/14 21:54:55 INFO HiveMetaStore: 1: get_database: rms_1000233_pqm
19/10/14 21:54:55 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: rms_1000233_pqm	
19/10/14 21:54:55 WARN ObjectStore: Failed to get database rms_1000233_pqm, returning NoSuchObjectException
19/10/14 21:54:55 INFO SQLAppStatusListener: Execution ID: 16 Total Executor Run Time: 0
19/10/14 21:54:55 INFO HiveMetaStore: 1: get_database: global_temp
19/10/14 21:54:55 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: global_temp	
19/10/14 21:54:55 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException
19/10/14 21:54:55 INFO HiveMetaStore: 1: create_database: Database(name:rms_1000233_pqm, description:, locationUri:adl://connectedsystemsuatdl.azuredatalakestore.net/user/hive/warehouse/RMS_1000233_PQM.db, parameters:{})
19/10/14 21:54:55 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=create_database: Database(name:rms_1000233_pqm, description:, locationUri:adl://connectedsystemsuatdl.azuredatalakestore.net/user/hive/warehouse/RMS_1000233_PQM.db, parameters:{})	
19/10/14 21:54:55 WARN ObjectStore: Failed to get database rms_1000233_pqm, returning NoSuchObjectException
19/10/14 21:54:55 DEBUG AccessTokenProvider: AADToken: no token. Returning expiring=true
19/10/14 21:54:55 DEBUG AccessTokenProvider: AAD Token is missing or expired: Calling refresh-token from abstract base class
19/10/14 21:54:55 DEBUG ClientCredsTokenProvider: AADToken: refreshing client-credential based token
19/10/14 21:54:55 DEBUG AzureADAuthenticator: AADToken: starting to fetch token using client creds for client ID 003106c3-ff8c-4c62-a060-24b8b32ea512
19/10/14 21:54:55 DEBUG AzureADAuthenticator: AADToken: fetched token with expiry Tue Oct 15 05:54:54 UTC 2019
19/10/14 21:54:56 DEBUG HttpTransport: HTTPRequest,Failed,cReqId:42dd39d2-f58c-416d-a4f1-65c3b1316808.0,lat:892,err:HTTP404(FileNotFoundException),Reqlen:0,Resplen:259,token_ns:175136400,sReqId:8cbf70b7-2182-4ced-a153-498aaa921967,path:/user/hive/warehouse/RMS_1000233_PQM.db,qp:op=GETFILESTATUS&tooid=true&api-version=2017-08-01
19/10/14 21:54:56 INFO FileUtils: Creating directory if it doesn't exist: adl://connectedsystemsuatdl.azuredatalakestore.net/user/hive/warehouse/RMS_1000233_PQM.db
19/10/14 21:54:56 INFO SQLAppStatusListener: Execution ID: 17 Total Executor Run Time: 0
19/10/14 21:54:56 INFO FileToDimension: FileToDimension Some(MarketCSV) for MRKT_DIM input file wasbs://teuatcontainer01@teuatstorage.blob.core.windows.net//RMS_1000233_PQM/MRKT_DIM.csv>>>>> Started
19/10/14 21:54:56 ERROR ScalaDriverLocal: User Code Stack Trace: 
org.apache.spark.sql.AnalysisException: Path does not exist: wasbs://teuatcontainer01@teuatstorage.blob.core.windows.net/RMS_1000233_PQM/MRKT_DIM.csv;
	at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary$1.apply(DataSource.scala:612)
	at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary$1.apply(DataSource.scala:595)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
	at scala.collection.immutable.List.foreach(List.scala:381)
	at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)
	at scala.collection.immutable.List.flatMap(List.scala:344)
	at org.apache.spark.sql.execution.datasources.DataSource.org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary(DataSource.scala:595)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:390)
	at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:298)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:279)
	at org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:707)
	at org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:551)
	at com.nielsen.te.v3.core.executors.FileToDimension.executeStep(FileToDimension.scala:30)
	at com.nielsen.te.v3.executor.LibraryStepExecutor.executeStep(StepExecutor.scala:56)
	at com.nielsen.te.v3.executor.StepProcessor$$anonfun$stepProcessor$1.apply(StepProcessor.scala:129)
	at com.nielsen.te.v3.executor.StepProcessor$$anonfun$stepProcessor$1.apply(StepProcessor.scala:127)
	at scala.collection.immutable.List.foreach(List.scala:381)
	at com.nielsen.te.v3.executor.StepProcessor$.stepProcessor(StepProcessor.scala:127)
	at com.nielsen.te.v3.TEv3Main$.appMain(TEv3Main.scala:47)
	at com.nielsen.te.v3.TEv3Main$.main(TEv3Main.scala:51)
	at line190cbeb1cfbc404da176cd057f8a5f2e25.$read$$iw$$iw$$iw$$iw$$iw$$iw.<init>(command--1:1)
	at line190cbeb1cfbc404da176cd057f8a5f2e25.$read$$iw$$iw$$iw$$iw$$iw.<init>(command--1:44)
	at line190cbeb1cfbc404da176cd057f8a5f2e25.$read$$iw$$iw$$iw$$iw.<init>(command--1:46)
	at line190cbeb1cfbc404da176cd057f8a5f2e25.$read$$iw$$iw$$iw.<init>(command--1:48)
	at line190cbeb1cfbc404da176cd057f8a5f2e25.$read$$iw$$iw.<init>(command--1:50)
	at line190cbeb1cfbc404da176cd057f8a5f2e25.$read$$iw.<init>(command--1:52)
	at line190cbeb1cfbc404da176cd057f8a5f2e25.$read.<init>(command--1:54)
	at line190cbeb1cfbc404da176cd057f8a5f2e25.$read$.<init>(command--1:58)
	at line190cbeb1cfbc404da176cd057f8a5f2e25.$read$.<clinit>(command--1)
	at line190cbeb1cfbc404da176cd057f8a5f2e25.$eval$.$print$lzycompute(<notebook>:7)
	at line190cbeb1cfbc404da176cd057f8a5f2e25.$eval$.$print(<notebook>:6)
	at line190cbeb1cfbc404da176cd057f8a5f2e25.$eval.$print(<notebook>)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:786)
	at scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:1047)
	at scala.tools.nsc.interpreter.IMain$WrappedRequest$$anonfun$loadAndRunReq$1.apply(IMain.scala:638)
	at scala.tools.nsc.interpreter.IMain$WrappedRequest$$anonfun$loadAndRunReq$1.apply(IMain.scala:637)
	at scala.reflect.internal.util.ScalaClassLoader$class.asContext(ScalaClassLoader.scala:31)
	at scala.reflect.internal.util.AbstractFileClassLoader.asContext(AbstractFileClassLoader.scala:19)
	at scala.tools.nsc.interpreter.IMain$WrappedRequest.loadAndRunReq(IMain.scala:637)
	at scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:569)
	at scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:565)
	at com.databricks.backend.daemon.driver.DriverILoop.execute(DriverILoop.scala:199)
	at com.databricks.backend.daemon.driver.ScalaDriverLocal$$anonfun$repl$1.apply$mcV$sp(ScalaDriverLocal.scala:189)
	at com.databricks.backend.daemon.driver.ScalaDriverLocal$$anonfun$repl$1.apply(ScalaDriverLocal.scala:189)
	at com.databricks.backend.daemon.driver.ScalaDriverLocal$$anonfun$repl$1.apply(ScalaDriverLocal.scala:189)
	at com.databricks.backend.daemon.driver.DriverLocal$TrapExitInternal$.trapExit(DriverLocal.scala:587)
	at com.databricks.backend.daemon.driver.DriverLocal$TrapExit$.apply(DriverLocal.scala:542)
	at com.databricks.backend.daemon.driver.ScalaDriverLocal.repl(ScalaDriverLocal.scala:189)
	at com.databricks.backend.daemon.driver.DriverLocal$$anonfun$execute$7.apply(DriverLocal.scala:324)
	at com.databricks.backend.daemon.driver.DriverLocal$$anonfun$execute$7.apply(DriverLocal.scala:304)
	at com.databricks.logging.UsageLogging$$anonfun$withAttributionContext$1.apply(UsageLogging.scala:235)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at com.databricks.logging.UsageLogging$class.withAttributionContext(UsageLogging.scala:230)
	at com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:45)
	at com.databricks.logging.UsageLogging$class.withAttributionTags(UsageLogging.scala:268)
	at com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:45)
	at com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:304)
	at com.databricks.backend.daemon.driver.DriverWrapper$$anonfun$tryExecutingCommand$2.apply(DriverWrapper.scala:589)
	at com.databricks.backend.daemon.driver.DriverWrapper$$anonfun$tryExecutingCommand$2.apply(DriverWrapper.scala:589)
	at scala.util.Try$.apply(Try.scala:192)
	at com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:584)
	at com.databricks.backend.daemon.driver.DriverWrapper.getCommandOutputAndError(DriverWrapper.scala:475)
	at com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:542)
	at com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:381)
	at com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:328)
	at com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:215)
	at java.lang.Thread.run(Thread.java:748)
19/10/14 21:54:56 INFO ProgressReporter$: Removed result fetcher for 4720655539542162123_5433164671276069350_job-67412-run-1-action-71566
19/10/14 21:54:57 INFO DriverCorral$: Cleaning the wrapper ReplId-41832-333bd-63fac-b (currently in status Idle(ReplId-41832-333bd-63fac-b))
19/10/14 21:54:57 INFO DriverCorral$: sending shutdown signal for REPL ReplId-41832-333bd-63fac-b
19/10/14 21:54:57 INFO DriverCorral$: sending the interrupt signal for REPL ReplId-41832-333bd-63fac-b
19/10/14 21:54:57 INFO DriverCorral$: waiting for localThread to stop for REPL ReplId-41832-333bd-63fac-b
19/10/14 21:54:57 INFO DriverCorral$: ReplId-41832-333bd-63fac-b successfully discarded
19/10/14 21:54:58 ERROR TaskSchedulerImpl: Lost executor 0 on 10.139.64.42: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
19/10/14 21:54:58 INFO DAGScheduler: Executor lost: 0 (epoch 0)
19/10/14 21:54:58 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20191014215245-0000/0 is now LOST (worker lost)
19/10/14 21:54:58 INFO BlockManagerMasterEndpoint: Trying to remove executor 0 from BlockManagerMaster.
19/10/14 21:54:58 INFO StandaloneSchedulerBackend: Executor app-20191014215245-0000/0 removed: worker lost
19/10/14 21:54:58 INFO BlockManagerMasterEndpoint: Removing block manager BlockManagerId(0, 10.139.64.42, 35707, None)
19/10/14 21:54:58 INFO BlockManagerMaster: Removed 0 successfully in removeExecutor
19/10/14 21:54:58 INFO StandaloneAppClient$ClientEndpoint: Master removed worker worker-20191014215244-10.139.64.42-37163: 10.139.64.42:37163 got disassociated
19/10/14 21:54:58 INFO StandaloneSchedulerBackend: Worker worker-20191014215244-10.139.64.42-37163 removed: 10.139.64.42:37163 got disassociated
19/10/14 21:54:58 INFO BlockManagerMasterEndpoint: Trying to remove executor 0 from BlockManagerMaster.
19/10/14 21:54:58 INFO BlockManagerMaster: Removal of executor 0 requested
19/10/14 21:54:58 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asked to remove non-existent executor 0
19/10/14 21:54:58 INFO TaskSchedulerImpl: Handle removed worker worker-20191014215244-10.139.64.42-37163: 10.139.64.42:37163 got disassociated
19/10/14 21:54:58 INFO DAGScheduler: Shuffle files lost for worker worker-20191014215244-10.139.64.42-37163 on host 10.139.64.42
